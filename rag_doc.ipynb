{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-1.5-pro\",google_api_key = GOOGLE_API_KEY,temperature =0, convert_system_message_to_human=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"C:\\Projects\\Langchain\\Rag_doc\\Applied Natural Language Processing.pdf\")\n",
    "pages = pdf_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 0}, page_content='Applied Natural \\nLanguage Processing \\nwith Python\\nImplementing Machine Learning  \\nand Deep Learning Algorithms for  \\nNatural Language Processing\\n—\\nTaweh Beysolow II'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 1}, page_content='Applied Natural \\nLanguage Processing \\nwith Python\\nImplementing Machine \\nLearning and Deep Learning \\nAlgorithms for Natural \\nLanguage Processing\\nTaweh  Beysolow  II'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 2}, page_content='Applied Natural Language Processing with Python\\nISBN-13 (pbk): 978-1-4842-3732-8     ISBN-13 (electronic): 978-1-4842-3733-5\\nhttps://doi.org/10.1007/978-1-4842-3733-5\\nLibrary of Congress Control Number: 2018956300\\nCopyright © 2018 by Taweh Beysolow II \\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or \\npart of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, \\nand transmission or information storage and retrieval, electronic adaptation, computer software, \\nor by similar or dissimilar methodology now known or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark \\nsymbol with every occurrence of a trademarked name, logo, or image we use the names, logos, \\nand images only in an editorial fashion and to the benefit of the trademark owner, with no \\nintention of infringement of the trademark. \\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if \\nthey are not identified as such, is not to be taken as an expression of opinion as to whether or not \\nthey are subject to proprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the date of \\npublication, neither the authors nor the editors nor the publisher can accept any legal \\nresponsibility for any errors or omissions that may be made. The publisher makes no warranty, \\nexpress or implied, with respect to the material contained herein.\\nManaging Director, Apress Media LLC: Welmoed Spahr\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Siddhi Chavan\\nCoordinating Editor: Divya Modi\\nCover designed by eStudioCalamar\\nCover image designed by Freepik (www.freepik.com)\\nDistributed to the book trade worldwide by Springer Science+Business Media New\\xa0York,  \\n233 Spring Street, 6th Floor, New\\xa0York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505,  \\ne-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a \\nCalifornia LLC and the sole member (owner) is Springer Science + Business Media Finance Inc \\n(SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.\\nFor information on translations, please e-mail rights@apress.com, or visit http://www.apress.\\ncom/rights-permissions.\\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook \\nversions and licenses are also available for most titles. For more information, reference our Print \\nand eBook Bulk Sales web page at http://www.apress.com/bulk-sales.\\nAny source code or other supplementary material referenced by the author in this book is \\navailable to readers on GitHub via the book’s product page, located at www.apress.com/ \\n978-1-4842-3732-8. For more detailed information, please visit http://www.apress.com/\\nsource-code.\\nPrinted on acid-free paperTaweh\\xa0Beysolow\\xa0II\\nSan Francisco, California, USA'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 3}, page_content='To my family, friends, and colleagues for their continued \\nsupport and encouragement to do more with myself than  \\nI often can conceive of doing'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 4}, page_content='vTable of Contents\\nChapter 1 :  What Is Natural Language Pr ocessing?  ������������������������������ 1\\nThe History of  Natural Language Processing  �������������������������������������������������������� 2\\nA Review of  Machine Learning and  Deep Lear ning ���������������������������������������������� 4\\nNLP, Machine Learning, and  Deep Lear ning Packages with  Python  ���������������� 4\\nApplications of  Deep Lear ning to  NLP ����������������������������������������������������������� 10\\nSummar y������������������������������������������������������������������������������������������������������������� 12\\nChapter 2 :  Review of\\xa0Deep Learning ��������������������������������������������������� 13\\nMultilayer Perceptrons and  Recurrent Neural Networks  ������������������������������������ 13\\nToy Example 1: Modeling Stock Returns with  the MLP Model  ����������������������� 15\\nVanishing Gradients and  Why ReLU Helps to  Prevent Them  �������������������������� 27\\nLoss Functions and  Backpropagation  ������������������������������������������������������������ 29\\nRecurr ent Neural Networks and  Long Short-T erm Memory  �������������������������� 30\\nToy Example 2: Modeling Stock Returns with  the RNN Model  ����������������������� 32\\nToy Example 3: Modeling Stock Returns with  the LSTM Model  ��������������������� 40\\nSummary ������������������������������������������������������������������������������������������������������������� 41About the Author  ��������������������������������������������������������������������������������� ix\\nAbout the Technical Review er ������������������������������������������������������������� xi\\nAcknowledgments  ����������������������������������������������������������������������������� xiii\\nIntroduction  ���������������������������������������������������������������������������������������� xv'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 5}, page_content='viChapter 3 :  Working with\\xa0 Ra w Text  ���������������������������������������������������� 43\\nTokenization and  Stop Words  ������������������������������������������������������������������������������ 44\\nThe Bag-of-Wor ds Model (BoW)  �������������������������������������������������������������������������� 50\\nCountV ectorizer  ��������������������������������������������������������������������������������������������� 51\\nExample Problem 1:  Spam Detection  ������������������������������������������������������������ 53\\nTerm Frequency Inverse Document Frequency  ��������������������������������������������� 57\\nExample Problem 2:  Classifying Movie Reviews  ������������������������������������������� 62\\nSummar y������������������������������������������������������������������������������������������������������������� 74\\nChapter 4 :  Topic Modeling and\\xa0 Word Embeddings  ���������������������������� 77\\nTopic Model and  Latent Dirichlet Allocation (LDA)  ���������������������������������������������� 77\\nTopic Modeling with  LDA on  Movie Review Data  ������������������������������������������� 81\\nNon-Negative Matrix Factorization (NMF)  ����������������������������������������������������������� 86\\nWord2Vec  ������������������������������������������������������������������������������������������������������������ 90\\nExample Problem 4 �2: Training a  Word Embedding (Skip-Gram)  ������������������� 94\\nContinuous Bag-of-Wor ds (CBoW)  �������������������������������������������������������������������� 103\\nExample Problem 4 �2: Training a  Word Embedding (CBoW)  ������������������������� 105\\nGlobal Vectors for  Word Representation (GloVe)  ����������������������������������������������� 106\\nExample Problem 4 �4: Using Trained Word Embeddings with  LSTMs  ���������� 111\\nParagraph2Vec: Distributed Memory of  Paragraph Vectors (PV-DM)  ���������������� 115\\nExample Problem 4 �5: Paragraph2Vec Example with  Movie  \\nReview Data  ������������������������������������������������������������������������������������������������ 116\\nSummar y����������������������������������������������������������������������������������������������������������� 118\\nChapter 5 :  Text Generation, Machine Translation, and\\xa0Other  \\nRecurr ent Language Modeling Tasks  ������������������������������ 121\\nText Generation with  LSTMs  ����������������������������������������������������������������������������� 122\\nBidirectional RNNs (BRNN)  �������������������������������������������������������������������������� 126Table of Con TenTs Table of Con TenTs'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 6}, page_content='viiCreating a  Name Entity Recognition Tagger  ������������������������������������������������������ 128\\nSequence-to-Sequence Models (Seq2Seq)  ������������������������������������������������������ 133\\nQuestion and  Answer with  Neur al Network Models  ������������������������������������������ 134\\nSummar y����������������������������������������������������������������������������������������������������������� 141\\nConclusion and  Final Statements  ��������������������������������������������������������������������� 142\\n Index  ������������������������������������������������������������������������������������������������� 145Table of Con TenTs Table of Con TenTs'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 7}, page_content='ixAbout the Author\\nTaweh\\xa0Beysolow  II\\xa0is a data scientist and \\nauthor currently based in San Francisco, \\nCalifornia. He has a bachelor’s degree in \\neconomics from St. Johns University and a \\nmaster’s degree in applied statistics from \\nFordham University. His professional \\nexperience has included working at Booz \\nAllen Hamilton, as a consultant and in various \\nstartups as a data scientist, specifically \\nfocusing on machine learning. He has applied machine learning to federal \\nconsulting, financial services, and agricultural sectors.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 8}, page_content='xiAbout the Technical Reviewer\\nSantanu\\xa0Pattanayak \\xa0currently works at GE \\nDigital as a staff data scientist and is the author \\nof the deep learning book  Pro Deep Learning \\nwith TensorFlow: A Mathematical Approach \\nto Advanced Artificial Intelligence in Python  \\n(Apress, 2017). He has more than eight years of \\nexperience in the data analytics/data science \\nfield and a background in development and \\ndatabase technologies. Prior to joining GE, \\nSantanu worked at companies such as RBS, \\nCapgemini, and IBM.\\xa0He graduated with a degree in electrical engineering \\nfrom Jadavpur University, Kolkata, and is an avid math enthusiast. Santanu \\nis currently pursuing a master’s degree in data science from the Indian \\nInstitute of Technology (IIT), Hyderabad. He also devotes his time to data \\nscience hackathons and Kaggle competitions, where he ranks within the \\ntop 500 across the globe. Santanu was born and brought up in West Bengal, \\nIndia, and currently resides in Bangalore, India, with his wife.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 9}, page_content='xiiiAcknowledgments\\nA special thanks to Santanu Pattanayak, Divya Modi, Celestin Suresh \\nJohn, and everyone at Apress for the wonderful experience. It has been a \\npleasure to work with you all on this text. I couldn’t have asked for a better \\nteam.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 10}, page_content='xvIntroduction\\nThank you for choosing Applied Natural Language Processing with Python  \\nfor your journey into natural language processing (NLP). Readers should \\nbe aware that this text should not be considered a comprehensive study \\nof machine learning, deep learning, or computer programming. As such, \\nit is assumed that you are familiar with these techniques to some degree. \\nRegardless, a brief review of the concepts necessary to understand the \\ntasks that you will perform in the book is provided.\\nAfter the brief review, we begin by examining how to work with raw \\ntext data, slowly working our way through how to present data to machine \\nlearning and deep learning algorithms. After you are familiar with some \\nbasic preprocessing algorithms, we will make our way into some of the \\nmore advanced NLP tasks, such as training and working with trained \\nword embeddings, spell-check, text generation, and question-and-answer \\ngeneration.\\nAll of the examples utilize the Python programming language and \\npopular deep learning and machine learning frameworks, such as scikit-  \\nlearn, Keras, and TensorFlow. Readers can feel free to access the source \\ncode utilized in this book on the corresponding GitHub page and/or try \\ntheir own methods for solving the various problems tackled in this book \\nwith the datasets provided.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 11}, page_content='1© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_1CHAPTER 1\\nWhat Is Natural \\nLanguage \\nProcessing?\\nDeep learning and machine learning continues to proliferate throughout \\nvarious industries, and has revolutionized the topic that I wish to discuss \\nin this book: natural language processing (NLP). NLP is a subfield of \\ncomputer science that is focused on allowing computers to understand \\nlanguage in a “natural” way, as humans do. Typically, this would refer to \\ntasks such as understanding the sentiment of text, speech recognition, and \\ngenerating responses to questions.\\nNLP has become a rapidly evolving field, and one whose applications \\nhave represented a large portion of artificial intelligence (AI) \\nbreakthroughs. Some examples of implementations using deep learning \\nare chatbots that handle customer service requests, auto-spellcheck on cell \\nphones, and AI assistants, such as Cortana and Siri, on smartphones. For \\nthose who have experience in machine learning and deep learning, natural \\nlanguage processing is one of the most exciting areas for individuals to \\napply their skills. To provide context for broader discussions, however, let’s \\ndiscuss the development of natural language processing as a field.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 12}, page_content='2 The History of\\xa0Natural Language Processing\\nNatural language processing can be classified as a subset of the broader \\nfield of speech and language processing. Because of this, NLP shares \\nsimilarities with parallel disciplines such as computational linguistics, \\nwhich is concerned with modeling language using rule-based models. \\nNLP’s inception can be traced back to the development of computer science \\nin the 1940s, moving forward along with advances in linguistics that led to \\nthe construction of formal language theory. Briefly, formal language theory \\nmodels language on increasingly complex structures and rules to these \\nstructures. For example, the alphabet is the simplest structure, in that it is \\na collection of letters that can form strings called words . A formal language \\nis one that has a regular, context-free, and formal grammar. In addition to \\nthe development of computer sciences as a whole, artificial intelligence’s \\nadvancements also played a role in our continuing understanding of NLP .\\nIn some sense, the single-layer perceptron (SLP) is considered to be the \\ninception of machine learning/AI.\\xa0Figure\\xa0 1-1 shows a photo of this model.\\nThe SLP was designed by neurophysiologist Warren McCulloch and \\nlogician Walter Pitt. It is the foundation of more advanced neural network \\nmodels that are heavily utilized today, such as multilayer perceptrons.  \\nFigure 1-1.  Single-layer perceptronChapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 13}, page_content='3The SLP model is seen to be in part due to Alan Turing’s research in the \\nlate 1930s on computation, which inspired other scientists and researchers \\nto develop different concepts, such as formal language theory.\\nMoving forward to the second half of the twentieth century, NLP starts \\nto bifurcate into two distinct groups of thought: (1) those who support a \\nsymbolic approach to language modelling, and (2) those who support a \\nstochastic approach. The former group was populated largely by linguists \\nwho used simple algorithms to solve NLP problems, often utilizing pattern \\nrecognition. The latter group was primarily composed of statisticians \\nand electrical engineers. Among the many approaches that were popular \\nwith the second group was Bayesian statistics. As the twentieth century \\nprogressed, NLP broadened as a field, including natural language \\nunderstanding (NLU) to the problem space (allowing computers to react \\naccurately to commands). For example, if someone spoke to a chatbot and \\nasked it to “find food near me, ” the chatbot would use NLU to translate this \\nsentence into tangible actions to yield a desirable outcome.\\nSkip closer to the present day, and we find that NLP has experienced \\na surge of interest alongside machine learning’s explosion in usage over \\nthe past 20 years. Part of this is due to the fact that large repositories of \\nlabeled data sets have become more available, in addition to an increase in \\ncomputing power. This increase in computing power is largely attributed \\nto the development of GPUs; nonetheless, it has proven vital to AI’s \\ndevelopment as a field. Accordingly, demand for materials to instruct \\ndata scientists and engineers on how to utilize various AI algorithms has \\nincreased, in part the reason for this book.\\nNow that you are aware of the history of NLP as it relates to the present \\nday, I will give a brief overview of what you should expect to learn. The \\nfocus, however, is primarily to discuss how deep learning has impacted \\nNLP , and how to utilize deep learning and machine learning techniques to \\nsolve NLP problems.Chapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 14}, page_content='4 A Review of\\xa0Machine Learning and\\xa0Deep \\nLearning\\nYou will be refreshed on important machine learning concepts, \\nparticularly deep learning models such as multilayer perceptrons  (MLPs), \\nrecurrent neural networks  (RNNs), and long short-term memory  (LSTM) \\nnetworks. You will be shown in-depth models utilizing toy examples before \\nyou tackle any specific NLP problems.\\n NLP , Machine Learning, and\\xa0Deep Learning \\nPackages with\\xa0Python\\nEqually important as understanding NLP theory is the ability to apply it in \\na practical context. This book utilizes the Python programming language, \\nas well as packages written in Python. Python has become the lingua \\nfranca for data scientists, and support of NLP , machine learning, and \\ndeep learning libraries is plentiful. I refer to many of these packages when \\nsolving the example problems and discussing general concepts.\\nIt is assumed that all readers of this book have a general understanding \\nof Python, such that you have the ability to write software in this language. \\nIf you are not familiar with this language, but you are familiar with others, \\nthe concepts in this book will be portable with respect to the methodology \\nused to solve problems, given the same data sets. Be that as it may, this \\nbook is not intended to instruct users on Python. Now, let’s discuss some of \\nthe technologies that are most important to understanding deep learning.\\n TensorFlow\\nOne of the groundbreaking releases in open source software, in addition \\nto machine learning at large, has undoubtedly been Google’s TensorFlow. \\nIt is an open source library for deep learning that is a successor to Theano, \\na similar machine learning library. Both utilize data flow graphs for Chapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 15}, page_content=\"5computational processes. Specifically, we can think of computations as \\ndependent on specific individual operations. TensorFlow functionally \\noperates by the user first defining a graph/model, which is then operated \\nby a TensorFlow session that the user also creates.\\nThe reasoning behind using a data flow graph rather than another \\ncomputational format computation is multifaceted, however one of the \\nmore simple benefits is the ability to port models from one language to \\nanother. Figure\\xa0 1-2 illustrates a data flow graph.\\nFor example, you may be working on a project where Java is the \\nlanguage that is most optimal for production software due to latency \\nreasons (high-frequency trading, for example); however, you would like to \\nutilize a neural network to make predictions in your production system. \\nRather than dealing with the time-consuming task of setting up a training \\nframework in Java for TensorFlow graphs, something could be written in \\nPython relatively quickly, and then the graph/model could be restored by \\nloading the weights in the production system by utilizing Java. TensorFlow \\ncode is similar to Theano code, as follows.\\n    #Creating weights and biases dictionaries\\n     weights = {'input': tf.Variable(tf.random_normal([state_\\nsize+1, state_size])),biases\\nweights\\ninputs\\ntargetsMatMulAdd Softmax\\nXentGraph of Nodes , also called operations (ops)\\nFigure 1-2.  Data flow graph diagramChapter 1  What Is Natural laNguage proCessINg?\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 16}, page_content=\"6         'output': tf.Variable(tf.random_normal([state_size, \\nn_classes]))}\\n     biases = {'input': tf.Variable(tf.random_normal([1, state_\\nsize])),\\n         'output': tf.Variable(tf.random_normal([1, n_classes]))}\\n    #Defining placeholders and variables\\n    X = tf.placeholder(tf.float32, [batch_size, bprop_len])\\n    Y = tf.placeholder(tf.int32, [batch_size, bprop_len])\\n     init_state = tf.placeholder(tf.float32, [batch_size, state_\\nsize])\\n    input_series = tf.unstack(X, axis=1)\\n    labels = tf.unstack(Y, axis=1)\\n    current_state = init_state\\n    hidden_states = []\\n    #Passing values from one hidden state to the next\\n     for input in input_series: #Evaluating each input within \\nthe series of inputs\\n         input = tf.reshape(input, [batch_size, 1]) #Reshaping \\ninput into MxN tensor\\n         input_state = tf.concat([input, current_state], axis=1) \\n#Concatenating input and current state tensors\\n         _hidden_state = tf.tanh(tf.add(tf.matmul(input_\\nstate, weights['input']), biases['input'])) #Tanh \\ntransformation\\n         hidden_states.append(_hidden_state) #Appending the next \\nstate\\n        current_state = _hidden_state #Updating the current state\\nTensorFlow is not always the easiest library to use, however, as there \\noften serious gaps between documentation for toy examples vs.  real-  \\nworld examples that reasonably walk the reader through the complexity of \\nimplementing a deep learning model.Chapter 1  What Is Natural laNguage proCessINg?\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 17}, page_content=\"7In some ways, TensorFlow can be thought of as a language inside of \\nPython, in that there are syntactical nuances that readers must become \\naware of before they can write applications seamlessly (if ever). These \\nconcerns, in some sense, were answered by Keras.\\n Keras\\nDue to the slow development process of applications in TensorFlow, \\nTheano, and similar deep learning frameworks, Keras was developed for \\nprototyping applications, but it is also utilized in production engineering \\nfor various problems. It is a wrapper for TensorFlow, Theano, MXNet, and \\nDeepLearning4j. Unlike these frameworks, defining a computational graph \\nis relatively easy, as shown in the following Keras demo code.\\ndef create_model():\\n    model = Sequential()\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       input_shape=(None, 40, 40, 1),\\n                       padding='same', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding='same', return_sequences=True))\\n    model.add(BatchNormalization())\\n    \\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding='same', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding='same', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),Chapter 1  What Is Natural laNguage proCessINg?\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 18}, page_content=\"8                   activation='sigmoid',\\n                   padding='same', data_format='channels_last'))\\n    model.compile(loss='binary_crossentropy', optimizer='adadelta')\\n    return model\\nAlthough having the added benefit of ease of use and speed with \\nrespect to implementing solutions, Keras has relative drawbacks when \\ncompared to TensorFlow. The broadest explanation is that Keras \\nusers have considerably less control over their computational graph \\nthan TensorFlow users. You work within the confines of a sandbox \\nwhen using Keras. TensorFlow is better at natively supporting more \\ncomplex operations, and providing access to the most cutting-edge \\nimplementations of various algorithms.\\n Theano\\nAlthough it is not covered in this book, it is important in the progression \\nof deep learning to discuss Theano. The library is similar to TensorFlow \\nin that it provides developers with various computational functions (add, \\nmatrix multiplication, subtract, etc.) that are embedded in tensors when \\nbuilding deep learning and machine learning models. For example, the \\nfollowing is a sample Theano code.\\n(code redacted please see github)\\nX, Y = T.fmatrix(), T.vector(dtype=theano.config.floatX)\\n    weights = init_weights(weight_shape)\\n    biases = init_biases(bias_shape)\\n    predicted_y = T.argmax(model(X, weights, biases), axis=1)\\n    cost = T.mean(T.nnet.categorical_crossentropy(predicted_y, Y))\\n    gradient = T.grad(cost=cost, wrt=weights)\\n    update = [[weights, weights  - gradient * 0.05]]Chapter 1  What Is Natural laNguage proCessINg?\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 19}, page_content=\"9    train = theano.function(inputs=[X, Y], outputs=cost, \\nupdates=update, allow_input_downcast=True)\\n    predict = theano.function(inputs=[X], outputs=predicted_y, \\nallow_input_downcast=True)\\n    for i in range(0, 10):\\n        print(predict(test_x_data[i:i+1]))\\nif __name__ == '__main__':\\n    model_predict()\\nWhen looking at the functions defined in this sample, notice that T is \\nthe variable defined for a tensor, an important concept that you should \\nbe familiar with. Tensors can be thought of as objects that are similar \\nto vectors; however, they are distinct in that they are often represented \\nby arrays of numbers, or functions, which are governed by specific \\ntransformation rules unique unto themselves. Tensors can specifically be \\na single point or a collection of points in space-time (any function/model \\nthat combines x, y, and z axes plus a dimension of time), or they may be a \\ndefined over a continuum, which is a tensor field . Theano and TensorFlow \\nuse tensors to perform most of the mathematical operations as data is \\npassed through a computational graph, colloquially known as a model .\\nIt is generally suggested that if you do not know Theano, you should \\nfocus on mastering TensorFlow and Keras. Those that are familiar with \\nthe Theano framework, however, may feel free to rewrite the existing \\nTensorFlow code in Theano.Chapter 1  What Is Natural laNguage proCessINg?\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 20}, page_content='10 Applications of\\xa0Deep Learning to\\xa0NLP\\nThis section discusses the applications of deep learning to NLP .\\n Introduction to\\xa0NLP Techniques and\\xa0Document \\nClassification\\nIn Chapter 3, we walk through some introductory techniques, such as \\nword tokenization, cleaning text data, term frequency, inverse document \\nfrequency, and more. We will apply these techniques during the course \\nof our data preprocessing as we prepare our data sets for some of the \\nalgorithms reviewed in Chapter 2. Specifically, we focus on classification \\ntasks and review the relative benefits of different feature extraction \\ntechniques when applied to document classification tasks.\\n Topic Modeling\\nIn Chapter 4, we discuss more advanced uses of deep learning, machine \\nlearning, and NLP .\\xa0We start with topic modeling and how to perform it via \\nlatent Dirichlet allocation, as well as non-negative matrix factorization. \\nTopic modeling is simply the process of extracting topics from documents. \\nYou can use these topics for exploratory purposes via data visualization or \\nas a preprocessing step when labeling data.\\n Word Embeddings\\nWord embeddings are a collection of models/techniques for mapping \\nwords (or phrases) to vector space, such that they appear in a high-  \\ndimension al field. From this, you can determine the degree of similarity, \\nor dissimilarity, between one word (or phrase, or document) and another. \\nWhen we project the word vectors into a high-dimensional space, we can \\nenvision that it appears as something like what’s shown in Figure\\xa0 1-3.Chapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 21}, page_content='11Ultimately, how you utilize word embeddings is up to your own \\ninterpretation. They can be modified for applications such as spell check, \\nbut can also be used for sentiment analysis, specifically when assessing \\nlarger entities, such as sentences or documents in respect to one another. \\nWe focus simply on how to train the algorithms and how to prepare data to \\ntrain the embeddings themselves.\\n Language Modeling Tasks Involving RNNs\\nIn Chapter 5, we end the book by tackling some of the more advanced NLP \\napplications, which is after you have been familiarized with preprocessing \\ntext data from various format and training different algorithms. \\nSpecifically, we focus on training RNNs to perform tasks such as name \\nentity recognition, answering questions, language generation, and \\ntranslating phrases from one language to another.walked\\nwalkingswam\\nswimming\\nVerb tense\\nFigure 1-3.  Visualization of word embeddingsChapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 22}, page_content='12 Summary\\nThe purpose of this book is to familiarize you with the field of natural \\nlanguage processing and then progress to examples in which you \\ncan apply this knowledge. This book covers machine learning where \\nnecessary, although it is assumed that you have already used machine \\nlearning models in a practical setting prior.\\nWhile this book is not intended to be exhaustive nor overly academic, \\nit is my intention to sufficiently cover the material such that readers are \\nable to process more advanced texts more easily than prior to reading \\nit. For those who are more interested in the tangible applications of NLP \\nas the field stands today, it is the vast majority of what is discussed and \\nshown in examples. Without further ado, let’s begin our review of machine \\nlearning, specifically as it relates to the models used in this book.Chapter 1  What Is Natural laNguage proCessINg?'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 23}, page_content='13© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_2CHAPTER 2\\nReview of\\xa0Deep \\nLearning\\nYou should be aware that we use deep learning and machine learning \\nmethods throughout this chapter. Although the chapter does not provide \\na comprehensive review of ML/DL, it is critical to discuss a few neural \\nnetwork models because we will be applying them later. This chapter also \\nbriefly familiarizes you with TensorFlow, which is one of the frameworks \\nutilized during the course of the book. All examples in this chapter use toy \\nnumerical data sets, as it would be difficult to both review neural networks \\nand learn to work with text data at the same time.\\nAgain, the purpose of these toy problems is to focus on learning how \\nto create a TensorFlow model, not to create a deployable solution. Moving \\nforward from this chapter, all examples focus on these models with text data.\\n Multilayer Perceptrons and\\xa0Recurrent \\nNeural Networks\\nTraditional neural network models, often referred to as multilayer \\nperceptron models  (MLPs), succeed single-layer perceptron models  (SLPs). \\nMLPs were created to overcome the largest shortcoming of the SLP model, \\nwhich was the inability to effectively handle data that is not linearly \\nseparable. In practical cases, we often observe that multivariate data is'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 24}, page_content='14non-linear, rendering the SLP null and void. MLPs are able to overcome \\nthis shortcoming—specifically because MLPs have multiple layers. We’ll \\ngo over this detail and more in depth while walking through some code to \\nmake the example more intuitive. However, let’s begin by looking at the \\nMLP visualization shown in Figure\\xa0 2-1.\\nEach layer of an MLP model is connected by weights, all of which are \\ninitialized randomly from a standard normal distribution. The input layer \\nhas a set number of nodes, each representative of a feature within a neural \\nnetwork. The number of hidden layers can vary, but each of them typically \\nhas the same number of nodes, which the user specifies. In regression, the \\noutput layer has one node. In classification, it has K nodes, where K is the \\nnumber of classes.\\nNext, let’s have an in-depth discussion on how an MLP works and \\ncomplete an example in TensorFlow.\\nFigure 2-1.  Multilayer perceptronChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 25}, page_content='15 Toy Example 1: Modeling Stock Returns \\nwith\\xa0the\\xa0MLP Model\\nLet’s imagine that we are trying to predict Ford Motor Company (F) \\nstock returns given the returns of other stocks on the same day. This is a \\nregression problem, given that we are trying to predict a continuous value. \\nLet’s begin by defining an mlp_model  function with arguments that will be \\nused later, as follows:\\ndef mlp_model(train_data=train_data, learning_rate=0.01, \\niters=100, num_hidden1=256):\\nThis Python function contains all the TensorFlow code that forms the \\nbody of the neural network. In addition to defining the graph, this function \\ninvokes the TensorFlow session that trains the network and makes \\npredictions. We’ll begin by walking through the function, line by line, while \\ntying the code back to the theory behind the model.\\nFirst, let’s address the arguments in our function: train_data  is the \\nvariable that contains our training data; in this example; it is the returns of \\nspecific stocks over a given period of time. The following is the header of \\nour data set:\\n0  0.002647 -0.001609   0.012800   0.000323   0.016132 -0.004664 \\n-0.018598\\n1  0.000704   0.000664   0.023697 -0.006137 -0.004840   0.003555 \\n-0.006664\\n2  0.004221   0.003600   0.002469 -0.010400 -0.008755 -0.002737    \\n0.025367\\n3  0.003328   0.001605   0.050493   0.006897   0.010206   0.002260 \\n-0.007156\\n4  0.001397   0.004052 -0.009965 -0.012720 -0.019235 -0.002255    \\n0.017916Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 26}, page_content='165 -0.009326 -0.003754 -0.014506 -0.006607 -0.034865   0.011463    \\n0.003844\\n6  0.008446   0.005747   0.022830   0.009312   0.021757 -0.000319    \\n0.023982\\n7  0.002705   0.002623   0.007636   0.020099 -0.007433 -0.008303  \\n-0.004330\\n8 -0.011224 -0.009530 -0.008161 -0.003230 -0.015381 -0.003381  \\n-0.010674\\n9  0.012496   0.010942   0.016750   0.007777   0.001233   0.008724    \\n0.033367\\nEach of the columns r epresent the percentage return of a stock on a \\ngiven day, with our training set containing 1180 observations and our test \\nset containing 582 observations.\\nMoving forward, we come to the learning rate and activation function. \\nIn machine learning literature, the learning rate is often represented by the \\nsymbol η (eta). The learning rate is a scalar value that controls the degree \\nto which the gradient is updated to the parameter that we wish to change. \\nWe can exemplify this technique when referring to the gradient descent \\nupdate method. Let’s first look at the equation, and then we can break it \\ndown iteratively.\\n qq hq tt iN i i\\nNhx y+==- ()- () 112 1S (2.1)\\n qq hqq q tt ii i i\\nNhx yh x+==- ()- () Ñ()1112S \\nIn Equation 2.1, we are updating some parameter, θ, at a given \\ntime step, t. hθ(x)i is equal to the hypothesized label/value, y being the \\nactual label/value, in addition to N being equal to the total number of \\nobservations in the data set we are training on.\\n∇θhθ(x)i is the gradient of the output with respect to the parameters of \\nthe model.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 27}, page_content=\"17Each unit in a neural network (with the exception of the input layer) \\nreceives the weighted sum of inputs multiplied by weights, all of which are \\nsummed with a bias. Mathematically, this can be described in Equation 2.2.\\n yf xw bT=() + , (2.2)\\nIn neural networks, the parameters are the weights and biases. When \\nreferring to Figure\\xa0 2-1, the weights are the lines that connect the units in \\na layer to one another and are typically initialized by randomly sampling \\nfrom a normal distribution. The following is the code where this occurs:\\n     weights = {'input': tf.Variable(tf.random_normal([train_x.\\nshape[1], num_hidden])),\\n             'hidden1': tf.Variable(tf.random_normal([num_\\nhidden, num_hidden])),\\n             'output': tf.Variable(tf.random_normal([num_hidden, \\n1]))}\\n     biases = {'input': tf.Variable(tf.random_normal([num_\\nhidden])),\\n             'hidden1': tf.Variable(tf.random_normal([num_\\nhidden])),\\n            'output': tf.Variable(tf.random_normal([1]))}\\nBecause they are part of the computational graph, weights and biases \\nin TensorFlow must be initialized as TensorFlow variables with the tf.\\nVariable() . TensorFlow thankfully has a function that generates numbers \\nrandomly from a normal distribution called tf.random_normal() , which \\ntakes an array as an argument that specifies the shape of the matrix that you \\nare creating. For people who are new to creating neural networks, choosing \\nthe proper dimensions for the weight and bias units is a typical source of \\nfrustration. The following are some quick pointers to keep in mind :\\n• When r eferring to the weights, the columns of a given \\nlayer need to match the rows of the next layer.Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 28}, page_content='18• The columns of the wei ghts for every layer must match \\nthe number of units for each layer in the biases.\\n• The o utput layer columns for the weights dictionary \\n(and array shape for the bias dictionary) should be \\nrepresentative of the problem that you are modeling.  \\n(If regression, 1; if classification, N, where N = the \\nnumber of classes).\\nYou might be curious as to why we initialize the weights and biases \\nrandomly. This leads us to one of the key components of neural networks’ \\nsuccess. The easiest explanation is to imagine the following two scenarios:\\n• All weig hts are initialized to 1 . If all the weights are \\ninitialized to 1, every neuron is passed the same value, \\nequal to the weighted sum, plus the bias, and then put \\ninto some activation function, whatever this value may be.\\n• All weig hts are initialized to 0 . Similar to the prior \\nscenario, all the neurons are passed the same value, \\nexcept that this time, the value is definitely zero.\\nThe more general problem associated with weights that are initialized \\nat the same location is that it makes the network susceptible to getting \\nstuck in\\xa0local minima. Let’s imagine an error function, such as the one \\nshown in Figure\\xa0 2-2.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 29}, page_content='19Imagine that when we initialize the neural network weights at 0, and \\nsubsequently that when it calculates the error, it yields the value at the Y \\nvariable in Figure\\xa0 2-2. The gradient descent algorithm always gives the \\nsame update for the weights from the first iteration of the algorithm, and \\nit likely gives the same value moving forward. Because of that, we are not \\ntaking advantage of the ability of neural networks to start from any point in \\nthe solution space. This effectively removes the stochastic nature of neural \\nnetworks, and considerably reduces the probability of reaching the best \\npossible solution for the weight optimization problem. Let’s discuss the \\nlearning rate.\\nFigure 2-2.  Error plotChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 30}, page_content='20 Learning Rate\\nThe learning rate is typically a static floating-point value that determines \\nthe degree to which the gradient, or error, affects the update to the \\nparameter that you seek to optimize. In example problems, it is common to \\nsee learning rates initialized anywhere from 0.01 to 1e–4. The initialization \\nof the learning rate parameter is another point worth mentioning, as it can \\naffect the speed at which the algorithm converges upon a solution. Briefly, \\nthe following are two scenarios that are problematic:\\n• The learning rate is too large.  When the learning rate \\nis too large, the error rate moves around in an erratic \\nfashion. Typically, we observe that the algorithm on one \\niteration seems to find a better solution than the prior \\none, only to get worse upon the next, and oscillating \\nbetween these two bounds. In a worst-case scenario, \\nwe eventually start to receive NaN values for error rates, \\nand all solutions effectively become defunct. This is the \\nexploding gradient problem, which I discuss later.\\n• The learning rate is too small.  Although, over time, \\nthis does not yield an incorrect, ultimately, we spend \\nan inordinate amount of time waiting for the solution \\nto converge upon an optimal solution.\\nThe optimal solution is to pick a learning rate that is large enough \\nto minimize the number of iterations needed to converge upon an \\noptimal solution, while not being so large that it passes this solution in \\nits optimization path. Some solutions, such as adaptive learning rate \\nalgorithms, solve the problem of having to grid search or iteratively \\nuse different learning rates. The mlp_model()  function uses the Adam \\n(ada ptive moment estimation) optimization algorithm, which updates the \\nlearning rate aw we learn. I briefly discuss how this algorithm works, and \\nwhy you should use it to expedite learning rate optimization.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 31}, page_content='21Adam was first described in a paper that written by Diederik Kingma \\nand Jimmy Lei Ba. Adam specifically seeks to optimize learning rates by \\nestimating the first and second moments of the gradients. For those who \\nare unfamiliar, moments  are defined as specific measures of the shape of a \\nset of points. As it relates to statistics, these points are typically the values \\nwithin a probability distribution. We can define the zeroth moment as the \\ntotal probability; the first moment as the mean; and second moment as the \\nvariance. In this paper, they describe the optimal parameters for Adam, in \\naddition to some initial assumptions, as follows:\\n• α\\xa0=\\xa0Stepsize; α\\xa0≔\\xa00.001,  \\xa0ϵ\\xa0=\\xa010−8\\n• β1, β2\\xa0=\\xa0Exponential decay rates for\\xa01st and 2nd\\xa0moment \\nestimateions\\xa0 β1\\xa0=\\xa00.9,  β2\\xa0=\\xa00.999; β1, β2\\xa0∈\\xa0[0, 1)\\n• f(θ)\\xa0=\\xa0Stochastic objective function that \\nwe\\xa0are\\xa0optimizing with parameters\\xa0 θ\\n• m\\xa0=\\xa01st\\xa0moment vector,  v\\xa0=\\xa02nd\\xa0moment vector\\xa0(Both \\ninitialized\\xa0as\\xa00s)\\nWith this in mind, although we have not converged upon an optimal \\nsolution, the following is the algorithm that we use:\\n• gt\\xa0=\\xa0∇θft(θt\\xa0−\\xa01)\\n• ˆˆ,mv=-Bias corrected first and second moment estimates  \\nresppect ively;\\n• mm gv vgtt tt tt ::==bb bb11 12 12211 *+ -() ** +-() *--\\n• ˆ:, : mmvv\\ntt\\nt tt\\nt==1112--bb\\uf0b5\\n• qq attt\\ntm\\nv:= -*-1ˆ\\n() +\\uf0b7\\uf0f2\\nWhile the preceding formulae describe Adam when optimizing \\none parameter, we can extrapolate the formulae to adjust for multiple \\nparameters (as is the case with multivariate problems). In the paper, Adam Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 32}, page_content='22outperformed other standard optimization techniques and was seen as the \\ndefault learning rate optimization algorithm.\\nAs for the final parameters, num_hidden  refers to the number of units in \\nthe hidden layer(s). A commonly referenced rule of thumb is to make this \\nnumber equal to the number of inputs plus the number of outputs, and \\nthen multiplied by 2/3.\\nEpochs  refers to the number of times the algorithm should iterate \\nthrough the entirety of the training set. Given that this is situation \\ndependent, there is no general suggestible number of epochs that a neural \\nnetwork should be trained. However, a suggestible method is to pick an \\narbitrarily large number (1500, for example), plot the training error, and \\nthen observe which number of epochs is sufficient. If needed, you can \\nenlarge the upper limit to allow the model to optimize its solution further.\\nNow that I have finished discussing the parameters, let’s walk through \\nthe architecture, code, and mathematics of the multilayer perceptron, as \\nfollows:\\n#Creating training and test sets\\ntrain_x, train_y = train_data[0:int(len(train_data)*.67), \\n1:train_data.shape[1]], train_data[0:int(len(train_data)*.67), 0]\\ntest_x, test_y = train_data[int(len(train_data)*.67):, 1:train_\\ndata.shape[1]], train_data[int(len(train_data)*.67):, 0]\\nObserve that we are creating both a training set and a test set. The \\ntraining and test sets contain 67% and 33%, respectively, of the original data \\nset labeled train_data . It is suggested that machine learning problems \\nhave these two data sets, at a minimum. It is optional to create a validation \\nset as well, but this step is omitted for the sake of brevity in this example.\\nNext, let’s discuss the following important aspect of working with \\nTensorFlow:\\n#Creating placeholder values and instantiating weights and \\nbiases as dictionariesChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 33}, page_content=\"23X = tf.placeholder('float', shape = (None, 7))\\nY = tf.placeholder('float', shape = (None, 1))\\nWhen working in TensorFlow, it is important to refer to machine \\nlearning models as graphs , since we are creating computational graphs \\nwith different tensor objects. Any typical deep learning or machine \\nlearning model expects an explanatory and response variable; however, \\nwe need to specify what these are. Since they are not a part of the graph, \\nbut are representational objects that we are passing data through, they are \\ndefined as placeholder variables , which we can access from TensorFlow \\n(imported as tf) by using tf.placeholder() . The three arguments for this \\nfunction are dtype (data type), shape, and name. dtype and shape are the \\nonly required arguments. The following are quick rules of thumb:\\n• Generally, the shape of the X and Y variables should \\nbe initialized as a tuple. When working with a two-  \\ndimensional data set, the shape of the X variable \\nshould be (none, number of features), and the shape \\nof the Y variable should be (none, [1 if regression, N if \\nclassification]), where N is the number of classes.\\n• The data type specified for these placeholders should \\nreflect the values that you are passing through them. \\nIn this instance, we are passing through a matrix of \\nfloating-point values and predicting a floating-point \\nvalue, so both placeholders for the response and \\nexplanatory variables have the float data type. In \\nthe instance that this was a classification problem, \\nassuming the same data passed through the \\nexplanatory variable, the response variable has the int \\ndata type since the labels for the classes are integers.Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 34}, page_content=\"24Since I discussed the weights in the neural network already, let’s get \\nto the heart of the neural network structure: the input through the output \\nlayers, as shown in the following code (inside mlp_model()  function):\\n#Passing data through input, hidden, and output layers\\ninput_layer = tf.add(tf.matmul(X, weights['input']), \\nbiases['input']) (1)\\ninput_layer = tf.nn.sigmoid(input_layer) (2)\\ninput_layer = tf.nn.dropout(input_layer, 0.20) (3)\\nhidden_layer = tf.add(tf.multiply(input_layer, \\nweights['hidden1']), biases['hidden1'])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nhidden_layer = tf.nn.dropout(hidden_layer, 0.20)\\noutput_layer = tf.add(tf.multiply(hidden_layer, weights \\n['output']),biases['output']) (4)\\nWhen looking at the first line of highlighted code (1), we see the input \\nlayer operation. Mathematically, operations from one neural network layer \\nto the next can be represented by the following equation:\\n layerf Xw biaskk kT\\nk =*() + ()  (2.2.1)\\nf(x) is equal to some activation function. The output from this \\noperation is passed to the next layer, where the same operation is run, \\nincluding any operations placed between layers. In TensorFlow, there are \\nbuilt-in mathematical operations to represent the preceding equation:  \\ntf.add()  and tf.matmul() .\\nAfter we create the output, which in this instance is a matrix of \\nshape (1, 256), we pass it to an activation function. In the second line of \\nhighlighted code (2), we first pass the weighted sum of the inputs and bias \\nto a sigmoid activation function, given in Equation 2.3.\\n s=+æ\\nèçö\\nø÷ -1\\n1ex (2.3)Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 35}, page_content='25e is the exponential function. Activation functions serve as a way to \\nscale the outputs from Equation 2.2, and are sometimes directly related to \\nhow we classify outputs. More importantly, this is the core component of \\nthe neural network that introduces non-linearity to the learning process. \\nSimply stated, if we use a linear activation function, where f(x) = x, we are \\nsimply repetitively passing the outputs of a linear function from the input \\nlayer to the output layer. Figure\\xa0 2-3 illustrates this activation function.\\nAlthough the range here is from –6 to 6, the function essentially looks \\nlike −∞  to ∞, in that there are asymptotes at 0 and 1 as X grows infinitely \\nlarger or infinitely smaller, respectively. This function is one of the more \\ncommon activation functions utilized in neural networks, which we use in \\nthe first layer.-6 -5 -4 -3 -2 -1 01 23 45 61.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0Activation Function Ouptut\\nX ValueSigmoid Activation Function\\nFigure 2-3.  Sigmoid activation functionChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 36}, page_content='26Also, we defined the derivative of this function, which is important \\nin mathematically explaining the vanishing gradient problem (discussed \\nlater in the chapter). Although going through all the activation functions \\nin neural networks would be exhaustive, it is worth discussing the other \\nactivation function that this neural network utilizes. The hidden layer uses a \\nReLU activation function, which is mathematically defined in Equation 2.4.\\n ReLU xx() = () max0,  (2.4)\\nThe function is illustrated in Figure\\xa0 2-4.\\nFigure 2-4.  ReLU activation functionChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 37}, page_content='27Both mathematically and visually, the ReLU activation function is \\nsimple. The output of a ReLU is a matrix of 0s, with some positive values. \\nOne of the major benefits of the ReLU activation function lies in the fact \\nthat it produces a sparse matrix as its output. This attribute is ultimately \\nwhy I have decided to include it as the activation function in the hidden \\nlayer, particularly as it relates to the vanishing gradient problem.\\n Vanishing Gradients and\\xa0Why ReLU Helps \\nto\\xa0Prevent Them\\nThe vanishing gradient problem is specific to the training of neural \\nnetworks, and it is part of the improvements that researchers sought to \\nmake with LSTM over RNN (both are discussed later in this chapter). The \\nvanishing gradient problem is a phenomenon observed when the gradient \\ngets so small that the updates to weights from one iteration to the next \\neither stops completely or is considerably negligible.\\nLogically, what proceeds is a situation in which the neural network \\neffectively stops training. In most cases, this results in poor weight \\noptimization, and ultimately, bad training and test set performance. Why \\nthis happens can be explained precisely by how the updates for each of the \\nweights are calculated:\\nWhen we look at Figure\\xa0 2-3, we see the derivative of the sigmoid \\nfunction. The majority of the function’s derivate falls in a narrow range, \\nwith most of the values being close to 0. When considering how to \\ncalculate the gradient of differing hidden layers, this is precisely what \\ncauses a problem as our network gets deeper. Mathematically, this is \\nrepresented by the following equation:\\n ¶\\n¶=¶\\n¶¶\\n¶¶\\n¶¶\\n¶=åE\\nWE\\nyy\\nss\\nss\\nWk kk 3\\n02\\n3\\n33\\n33\\n\\uf0b5 Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 38}, page_content='28As you can see, when we backpropagate the error to layer k, which in \\nthis example is 0 (the input layer), we are multiplying several derivatives of \\nthe activation function’s output several times. This is a brief explanation of \\nthe chain rule and underlies most of a neural networks’ backpropagation \\ntraining algorithm. The chain rule is a formula that specifies how to calculate \\na derivative that is composed of two or more functions. Assume that we have \\na two-layer neural network. Let’s also assume that our respective gradients \\nare 0.001 and 0.002. This yields 2 e–6 as a respective gradient of the output \\nlayer. Our update to the next gradient would be described as negligible.\\nYou should know that any activation function that yields non-  sparse \\noutputs, particularly when used for multiple layers in succession, \\ntypically causes vanishing gradients. We are able to substantially mitigate \\nthis problem by using a combination of sparse and non-sparse output \\nactivation functions, or exclusively utilize non-spare activation functions. \\nWe illustrate an example of such a neural network in the mlp_model()  \\nfunction. For now, however, let’s take a look at one last activation layer \\nbefore we finish analyzing this MLP .\\nObserve that after every activation layer, we use the dropout layer , invoked \\nby tf.nn.dropout() . Dropout layers have the same dimensions as the layer \\npreceding them; however, they arbitrarily set a random selection of weights’ \\nvalues to 0, effectively “shutting off” the neurons that are connected to them. \\nIn every iteration, there are a different set of random neurons that shut off. \\nThe benefit of using dropout is to prevent overfitting, which is the instance in \\nwhich a model performs well in training data but poorly in test data.\\nThere are a multitude of factors that can cause overfitting, including (but \\nnot limited to) not having enough training data or not cross-  validating data \\n(which induces a model to memorize idiosyncrasies of a given orientation of \\na data set rather than generalizing to the distribution underlying the data). \\nAlthough you should solve issues like these first, adding dropout is not a bad \\nidea. When you execute functions without dropout, you notice overfitting \\nrelative to the models that do contain dropout.\\nLet’s discuss some final MLP topics—specifically, the key components \\nto what causes the model to learn.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 39}, page_content='29 Loss Functions and\\xa0Backpropagation\\nLoss functions are specifically how we define the degree to which our \\nmodel was incorrect. In regression, the most typical choices are mean \\nsquared error  (MSE) or root mean squared error  (RMSE). Mathematically, \\nthey are defined as follows:\\n MSENhx y\\niN\\nii= ()- ()\\n=å1\\n12\\nq (2.5)\\n RMSENhx y\\niN\\nii= ()- ()\\n=å1\\n12\\nq (2.6)\\nerror = tf.reduce_mean(tf.pow(output_layer\\xa0– Y,2)) (mean squared error \\nin code)\\nIntuitively, MSE (see Equation 2.5) provides a method for assessing \\nwhat was the average error over all predictions in a given epoch. RMSE \\n(see Equation 2.6) provides the same statistic, but takes the square root \\nof the MSE value. The benefit of RMSE is that it provides a statistic in \\nthe same unit as the predicted value, allowing the user to assess the \\nperformance of the model more precisely. MSE does not have this benefit, \\nand as such, it becomes less interpretable—except in the sense that a lower \\nMSE from one epoch to the next is desirable.\\nAs an example, if we are predicting money, what does it mean that our \\nprediction is $0.30 squared inaccurate? While we can tell that we have a \\nbetter solution if the next epoch yields an MSE of $0.10, it is much harder \\nto tell precisely what an MSE of $0.10 translates to in a given prediction. \\nWe compare the results of using RMSE vs. MSE in the final toy example in \\nthe chapter. In natural language processing, however, we more often deal \\nwith error functions reserved for classification tasks. With that in mind, \\nyou should be accustomed to the following formulas.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 40}, page_content='30The binary cross entropy is\\n \\uf04cyh yp ypxi,q() () =- ()+-() -() logl og ) 11 (2.7)\\nThe multiclass cross entropy is\\n \\uf04cyh ssxijyi,,q() () =- + () max0 D (2.8)\\nCross entropy  is the number of bits needed to identify an event drawn \\nfrom a set. The same principles (with respect to training using an MSE \\nor RMSE loss function) are carried when using a cross-entropy-based \\nloss function. Our objective is to optimize the weights in a direction that \\nminimizes the error as much as possible.\\nAt this point, we have walked through the MLP from the initialization \\nof the parameters, what they mean, how the layer moves from each layer, \\nwhat the activation functions do to it, and how the error is calculated. Next, \\nlet’s dig into recurrent neural networks, long short-term memory, and their \\nrelative importance in the field of natural language processing.\\n Recurrent Neural Networks and\\xa0Long Short-Term \\nMemory\\nDespite the relative robustness of MLPs, they have their limitations. The \\nmodel assumes independence between inputs and outputs, making \\nit a suboptimal choice for problems in which the output of function is \\nstatistically dependent on the preceding inputs. As this relates to natural \\nlanguage processing (NLP), there are tasks that MLPs might be particularly \\nuseful for, such as sentiment analysis. In these problems, one body of text \\nbeing classified as negative is not dependent on assessing the sentiment of \\na separate body of text.\\nAs an example, I wouldn’t need to read multiple restaurant reviews to \\ndetermine whether an individual review is positive or negative. It can be \\ndetermined by the attributes of a given observation. However, this is not Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 41}, page_content='31always the type of NLP problem we encounter. For example, let’s assume \\nthat we are trying to spell-check on the following sentences:\\n“I am happy that we are going too the mall!”\\n“I am happy to. That class was excellent. ”\\nBoth sentences are incorrect in their usage of the words too and to, \\nrespectively, because of the context in which they appear. We must use the \\nsequence of words prior, and perhaps even the words after, to determine \\nwhat is incorrect. Another similar problem would be predicting words in a \\nsentence; for example, let’s look at the following sentence.\\n“I was born in Germany. I speak _______. ”\\nAlthough there isn’t necessarily one answer to complete this sentence, \\nas being born in Germany does not predetermine someone to speaking \\nonly German, there is a high probability that the missing word is German . \\nHowever, we can only say that because of the context that surrounds the \\nwords, and assuming that the neural network was trained on sentences (or \\nphrases) and has a similar structure. Regardless, these types of problems \\ncall for a model that can accommodate some sort of memory related to the \\nprior inputs, which brings us to recurrent neural network. Figure\\xa0 2-5 shows \\nthe structure of an RNN.\\no\\nVWW\\nW\\nUV\\nU\\nxs\\nUnfoldot-1\\nxt-1st-1\\nWW\\nUVot +1\\nxt +1st +1\\nUVot\\nxtst\\nFigure 2-5.  Recurrent neural networkChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 42}, page_content='32It is important to examine the structure of the RNN as it relates to \\nresolving the statistical dependency problem. Similar to the prior example, \\nlet’s walk through some example code in TensorFlow to illustrate the \\nmodel structure using a toy problem. Similar to the MLP , we will work with \\na toy problem to create a function that loads and preprocesses our data for \\nthe neural network, and then make a function to build our neural network. \\nThe following is the beginning of the function:\\ndef build_rnn(learning_rate=0.02, epochs=100, state_size=4):\\nThe first two arguments should be familiar. They represent the same \\nconcepts as in the MLP example. However, we have a new argument called \\nstate_size . In a vanilla RNN, the model we are building here, we pass \\nwhat is called the hidden state  from a given time step forward. The hidden \\nstate is similar to the hidden layer of an MLP in that it is a function of the \\nhidden states at previous time steps. The following defines the hidden state \\nand output as\\n hf Wx Wh btx ht hh th =+ + ()-1  (2.9)\\n yW hbth ot o =+  (2.10)\\nht is the hidden state, W is the weight matrix, b is the bias array, y is the \\noutput of the function, and f(x) is the activation function of our choosing.\\n Toy Example 2: Modeling Stock Returns \\nwith\\xa0the\\xa0RNN Model\\nUsing the code in the build_rnn()  function, observe the following.\\n#Loading data\\n     x, y = load_data(); scaler = MinMaxScaler(feature_range=(0, 1))\\n    x, y = scaler.fit_transform(x), scaler.fit_transform(y)Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 43}, page_content=\"33     train_x, train_y = x[0:int(math.floor(len(x)*.67)),   :], \\ny[0:int(math.floor(len(y)*.67))]\\n    #Creating weights and biases dictionaries\\n     weights = {'input': tf.Variable(tf.random_normal([state_\\nsize+1, state_size])),\\n         'output': tf.Variable(tf.random_normal([state_size, \\ntrain_y.shape[1]]))}\\n     biases = {'input': tf.Variable(tf.random_normal([1, state_\\nsize])),\\n         'output': tf.Variable(tf.random_normal([1, train_y.\\nshape[1]]))}\\nWe b egin by loading the training and test data, performing a similar \\nsplit in the test set such that the first 67% of the complete data set becomes \\nthe training set and the remaining 33% becomes the test set. In this \\ninstance, we distinguish between two classes, 0 or 1, indicating whether \\nthe price went up or down. Moving forward, however, we must refer back \\nto the state size parameter to understand the shape of the matrices we \\nproduce, again as TensorFlow variables, for the weight and bias matrices.\\nTo crystallize your understanding of the state size parameter, refer to \\nFigure\\xa0 2-5, in which the center of the neural network represents a state. We \\nmultiply the given input, as well as the previous state, by a weight matrix, \\nand sum all of this with the bias. Similar to the MLP , the weighted sum \\nvalue forms the input for the activation function.\\nThe output of the activation function forms the hidden state at time \\nstep t, whose value becomes part of the weighted sum in Equation 2.10 . \\nThe value of this matrix application ultimately forms the output for the \\nRNN.\\xa0We repeat these operations for as many states that we have, which \\nis equal to the number of inputs that we pass through the neural network. \\nWhen referring back to the image, this is what is meant by the RNN being \\n“unfolded. ” The state_size  in our example is set to 4, meaning that we are \\ninputting four input sequences before we make a prediction.Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 44}, page_content=\"34Let’s now walk through the TensorFlow code associated with these \\noperations.\\n#Defining placeholders and variables\\n    X = tf.placeholder(tf.float32, [batch_size, train_x.shape[1]])\\n    Y = tf.placeholder(tf.int32, [batch_size, train_y.shape[1]])\\n     init_state = tf.placeholder(tf.float32, [batch_size, state_\\nsize])\\n    input_series = tf.unstack(X, axis=1)\\n    labels = tf.unstack(Y, axis=1)\\n    current_state = init_state\\n    hidden_states = []\\n    #Passing values from one hidden state to the next\\n     for input in input_series: #Evaluating each input within \\nthe series of inputs\\n         input = tf.reshape(input, [batch_size, 1]) #Reshaping \\ninput into MxN tensor\\n         input_state = tf.concat([input, current_state], axis=1) \\n#Concatenating input and current state tensors\\n         _hidden_state = tf.tanh(tf.add(tf.matmul(input_\\nstate, weights['input']), biases['input'])) #Tanh \\ntransformation\\n         hidden_states.append(_hidden_state) #Appending the next \\nstate\\n        current_state = _hidden_state #Updating the current state\\nSimilar to the MLP model, we need to define place holder variables for \\nboth the x and y tensors that our data will pass through. However, a new \\nplaceholder will be here, which is the init_state , representing the initial \\nstate matrix. Notice that the current state is the init_state  placeholder for \\nthe first iteration through the next. It also holds the same dimensions and \\nexpects the same data type.Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 45}, page_content=\"35Moving forward, we iterate through every input_sequence  in the data set, \\nwhere _hidden_state  is the Python definition of formula (see Equation 2.9). \\nFinally, we must come to the output state, given by the following:\\nlogits = [tf.add(tf.matmul(state, weights['output']), \\nbiases['output']) for state in hidden_states]\\nThe code here is representative of Equation 2.10 . However, this will \\nonly give us a floating-point decimal, which we need to convert into a label \\nsomehow. This brings us to an activation function which will be important \\nto remember for multiclass classification, and therefore for the remainder \\nof this text, the softmax activation function. Subsequently, we define this \\nactivation function as the following:\\n Sye\\neiy\\niNyi\\ni () =æ\\nèç\\nçö\\nø÷\\n÷\\n=å1  (2.11)\\nWhen you look at the formula, we are summing some value over all \\nthe possible values. As such, we define this as a probability score. When \\nrelating this back to classification, particularly with the RNN, we are \\noutputting the relative probability of an observation being of one class vs \\nanother (or others). The label we choose in this instance is the one with the \\nhighest relative score, meaning that we choose a given label k because it \\nhas the highest probability of being true based on the model’s prediction. \\nEquation 2.11  is subsequently represented in the code by the following line:\\npredicted_labels = [tf.nn.softmax(logit) for logit in logits] \\n#predictions for each logit within the series\\nBeing that this is a classification problem, we use a cross entropy–\\nbased loss function and for this toy example we will use the gradient \\ndescent algorithm, both of which were elaborated upon in the prior \\nsection MLPs. Invoking the TensorFlow session also is performed in the \\nsame fashion as it would be for the MLP graph (and furthermore for all \\nTensorFlow computational graphs). In a slight derivation from the MLP , Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 46}, page_content='36we calculate errors at each time step of an unrolled network and sum these \\nerrors. This is known as backpropagation through time  (BPTT), which is \\nutilized specifically because the same weight matrix is used for every time \\nstep. As such, the only changing variable besides the input is the hidden \\nstate matrix. As such, we can calculate each time step’s contribution to the \\nerror. We then sum these time step errors to get the error. Mathematically, \\nthis is represented by the following equation:\\n ¶\\n¶=¶\\n¶¶\\n¶¶\\n¶¶\\n¶=åE\\nWE\\nyy\\nss\\nss\\nWk kk 3\\n03\\n3\\n33\\n33\\n\\uf0b5 \\nThis is an application of the chain rule, as described briefly in the \\nsection on how we backpropagate the error from the output layer back to \\nthe input layer to update the weights with respect to their contribution to \\nthe total error. BPPT applies the same logic; instead, we treat the time steps \\nas the layers. However, although RNNs solved many problems of MLPs, \\nthey had relative limitations, which you should be aware of.\\nOne of the largest drawbacks of RNNs is that the vanishing gradient \\nproblem reappears. However, instead of it being due to having very \\ndeep neural network layers, it is caused by trying to evaluate arbitrarily \\nlong sequences. The activation function used in RNNs is often the tanh \\nactivation function. Mathematically, we define this as follows:\\n tanh x()=-\\n+-\\n-ee\\neexx\\nxx  \\nFigure 2-6 illustrates the activation function.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 47}, page_content='37Similar to the problem with the sigmoid activation function, the \\nderivative of the tanh function can 0, such that when backpropagated \\nover large sequences results in a gradient that is equal to 0. Similar to the \\nMLP , this can cause problems with learning. Depending on the choice of \\nactivation function, we also might experience the opposite of the vanishing \\ngradient problem—the exploding gradient. Simply stated, this is the result \\nof the gradients appearing as NaN values. There are couple of solutions for \\nthe vanishing gradient function in RNNs. Among them are to try weight \\nregularization via an L1 or L2 norm, or to try different activation functions \\nas we did in the MLP , utilizing functions such as ReLU.\\xa0However, one of the \\nmore straightforward solutions is to use a model devised in the 1990s by \\nSepp Hochreiter and Jürgen Schmidhuber: the long short-term memory unit, \\nor LSTM. Let’s start with what this model looks like, as shown in Figure\\xa0 2-7.\\nFigure 2-6.  Tanh activation and derivative functionChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 48}, page_content='38LSTMs are distinguished structurally by the fact that we observe them \\nas blocks, or units, rather than the traditional structure a neural network \\noften appears as. That said, the same principles are generally applied here. \\nHowever, we have an improvement over the hidden state from the vanilla \\nRNN.\\xa0I will walk through the formulae associated with the LSTM.\\n iW xW hW cbtx it hith ct i =+ ++ ()-- s11  (2.12)\\n fW xW hW cbtx ft hf th ft f =+ ++ () -- s11  (2.13)\\n cf ci Wx Wh btt tt xc th ct c =+ ++ ()--\\uf06f\\uf06f11 tanh  (2.14)\\n oW xW hW cbtx ot ho tc ot o =+ ++ ()- s1  (2.15)\\n ho ctt t = ()\\uf06ftanh  (2.16)\\nit is the input gate, ft is the forget gate, ct is the cell state, ot is the output \\ngate, htis the output vector, σ is the sigmoid activation function, and tanh is \\nthe tanh activation function. Both the hidden and cell states are initialized \\nat 0 upon initialization of the algorithm.\\nThe formulae from the LSTM is similar to that of the vanilla RNN, \\nhowever there is some slight complexity added. Initially, let’s draw our \\nattention to the diagram, specifically the LSTM unit in the center, and \\nunderstand the directional flow as they relate to the formulae. x\\nxx+\\ntanhtanhx\\nx+\\ntanhx\\nx+\\ntanhx\\nx+\\ntanhtanhx\\nx+\\ntanhtanhAA\\nXt-1ht-1 ht ht+1\\nXt+1 Xt\\nFigure 2-7.  LSTM units/blocksChapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 49}, page_content='39Preliminarily, let’s discuss the notation. Each block, denoted by \\n , \\nrepresents a neural network layer, through which we pass through values. \\nThe horizontal lines with arrows represent the vectors and direction in \\nwhich the data moves. After it moves through a neural network layer, the \\ndata often is passed to a pointwise operation object, represented by \\n .\\nNow that I have discussed how to read the diagram, let’s dive in deeper.\\nLSTMs are distinguished by having gates that regulate the information \\nthat passes through individual units, as well as what information passes to \\nthe next unit. Individually, these are the input gate, the output gate, and \\nthe forget gate. In addition to these three gates, an LSTM also contains a \\ncell, which is an important aspect of the unit.\\nOn the diagram, the cell is represented by the horizontal line, and it \\nis mathematically represented in Equation 2.14 . The cell state is similar \\nto the hidden state, featured here as well as in the RNN, except there is \\ndiscretion as to how much information we pass from one unit to the next. \\nWhen looking at the diagram, an input, xt, is passed through the input \\ngate. Here the neural network is put through a neural network layer, \\nwith a sigmoid activation function that passes the output to a pointwise \\nmultiplication operator. This operation is combined with the forget gate, ft, \\nwhich is the entirety of Equation 2.14 .\\nAbove all, what you should take away from this operation is that its \\noutput is a number between and including 0 and 1. The closer the number \\nis to 1, information is increasingly passed to the subsequent unit. In \\ncontrast, the closer the number is to 0, information is decreasingly passed \\nto the subsequent unit.\\nIn Equation 2.13 , the forget gate, is what regulates this acceptance of \\ninformation, which is represented by ct\\xa0−\\xa01.\\nMoving to Equation 2.15  and relating it to the diagram, this is the \\nneural network layer furthest to the right that is passed through another \\nsigmoid layer, in similar fashion in to the input layer. The output of this \\nsigmoid activated neural network layer is then multiplied with the tanh \\nactivated cell state vector, in Equation 2.16  Finally, we pass both the Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 50}, page_content=\"40cell state vector and the output vector to the next LSTM unit. While I \\ndo not draw out the LSTM in the same fashion as the RNN, I utilize the \\nTensorFlow API’s implementation of the LSTM.\\n Toy Example 3: Modeling Stock Returns \\nwith\\xa0the\\xa0LSTM Model\\nAs was the case in our prior neural network examples, we must still create \\nTensorFlow placeholders and variables. For this example, the LSTM \\nexpects sequences of data, which we facilitate by first creating a three-  \\ndimensional X placeholder variables. To avoid debugging issues when \\ndeploying this API with different data sets, you should be careful to read the \\nfollowing instructions carefully .\\n    X = tf.placeholder(tf.float32, (None, None, train_x.shape[1]))\\n    Y = tf.placeholder(tf.float32, (None, train_y.shape[1]))\\n     weights = {'output': tf.Variable(tf.random_normal([n_\\nhidden, train_y.shape[1]]))}\\n     biases = {'output':  tf.Variable(tf.random_normal([train_y.\\nshape[1]]))}\\n    input_series = tf.reshape(X, [-1, train_x.shape[1]])\\n    input_series = tf.split(input_series, train_x.shape[1], 1)\\n     lstm = rnn.core_rnn_cell.BasicLSTMCell(num_units=n_hidden, \\nforget_bias=1.0, reuse=None, state_is_tuple=True)\\n     _outputs, states = rnn.static_rnn(lstm, input_series, \\ndtype=tf.float32)\\n     predictions = tf.add(tf.matmul(_outputs[-1], \\nweights['output']), biases['output'])\\n     accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax \\n(tf.nn.softmax(predictions), 1)tf.argmax(Y, 1)), dtype=tf.\\n     float32)),Chapter 2  review of\\xa0Deep Learning\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 51}, page_content='41     error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_\\nlogits(labels=Y, logits=predictions))\\n     adam_optimizer = tf.train.AdamOptimizer(learning_rate).\\nminimize(error)\\nWhen creating a sequence of variables, we start by creating a three-  \\ndimensional placeholder named X, which is what we feed our data into. \\nWe transform this variable by creating a two-dimensional vector of the \\nobservations with the tf.reshape() .\\nNext, we create a tensor object for each of these observations with the tf.\\nsplit()  function, which are then stored as a list underneath  input_series .\\nThen, we can create an LSTM cell using the BasicLSTMCell()  function. \\nThe static_rnn()  function accepts any type of RNN cell, so you can \\nutilize other types of RNNs, such as GRUs or vanilla RNNs, and the inputs. \\nEverything else follows the same pattern as the prior examples, in that we \\ncreate TensorFlow variables to calculate accuracy, the error rate, and the \\nAdam optimizer.\\n Summary\\nWe have reached the end of our brief, but necessary review of machine \\nlearning before we dive deeply into tackling problems using these models \\non text data. However, it is important for us to review some key concepts:\\n• Model choice matters!  Understand the data that you \\nare analyzing. Is the label you are predicting dependent \\non other prior observed labels, or are these inputs \\nand outputs statistically independent of one another? \\nFailing to inspect these key properties of your data \\nbeforehand will waste time and provide you with \\nsuboptimal results. Do not skip these steps.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 52}, page_content='42• Parameter choice matters!  Picking the right model \\nfor a problem is the first step, but you have to tune this \\nmodel properly to get optimal results. Inspect model \\nperformance when you alter the number of hidden \\nunits and epochs. I suggest utilizing algorithms such \\nas Adam to tune the learning rate while the network \\nis training. Where possible, grid search or use similar \\nreactive search methods to find better parameters.\\n• Activation functions matter!  Be mindful of how your \\nneural network behaves with respect to the vanishing \\ngradient problem, particularly if you are working with \\nlong sequences or have very deep neural networks.\\nWith these concepts in mind, there is one that we did not cover in this \\nchapter: data preprocessing. It is more appropriate to discuss with the \\nproblems we are facing.\\nLet’s move from this chapter and get into the weeds of natural \\nlanguage processing with a couple of example problems. In the next \\nchapter, we walk through a couple of methods for preprocessing text, \\ndiscuss their relative advantages and disadvantages, and compare model \\nperformance when using them.Chapter 2  review of\\xa0Deep Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 53}, page_content='43© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_3CHAPTER 3\\nWorking with\\xa0 \\nRaw Text\\nThose who approach NLP with the intention of applying deep learning \\nare most likely immediately confronted with a simple question: How \\ndoes a machine learning algorithm learn to interpret text data? Similar \\nto the situations in which a feature set may have a categorical feature, we \\nmust perform some preprocessing. While the preprocessing we perform \\nin NLP often is more involved than simply converting a categorical \\nfeature using label encoding, the principle is the same. We need to find \\na way to represent individual observations of texts as a row, and encode \\na static number of features, represented as columns, across all of these \\nobservations. As such, feature extraction becomes the most important \\naspect of text preprocessing.\\nThankfully, there has been a considerable amount of work, \\nincluding ongoing work, to develop preprocessing algorithms of various \\ncomplexities. This chapter introduces these preprocessing methods, walks \\nthrough which situations they each work well with, and applies them to \\nexample NLP problems that focus on document classification. Let’s start \\nby discussing what you should be aware of prior to performing feature \\nextraction from text.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 54}, page_content='44 Tokenization and\\xa0Stop Words\\nWhen you are working with raw text data, particularly if it uses a web \\ncrawler to pull information from a website, for example, you must assume \\nthat not all of the text will be useful to extract features from. In fact, it is \\nlikely that more noise will be introduced to the data set and make the \\ntraining of a given machine learning model less effective. As such, I suggest \\nthat you perform preliminary steps. Let’s walk through these steps using \\nthe following sample text.\\nsample_text = \"\\'I am a student from the University of Alabama. I\\nwas born in Ontario, Canada and I am a huge fan of the United \\nStates. I am going to get a degree in Philosophy to improve\\nmy chances of becoming a Philosophy professor. I have been\\nworking towards this goal for 4 years. I am currently enrolled\\nin a PhD program. It is very difficult, but I am confident that\\nit will be a good decision\"\\'\\nWhen the sample_text  variable prints, there is the following output:\\n\\'I am a student from the University of Alabama. I\\nwas born in Ontario, Canada and I am a huge fan of the United\\nStates. I am going to get a degree in Philosophy to improve my\\nchances of becoming a Philosophy professor. I have been working\\ntowards this goal for 4 years. I am currently enrolled in a PhD \\nprogram. It is very difficult, but I am confident that it will\\nbe a good decision\\'\\nYou should observe that the computer reads bodies of text, even if \\npunctuated, as single string objects. Because of this, we need to find a way \\nto separate this single body of text so that the computer evaluates each \\nword as an individual string object. This brings us to the concept of word \\ntokenization , which is simply the process of separating a single string Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 55}, page_content=\"45object, usually a body of text of varying length, into individual tokens \\nthat represent words or characters that we would like to evaluate further. \\nAlthough you can find ways to implement this from scratch, for brevity’s \\nsake, I suggest that you utilize the Natural Language Toolkit (NLTK) \\nmodule.\\nNLTK allows you to use some of the more basic NLP functionalities, \\nas well as pretrained models for different tasks. It is my goal to allow \\nyou to train your own models, so we will not be working with any of the \\npretrained models in NLTK.\\xa0However, you should read through the NLTK \\nmodule documentation to become familiar with certain functions and \\nalgorithms that expedite text preprocessing. Relating back to our example, \\nlet’s tokenize the sample data via the following code:\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nsample_word_tokens = word_tokenize(sample_text)\\nsample_sent_tokens = sent_tokenize(sample_text)\\nWhen you print the sample_word_tokens  variable, you should observe \\nthe following:\\n['I', 'am', 'a', 'student', 'from', 'the', 'University', 'of', \\n'Alabama', '.', 'I', 'was', 'born', 'in', 'Ontario', ',', \\n'Canada', 'and', 'I', 'am', 'a', 'huge', 'fan', 'of', 'the', \\n'United', 'States', '.', 'I', 'am', 'going', 'to', 'get', 'a', \\n'degree', 'in', 'Philosophy', 'to', 'improve', 'my', 'chances', \\n'of', 'becoming', 'a', 'Philosophy', 'professor', '.', 'I', \\n'have', 'been', 'working', 'towards', 'this', 'goal', 'for', \\n'4', 'years', '.', 'I', 'am', 'currently', 'enrolled', 'in', \\n'a', 'PhD', 'program', '.', 'It', 'is', 'very', 'difficult', \\n',', 'but', 'I', 'am', 'confident', 'that', 'it', 'will', 'be', \\n'a', 'good', 'decision']Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 56}, page_content='46You will also observe that we have defined another tokenized object, \\nsample_sent_tokens . The difference between word_tokenize()  and  \\nsent_tokenize()  is simply that the latter tokenizes text by sentence \\ndelimiters. This is observed in the following output:\\n  [\\'I am a student from the University of Alabama.\\', \\'I \\\\nwas \\nborn in Ontario, Canada and I am a huge fan of the United \\nStates.\\', \\'I am going to get a degree in Philosophy to improve \\nmy chances of \\\\nbecoming a Philosophy professor.\\', \\'I have \\nbeen working towards this goal\\\\nfor 4 years.\\', \\'I am currently \\nenrolled in a PhD program.\\', \\'It is very difficult, \\\\nbut I am \\nconfident that it will be a good decision\\']\\nNow we have individual tokens that we can preprocess! From this \\nstep forward, we can clean out some of the junk text that we would not \\nwant to extract features from. Typically, the first thing we want to get rid \\nof are stop words , which are usually defined as very common words in a \\ngiven language. Most often, lists of stop words that we build or utilize in \\nsoftware packages include function words , which are words that express \\na grammatical relationship (rather than having an intrinsic meaning). \\nExamples of function words include the, and , for, and of.\\nIn this example, we use the list of stop words from the NLTK package.\\n[u\\'i\\', u\\'me\\', u\\'my\\', u\\'myself\\', u\\'we\\', u\\'our\\', u\\'ours\\', \\nu\\'ourselves\\', u\\'you\\', u\"you\\'re\", u\"you\\'ve\", u\"you\\'ll\", \\nu\"you\\'d\", u\\'your\\', u\\'yours\\', u\\'yourself\\', u\\'yourselves\\', \\nu\\'he\\', u\\'him\\', u\\'his\\', u\\'himself\\', u\\'she\\', u\"she\\'s\", u\\'her\\', \\nu\\'hers\\', u\\'herself\\', u\\'it\\', u\"it\\'s\", u\\'its\\', u\\'itself\\', \\nu\\'they\\', u\\'them\\', u\\'their\\', u\\'theirs\\', u\\'themselves\\', u\\'what\\', \\nu\\'which\\', u\\'who\\', u\\'whom\\', u\\'this\\', u\\'that\\', u\"that\\'ll\", \\nu\\'these\\', u\\'those\\', u\\'am\\', u\\'is\\', u\\'are\\', u\\'was\\', u\\'were\\', Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 57}, page_content='47u\\'be\\', u\\'been\\', u\\'being\\', u\\'have\\', u\\'has\\', u\\'had\\', u\\'having\\', \\nu\\'do\\', u\\'does\\', u\\'did\\', u\\'doing\\', u\\'a\\', u\\'an\\', u\\'the\\', u\\'and\\', \\nu\\'but\\', u\\'if\\', u\\'or\\', u\\'because\\', u\\'as\\', u\\'until\\', u\\'while\\', \\nu\\'of\\', u\\'at\\', u\\'by\\', u\\'for\\', u\\'with\\', u\\'about\\', u\\'against\\', \\nu\\'between\\', u\\'into\\', u\\'through\\', u\\'during\\', u\\'before\\', \\nu\\'after\\', u\\'above\\', u\\'below\\', u\\'to\\', u\\'from\\', u\\'up\\', u\\'down\\', \\nu\\'in\\', u\\'out\\', u\\'on\\', u\\'off\\', u\\'over\\', u\\'under\\', u\\'again\\', \\nu\\'further\\', u\\'then\\', u\\'once\\', u\\'here\\', u\\'there\\', u\\'when\\', \\nu\\'where\\', u\\'why\\', u\\'how\\', u\\'all\\', u\\'any\\', u\\'both\\', u\\'each\\', \\nu\\'few\\', u\\'more\\', u\\'most\\', u\\'other\\', u\\'some\\', u\\'such\\', u\\'no\\', \\nu\\'nor\\', u\\'not\\', u\\'only\\', u\\'own\\', u\\'same\\', u\\'so\\', u\\'than\\', \\nu\\'too\\', u\\'very\\', u\\'s\\', u\\'t\\', u\\'can\\', u\\'will\\', u\\'just\\', u\\'don\\', \\nu\"don\\'t\", u\\'should\\', u\"should\\'ve\", u\\'now\\', u\\'d\\', u\\'ll\\', \\nu\\'m\\', u\\'o\\', u\\'re\\', u\\'ve\\', u\\'y\\', u\\'ain\\', u\\'aren\\', u\"aren\\'t\", \\nu\\'couldn\\', u\"couldn\\'t\", u\\'didn\\', u\"didn\\'t\", u\\'doesn\\', \\nu\"doesn\\'t\", u\\'hadn\\', u\"hadn\\'t\", u\\'hasn\\', u\"hasn\\'t\", u\\'haven\\', \\nu\"haven\\'t\", u\\'isn\\', u\"isn\\'t\", u\\'ma\\', u\\'mightn\\', u\"mightn\\'t\", \\nu\\'mustn\\', u\"mustn\\'t\", u\\'needn\\', u\"needn\\'t\", u\\'shan\\', u\"shan\\'t\", \\nu\\'shouldn\\', u\"shouldn\\'t\", u\\'wasn\\', u\"wasn\\'t\", u\\'weren\\', \\nu\"weren\\'t\", u\\'won\\', u\"won\\'t\", u\\'wouldn\\', u\"wouldn\\'t\"]\\nAll of these words are lowercase by default. You should be aware that \\nstring objects must exactly match to return a true Boolean variable when \\ncomparing two individual strings. To put this more plainly, if we were to \\nexecute the code “you” == “YOU” ,  the Python interpreter returns false. \\nThe specific instance in which this affects our example can be observed \\nby executing the mistake()  and advised_preprocessing()  functions, \\nrespectively. Observe the following outputs:Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 58}, page_content=\"48['I', 'student', 'University', 'Alabama', '.', 'I', 'born', \\n'Ontario', ',', 'Canada', 'I', 'huge', 'fan', 'United', \\n'States', '.', 'I', 'going', 'get', 'degree', 'Philosophy', \\n'improve', 'chances', 'becoming', 'Philosophy', 'professor', \\n'.', 'I', 'working', 'towards', 'goal', '4', 'years', '.', \\n'I', 'currently', 'enrolled', 'PhD', 'program', '.', 'It', \\n'difficult', ',', 'I', 'confident', 'good', 'decision']\\n['student', 'University', 'Alabama', '.', 'born', 'Ontario', \\n',', 'Canada', 'huge', 'fan', 'United', 'States', '.', \\n'going', 'get', 'degree', 'Philosophy', 'improve', 'chances', \\n'becoming', 'Philosophy', 'professor', '.', 'working', \\n'towards', 'goal', '4', 'years', '.', 'currently', 'enrolled', \\n'PhD', 'program', '.', 'difficult', ',', 'confident', 'good', \\n'decision']\\nAs you can see, the mistake()  function does not catch the uppercase \\n“I” characters, meaning that there are several stop words still in the text. \\nThis is solved by uppercasing all the stop words and then evaluating \\nwhether each uppercase word in the sample text was in the stop_words  \\nlist. This is exemplified with the following two lines of code:\\nstop_words = [word.upper() for word in stopwords.\\nwords('english')]\\nword_tokens = [word for word in sample_word_tokens if word.\\nupper() not in stop_words]\\nAlthough embedded methods in feature extraction algorithms likely \\naccount for this case, you should be aware that strings must match exactly, \\nand you must account for this when preprocessing manually.\\nThat said, there is junk data that you should be aware of—specifically, \\nthe grammatical characters. You will be relieved to hear that the word_\\ntokenize()  function also categorizes colons and semicolons as individual Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 59}, page_content=\"49word tokens, but you still have to get rid of them. Thankfully, NLTK \\ncontains another tokenizer worth knowing about, which is defined and \\nutilized in the following code:\\nfrom nltk.tokenize import RegexpTokenizer\\ntokenizer = RegexpTokenizer(r'\\\\w+')\\nsample_word_tokens = tokenizer.tokenize(str(sample_word_\\ntokens))\\nsample_word_tokens = [word.lower() for word in sample_word_\\ntokens]\\nWhen we print the sample_word_tokens  variable, we get the following \\noutput:\\n['student', 'university', 'alabama', 'born', 'ontario', \\n'canada', 'huge', 'fan', 'united', 'states', 'going', 'get', \\n'degree', 'philosophy', 'improve', 'chances', 'becoming', \\n'philosophy', 'professor', 'working', 'towards', 'goal', \\n'4', 'years', 'currently', 'enrolled', 'phd', 'program', \\n'difficult', 'confident', 'good', 'decision']\\nIn the course of this example, we have reached the final step! We have \\nremoved all the standard stop words, as well as all grammatical tokens. \\nThis is an example of a document that is ready for feature extraction, \\nwhereupon some additional preprocessing may occur.\\nNext, I’ll discuss some of the various feature extraction algorithms. \\nAnd let’s work on denser sample data alongside a preprocessed example \\nparagraph.Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 60}, page_content=\"50 The Bag-of-Words Model (BoW)\\nA BoW model is one of the more simplistic feature extraction algorithms \\nthat you will come across. The name “bag-of-words” comes from the \\nalgorithm simply seeking to know the number of times a given word is \\npresent within a body of text. The order or context of the words is not \\nanalyzed here. Similarly, if we have a bag filled with six pencils, eight \\npens, and four notebooks, the algorithm merely cares about recording the \\nnumber of each of these objects, not the order in which they are found, or \\ntheir orientation.\\nHere, I have defined a sample bag-of-words function.\\ndef bag_of_words(text):\\n     _bag_of_words = [collections.Counter(re.findall(r'\\\\w+', \\nword)) for word in text]\\n    bag_of_words = sum(_bag_of_words, collections.Counter())\\n    return bag_of_words\\nsample_word_tokens_bow = bag_of_words(text=sample_word_tokens)\\nprint(sample_word_tokens_bow)\\nWhen we execute the preceding code, we get the following output:\\nCounter({'philosophy': 2, 'program': 1, 'chances': 1, 'years': 1,  \\n'states': 1, 'born': 1, 'towards': 1, 'canada': 1, 'huge': 1,  \\n'united': 1, 'goal': 1, 'working': 1, 'decision': 1, \\n'currently': 1, 'confident': 1, 'going': 1, '4': 1, \\n'difficult': 1, 'good': 1, 'degree': 1, 'get': 1, 'becoming': 1,  \\n'phd': 1, 'ontario': 1, 'fan': 1, 'student': 1, 'improve': 1, \\n'professor': 1, 'enrolled': 1, 'alabama': 1, 'university': 1})\\nThis is an example of a BoW model when presented as a dictionary. \\nObviously, this is not a suitable input format for a machine learning \\nalgorithm. This brings me to discuss the myriad of text preprocessing Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 61}, page_content=\"51functions available in the scikit-learn library, which is a Python library \\nthat all data scientists and machine learning engineers should be familiar \\nwith. For those who are new to it, this library provides implementations \\nof machine learning algorithms, as well as several data preprocessing \\nalgorithms. Although we won’t walk through much of this package, the text \\npreprocessing functions are extremely useful.\\n CountVectorizer\\nLet’s start by walking through the BoW equivalent—CountVectorizer, \\nan implementation of bag-of-words in which we code text data as a \\nrepresentation of features/words. The values of each of these features \\nrepresent the occurrence counts of words across all documents. If you \\nrecall, we defined a sample_sent_tokens  variable, which we will analyze. \\nWe define a bow_sklearn()  function beneath where we preprocess our \\ndata. The function is defined as follows:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ndef bow_sklearn(text=sample_sent_tokens):\\n     c = CountVectorizer(stop_words='english',  \\ntoken_pattern=r'\\\\w+')\\n    converted_data = c.fit_transform(text).todense()\\n    print(converted_data.shape)\\n    return converted_data, c.get_feature_names()\\nTo provide context, in this example, we are assuming that each sentence \\nis an individual document, and we are creating a feature set in which each \\nfeature is an individual token. When we instantiate CountVectorizer() , we \\nset two parameters: stop_words , and token_pattern . These two arguments \\nare the embedded methods in the feature extraction that remove stop \\nwords and grammatical tokens. The fit_transform()  attribute expects \\nto receive a list, an array, or a similar object of iterable string objects. We \\nassign the bow_data  and feature_names  variables to the data that the  Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 62}, page_content=\"52bow_sklearn()  returns, respectively. Our converted data set is a 6 × 50 matrix, \\nwhich means that we have six sentences, all of which have 50 features. \\nObserve our data set and feature names, respectively, in the following \\noutputs:\\n[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\\n [0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0]\\n [0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0]\\n [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\\n [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\\n [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\\n[u'4', u'alabama', u'born', u'canada', u'chances', \\nu'confident', u'currently', u'decision', u'degree', \\nu'difficult', u'enrolled', u'fan', u'goal', u'going', u'good', \\nu'huge', u'improve', u'ontario', u'phd', u'philosophy', \\nu'professor', u'program', u'states', u'student', u'united', \\nu'university', u'working', u'years']\\nTo extrapolate this example to a larger number of documents, and \\nostensibly larger vocabulary sizes, our matrices for preprocessed text data \\ntends to have a large number of features, sometimes well over 1000. How \\nto evaluate these features effectively is the machine learning challenge \\nwe seek to solve. You typically want to use the bag-of-words feature \\nextraction technique for document classification. Why is this the case? \\nWe assume that documents of certain classifications contain certain \\nwords. For example, we expect a document referencing political science \\nto perhaps feature jargon such as dialectical materialism  or free market \\ncapitalism ; whereas a document that is referring to classical music will \\nhave terms such as crescendo , diminuendo , and so forth. In these instances \\nof document classification, the location of the word itself is not terribly \\nimportant. It’s important to know what portion of the vocabulary is present \\nin one class of document vs. another.Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 63}, page_content='53Next, let’s look at our first example problem in the code in the  \\ntext_classifiction_demo.py  file.\\n Example Problem 1: Spam Detection\\nSpam detection is a relatively common task in that most people have an \\ninbox (email, social media instant messenger account, or similar entity) \\ntargeted by advertisers or malicious actors. Being able to block unwanted \\nadvertisements or malicious files is an important task. Because of this, we \\nare interested in pursuing a machine learning approach to spam detection. \\nLet’s begin by describing the data set before digging into the problem.\\nThis data set was downloaded from the UCI Machine Learning \\nRepository, specifically the Text Data section. Our data set consists of 5574 \\nobservations—all SMS messages. We observe from our data set that most \\nof the messages are not terribly long. Figure\\xa0 3-1 is a histogram of our entire \\ndata set.\\nFigure 3-1.  SMS mes sage length histogramChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 64}, page_content=\"54Something else we should be mindful of is the distribution between \\nthe class labels, which tends to be heavily skewed. In this data set, 4825 \\nobservations are marked as “ham” (being not spam), and 747 are marked \\nas “spam” . You must be vigilant in evaluating your machine learning \\nsolutions to ensure that they do not overfit the training data, and then fail \\nmiserably on test data.\\nLet’s briefly do some additional data set discovery before we move on \\nto tackling the problem directly. When we look at the header of our data \\nset, we observe the following:\\n0   ham  Go until jurong point, crazy.. Available only ...\\n1   ham                      Ok lar... Joking wif u oni...\\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\\n3   ham  U dun say so early hor... U c already then say...\\n4   ham  Nah I don't think he goes to usf, he lives aro...\\nThe first column is our categorical label/response variable. The second \\ncolumn comprises text contained within each individual SMS.\\xa0We will use \\na bag-of-words representation via the CountVectorizer() . Our entire data \\nset has a vocabulary size of 8477 words. The load_spam_data()  function \\nshows that the preprocessing steps mimic the warmup example at the \\nbeginning of the chapter.\\nLet’s fit and train our model and evaluate the results. When beginning \\na classification task, I suggest that you evaluate the results of the logistic \\nregression. This determines if your data is linearly separable or not. If it \\nis, the logistic regression should work fine, which saves you from further \\nmodel selection and time-consuming hyper-parameter optimization. If it \\ndoes fail, then you can use those methods.\\nWe train a model using both L1 and L2 weight regularization in the \\ntext_classifiction_demo.py  file; however, we will walk through the L1 \\nnorm regularized example here because it yielded better test results:Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 65}, page_content=\"55#Fitting training algorithm\\nl = LogisticRegression(penalty='l1')\\naccuracy_scores, auc_scores = [], []\\nThose of you that are not familiar with logistic regression you should \\nlearn about elsewhere; however, I will discuss the L1-regularized logistic \\nregression briefly. L1 norm regularization in linear models is standard \\nfor LASSO (least absolute shrinkage selection operator), where during \\nthe learning process, the L1 norm can theoretically force some regression \\ncoefficients to 0. In contrast, the L2 norm, often seen in ridge regression, \\ncan force some regression coefficients during the learning process to \\nnumbers close to 0. The difference between this is that coefficients that \\nare 0 effectively perform feature selection on our feature set by eliminating \\nthem. Mathematically, we represent this regularization via Equation 3.1.\\n minl og ;)\\niM\\niIpy x\\n=å-+\\n11(| qb q\\n (3.1)\\nWe w ill evaluate the distribution of test scores over several trials. scikit-  \\nlearn al gorithms’ fit()  method trains the algorithm of a given data set. As \\nsuch, all the iterations that optimize the parameters are performed. To see \\nlogging information in the training process, set the verbose  parameter to 1.\\nLet’s look at the code that will collect the distribution of both accuracy \\nand AUC scores.\\nfor i in range(trials):\\n   if i%10 == 0 and i > 0:\\n        print('Trial ' + str(i) + ' out of 100 completed')\\n   l.fit(train_x, train_y)\\n   predicted_y_values = l.predict(train_x)\\n    accuracy_scores.append(accuracy_score(train_y, predicted_y_\\nvalues))Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 66}, page_content='56    fpr, tpr = roc_curve(train_y, predicted_y_values)[0],  \\nroc_curve(train_y, predicted_y_values)[1]\\n   auc_scores.append(auc(fpr, tpr))\\nscikit-learn performs cross-validation so long as you define a random \\nseed utilizing the np.random.seed()  function, which we do near the \\nbeginning of the file. During each trial, we are fitting the data set to the \\nalgorithm, predicting the accuracy and AUC score, and appending them \\nto a list that we defined. When we evaluate our results from training, we \\nobserve the following:\\nSummary Statistics (AUC):\\n        Min       Max      Mean      SDev    Range\\n0  0.965348   0.968378   0.967126   0.000882   0.00303\\nSummary Statistics (Accuracy Scores):\\n        Min      Max      Mean      SDev     Range\\n0  0.990356   0.99116  0.990828   0.000234   0.000804\\nTest Model Accuracy: 0.9744426318651441\\nTest True Positive Rate: 0.8412698412698413\\nTest False Positive Rate: 0.004410838059231254\\n[[1580    7]\\n [  40  212]]\\nFortunately, we see that logistic regression performs excellently on \\nthis problem. We have excellent accuracy and AUC scores, with very little \\nvariance from one trial to the next. Let’s evaluate the AUC score, as shown \\nin Figure\\xa0 3-2.Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 67}, page_content='57Our test AUC score is 0.92. This algorithm would be deployable in an \\napplication to test for spam results. In the course of solution discovery, \\nI suggest that you use this model rather than others. Although you are \\nencouraged to find other methods, I observed that the gradient-boosted \\nclassification tree and random forests performed considerably worse, \\nwith AUC scores of roughly 0.72. Let’s discuss a more sophisticated term \\nfrequency scheme.\\n Term Frequency Inverse Document Frequency\\nTerm frequency–inverse document frequency  (TFIDF) is based on BoW, but \\nprovides more detail than simply taking term frequency, as was done in \\nthe prior example. TFIDF yields a value that shows how important a given \\nword is by not only looking at term frequency, but also analyzing how \\nmany times the word appears across all documents. The first portion, term \\nfrequency, is relatively straightforward.\\nFigure 3-2.  Test set ROC curveChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 68}, page_content='58Let’s look at an example to see how to calculate TFIDF .\\xa0We define a \\nnew body of text and use the sample text defined at the beginning of the \\nchapter, as follows:\\ntext = “‘I was a student at the University of \\nPennsylvania, but now work on\\nWall Street as a Lawyer. I have been living in \\nNew\\xa0York for roughly five years\\nnow, however I am looking forward to eventually \\nretiring to Texas once I have\\nsaved up enough money to do so. ”’\\ndocument_list = list([sample_text, text])\\nNow that we have a list of documents, let’s look at exactly what the \\nTFIDF algorithm does. The first portion, term frequency, has several \\nvariants, but we will focus on the standard raw count scheme. We simply \\nsum the terms across all documents. The term frequency is equivalent to \\nEquation 3.2.\\n f\\nftd\\ntdtd,\\n, ¢¢Îå (3.2)\\nft, d is equal to the frequency of the term across all documents. ftd¢, \\nis equal to the frequency of that same term but within each individual \\ndocument. In our code, we document these steps in the tf_idf_example()  \\nfunction, as follows:\\ndef tf_idf_example(textblobs=[text, text2]):\\ndef term_frequency(word, textblob): (1)Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 69}, page_content='59return textblob.words.count(word)/float(len(textblob.words))\\ndef document_counter(word, text):\\nreturn sum(1 for blob in text if word in blob)\\ndef idf(word, text): (2)\\nreturn np.log(len(text) /1 + float(document_counter(word, \\ntext)))\\ndef tf_idf(word, blob, text):\\nreturn term_frequency(word, blob) * idf(word, text)\\noutput = list()\\nfor i, blob in enumerate(textblobs):\\noutput.append({word: tf_idf(word, blob, textblobs) for word in \\nblob.words})\\nprint(output)\\nThanks to the TextBlob package, we are able to fairly quickly re-create \\nthe TFIDF toy implementation. I will address each of the functions within \\nthe tf_idf_example()  function. You are aware of term frequency, so I \\ncan discuss inverse document frequency. We define inverse document \\nfrequency as a measure of how frequently a word appears across the entire \\ncorpus. Mathematically, this relationship is expressed in Equation 3.3.\\n idf,tDN\\ndD td() =ÎÎ{}log:  (3.3)\\nThis e quation calculates the log of the total number of documents in \\nour corpus, divided by all the documents in which the term that we are \\nevaluating appears. In our code, we calculate this with the function (2). \\nNow, we are ready to proceed to the final step of the algorithm, which is \\nmultiplying the term frequency by the inverse document frequency, as \\nshown in the preceding code. We then yield the following output:Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 70}, page_content=\"60 [{'up': '0.027725887222397813', 'money': \\n'0.021972245773362195', 'am': '0.027725887222397813', 'years': \\n'0.027725887222397813', 'as': '0.027725887222397813', 'at': \\n'0.027725887222397813', 'have': '0.055451774444795626', \\n'in': '0.027725887222397813', 'New': '0.021972245773362195', \\n'saved': '0.021972245773362195', 'Texas': \\n'0.021972245773362195', 'living': '0.021972245773362195', \\n'for': '0.027725887222397813', 'to': '0.08317766166719343', \\n'retiring': '0.027725887222397813', 'been': \\n'0.021972245773362195', 'looking': '0.021972245773362195', \\n'Pennsylvania': '0.021972245773362195', 'enough': \\n'0.021972245773362195', 'York': '0.021972245773362195', \\n'forward': '0.027725887222397813', 'was': \\n'0.027725887222397813', 'eventually': '0.021972245773362195', \\n'do': '0.027725887222397813', 'I': '0.11090354888959125', \\n'University': '0.027725887222397813', 'however': \\n'0.027725887222397813', 'but': '0.021972245773362195', 'five': \\n'0.021972245773362195', 'student': '0.021972245773362195', \\n'now': '0.04394449154672439', 'a': '0.055451774444795626', \\n'on': '0.027725887222397813', 'Wall': '0.021972245773362195', \\n'of': '0.027725887222397813', 'work': '0.021972245773362195', \\n'roughly': '0.021972245773362195', 'Street': \\n'0.021972245773362195', 'so': '0.021972245773362195', 'Lawyer': \\n'0.021972245773362195', 'the': '0.027725887222397813', 'once': \\n'0.021972245773362195'}, {'and': '0.0207285337484549', 'is': \\n'0.0207285337484549', 'each': '0.0207285337484549', 'am': \\n'0.026156497379620575', 'years': '0.026156497379620575', \\n'have': '0.05231299475924115', 'in': '0.026156497379620575', \\n'children': '0.0414570674969098', 'considering': \\n'0.0207285337484549', 'retirement': '0.0207285337484549', \\n'doctor': '0.0207285337484549', 'retiring': Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 71}, page_content=\"61'0.026156497379620575', 'two': '0.0207285337484549', 'long': \\n'0.0207285337484549', 'next': '0.0207285337484549', 'to': \\n'0.05231299475924115', 'forward': '0.026156497379620575', \\n'was': '0.026156497379620575', 'couple': '0.0207285337484549', \\n'more': '0.0207285337484549', 'ago': '0.0207285337484549', \\n'them': '0.0207285337484549', 'that': '0.0207285337484549', \\n'I': '0.1046259895184823', 'University': \\n'0.026156497379620575', 'who': '0.0414570674969098', 'however': \\n'0.026156497379620575', 'quite': '0.0207285337484549', \\n'me': '0.0207285337484549', 'Yale': '0.0207285337484549', \\n'with': '0.0207285337484549', 'the': '0.05231299475924115', \\n'a': '0.07846949213886173', 'both': '0.0207285337484549', \\n'look': '0.026156497379620575', 'of': '0.026156497379620575', \\n'grandfather': '0.0207285337484549', 'spending': \\n'0.0207285337484549', 'three': '0.0207285337484549', 'time': \\n'0.0414570674969098', 'making': '0.0207285337484549', 'went': \\n'0.0207285337484549'}]\\nThis brings us to the end of our toy example using TFIDF .\\xa0Before we \\njump into the example, let’s review how we would utilize this example \\nin scikit-learn, such that we can input this data into a machine learning \\nalgorithm. Similar to CountVectorizer() , scikit-learn has provided a \\nTfidfVectorizer()  method that comes in handy. The following shows its \\nutilization. I will dive into a deeper use of its preprocessing methods later.\\ndef tf_idf_sklearn(document=document_list):\\n     t = TfidfVectorizer(stop_words='english',  \\ntoken_pattern=r'\\\\w+')\\n    x = t.fit_transform(document_list).todense()\\n    print(x)Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 72}, page_content='62When we execute the function, it yields the following result:\\n[[0.         0.         0.         0.         0.         0.24235766\\n  0.17243947 0.          0.24235766 0.24235766 0.          0.\\n  0.24235766 0.          0.24235766 0.24235766 0.24235766 0.\\n  0.         0.17243947 0.24235766 0.24235766 0.          0.24235766\\n  0.24235766 0.24235766 0.          0.17243947 0.24235766 0.\\n  0.24235766 0.          0.17243947 0.24235766]\\n [0.20840129 0.41680258 0.20840129 0.20840129 0.20840129 0.\\n  0.14827924 0.20840129 0.          0.         0.20840129 0.20840129\\n  0.         0.20840129 0.          0.         0.         0.20840129\\n  0.20840129 0.14827924 0.          0.         0.20840129 0.\\n  0.         0.         0.41680258 0.14827924 0.          0.20840129\\n  0.         0.20840129 0.14827924 0.         ]]\\nThis function yields a 2 × 44 matrix, and it is ready for input into a \\nmachine learning algorithm for evaluation.\\nNow let’s work through another example problem using TFIDF as our \\nfeature extractor while utilizing another machine learning algorithm as we \\ndid for the BoW feature extraction.\\n Example Problem 2: Classifying Movie Reviews\\nWe obtained the following IMDB movie review data set from http://www.\\ncs.cornell.edu/people/pabo/movie-review-data/ .\\nWe are going to work with the raw text directly, rather than using \\npreprocessed text data sets often provided via various machine learning \\npackages.\\nLet’s take a snapshot of the data.\\ntristar / 1 : 30 / 1997 / r ( language , violence , dennis \\nrodman ) cast : jean-claude van damme ; mickey rourke ; dennis \\nrodman ; natacha lindinger ; paul freeman director : tsui hark Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 73}, page_content='63screenplay : dan jakoby ; paul mones ripe with explosions , \\nmass death and really weird hairdos , tsui hark\\'s \" double \\nteam \" must be the result of a tipsy hollywood power lunch \\nthat decided jean-claude van damme needs another notch on his \\nbad movie-bedpost and nba superstar dennis rodman should have \\nan acting career . actually , in \" double team , \" neither\\'s \\nperformance is all that bad . i\\'ve always been the one critic \\nto defend van damme -- he possesses a high charisma level that \\nsome genre stars ( namely steven seagal ) never aim for ; it\\'s \\njust that he\\'s never made a movie so exuberantly witty since \\n1994\\'s \" timecop . \" and rodman . . . well , he\\'s pretty much \\nrodman . he\\'s extremely colorful , and therefore he pretty much \\nfits his role to a t , even if the role is that of an ex-cia\\nAs you can see, this data is filled with lots of grammatical noise that we \\nwill need to remove, but is also rich in descriptive text. We will opt to use \\nthe TfidfVectorizer()  method on this data.\\nFirst, I would like to direct you to two functions at the beginning of  \\nthe file:\\ndef remove_non_ascii(text):\\n    return \".join([word for word in text if ord(word) < 128])\\nNotice that we are using the native Python function ord() . This \\nfunction expects a string, and it returns either the Unicode point for \\nUnicode objects or the value of the byte. If the ord()  function returns \\nan integer less than 128, this poses no problem for our preprocesser \\nand therefore we keep the string in question; otherwise, we remove \\nthe character. We end this step by joining all the remaining words back \\ntogether with the \".join()  function. The reasoning for preprocessing \\nduring data preparation is that our text preprocessor expects Unicode \\nobjects when being fed to it. When we are capturing raw text data, \\nparticularly if it is from an HTML page, many of the string objects Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 74}, page_content=\"64loaded before preprocessing and removal of stop words will not be \\nUnicode-compatible.\\nLet’s look at the function that loads our data.\\ndef load_data():\\n     negative_review_strings = os.listdir('/Users/tawehbeysolow/\\nDownloads/review_data/tokens/neg')\\n     positive_review_strings =  os.listdir('/Users/tawehbeysolow/\\nDownloads/review_data/tokens/pos')\\n    negative_reviews, positive_reviews = [], []\\nWe start by loading the file names of all the .txt  files to be processed. \\nTo do this, we use the os.listdir()  function. I suggest you use this \\nfunction when building similar applications that require preprocessing a \\nlarge number of files.\\nNext, we load our files with the open()  function, and then apply the \\nremove_non_ascii()  function, as follows:\\n    for positive_review in positive_review_strings:\\n         with open('/Users/tawehbeysolow/Downloads/review_data/\\ntokens/pos/'+str(positive_review), 'r') as positive_file:\\n             positive_reviews.append(remove_non_ascii(positive_\\nfile.read()))\\n    for negative_review in negative_review_strings:\\n         with open('/Users/tawehbeysolow/Downloads/review_data/\\ntokens/neg/'+str(negative_review), 'r') as negative_file:\\n             negative_reviews.append(remove_non_ascii(negative_\\nfile.read()))\\nWith our initial preprocessing done, we end by concatenating both \\nthe positive and negative reviews, in addition to the respective vectors \\nthat contain their labels. Now, we can get to the meat and potatoes of this \\nmachine learning problem, starting with the train_logistic_model()  Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 75}, page_content=\"65function. In a similar fashion, we use logistic regression as the baseline \\nfor the problem. Although most of the following functions are similar in \\nstructure to Example Problem 1, let’s look at the beginning of this function \\nto analyze what we have changed.\\n#Load and preprocess text data\\nx, y = load_data()\\nt = TfidfVectorizer(min_df=10, max_df=300, stop_\\nwords='english', token_pattern=r'\\\\w+')\\nx = t.fit_transform(x).todense()\\nWe are utilizing two new arguments: min_df  corresponds to the \\nminimum document frequency to retain a word, and max_df  refers to \\nthe maximum amount of documents that a word can appear in before \\nit is omitted from the sparse matrix that we create. When increasing the \\nmaximum and minimum document frequencies, I noticed that the L1 \\npenalty model performed better than the L2 penalty model. I would posit \\nthat this is likely due to the fact that as we increase the min_df  parameter, \\nwe are creating a considerably sparser matrix than if we had a denser \\nmatrix. You should keep this in mind so as to not overselect features if they \\nperformed any feature selection on their matrices beforehand.\\nLet’s evaluate the results of the logistic regression, as shown in the \\nfollowing output (also see Figures\\xa0 3-3 and 3-4).\\nSummary Statistics from Training Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.723874   0.723874     0.0  0.723874    0.0\\nSummary Statistics from Training Set (Accuracy):\\n       Mean       Max  Range      Mean  SDev\\n0  0.726788   0.726788     0.0  0.726788    0.0\\nTraining Data Confusion Matrix:\\n[[272 186]\\n [ 70 409]]Chapter 3  Working With\\xa0raW text\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 76}, page_content='66Summary Statistics from Test Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.723874   0.723874     0.0  0.723874    0.0\\nSummary Statistics from Test Set (Accuracy):\\n        Mean       Max  Range      Mean  SDev\\n0  0.726788   0.726788     0.0  0.726788    0.0\\nTest Data Confusion Matrix:\\n[[272 186]\\n [ 70 409]]\\nSummary Statistics from Training Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981824   0.981824     0.0  0.981824    0.0\\nSummary Statistics from Training Set (Accuracy):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981857   0.981857     0.0  0.981857    0.0\\nTraining Data Confusion Matrix:\\n[[449   9]\\n [  8 471]]\\nSummary Statistics from Test Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981824   0.981824     0.0  0.981824    0.0\\nSummary Statistics from Test Set (Accuracy):\\n        Mean       Max  Range      Mean  SDev\\n0  0.981857   0.981857     0.0  0.981857    0.0\\nTest Data Confusion Matrix:\\n[[449   9]\\n [  8 471]]Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 77}, page_content='67Both in training and performance, logistic regression performs \\nconsiderably better when utilizing the L2 weight regularization method, \\ngiven the parameters we used for the TfidfVectorizer()  feature \\nextraction algorithm.\\nFigure 3-3.  L1 logistic regression test set ROC curveChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 78}, page_content='68I created multiple solutions to evaluate: a random forest classifier, a \\nnaïve Bayes classifier, and a multilayer perceptron. We begin with a general \\noverview of all of our methods and their respective orientations.\\nStarting with the multilayer perceptron in the mlp_movie_\\nclassification_model.py  file, notice that much of the neural network \\nis the same as the example in Chapter 2, with the exception of an extra \\nhidden layer. That said, I would like to direct your attention to lines 92 \\nthrough 94.\\nregularization = tf.contrib.layers.l2_regularizer(scale=0.0005, \\nscope=None)\\nregularization_penalty = tf.contrib.layers.apply_\\nregularization(regularization, weights.values())\\ncross_entropy = cross_entropy + regularization_penalty\\nFigure 3-4.  L2 logi stic regression test set ROC curveChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 79}, page_content='69In these lines, we are performing weight regularization, as \\ndiscussed earlier in this chapter with the logistic regression L2 and L1 \\nloss parameters. Those of you who wish to apply this in TensorFlow \\ncan rest assured that these are the only modifications needed to add a \\nweight penalty to your neural network. While developing this solution, \\nI tried weight regularization utilizing L1 and L2 loss penalties, and I \\nexperimented with dropout. Weight regularization is the process of \\nlimiting the scope to which the weights can grow when utilizing different \\nvector norms. The two most referenced norms for weight regularization \\nare L1 and L2 norms. The following are their respective equations, which \\nare also illustrated in Figure\\xa0 3-5.\\n L1==\\n=å v1\\n11\\niN\\niv \\n L2==\\n=å v2\\n12\\niN\\niv \\nFigure 3-5.  L1 and L2 norm visualizationChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 80}, page_content='70When initially utilizing both one and two hidden layer(s), I noticed  \\nthat both the test and training performance were considerably worse  \\nwith dropout, even using dropout percentages as low as 0.05. As such,  \\nI cannot suggest that you utilize dropout for this problem. As for weight \\nregularization, additional parameter selection is not advisable; however, \\nI found negligible differences with L1 vs. L2 regularization. The confusion \\nmatrix and the ROC curve are shown in Figure\\xa0 3-6.\\nTest Set Accuracy Score: 0.8285714285714286\\nTest Set Confusion Matrix:\\n[[122  26]\\n [ 22 110]]\\nFigure 3-6.  ROC curve for multilayer perceptronChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 81}, page_content='71Let’s analyze the choice of parameters for the random forest and \\nnaïve Bayes classifiers. We kept our trees relatively short at a max_depth  \\nof ten splits. As for the naïve Bayes classifier, the only parameter we chose \\nis alpha, which we set to 0.005. Let’s evaluate Figures\\xa0 3-6 and 3-7 for the \\nresults of the model.\\nThe Figure\\xa0 3-8 shows the result of a naïve Bayes classifier.\\nFigure 3-7.  ROC curve for random forestChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 82}, page_content='72Summary Statistics from Training Set Random Forest (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.987991   0.987991     0.0  0.987991    0.0\\nSummary Statistics from Training Set Random Forest (Accuracy):\\n      Mean      Max  Range     Mean  SDev\\n0  0.98826  0.98826    0.0  0.98826   0.0\\nTraining Data Confusion Matrix (Random Forest):\\n[[447  11]\\n [  0 479]]\\nSummary Statistics from Training Set Naive Bayes (AUC):\\n       Mean       Max  Range      Mean          SDev\\n0  0.965362   0.965362     0.0  0.965362   2.220446e-16\\nFigure 3-8.  ROC curve for naïve Bayes classifierChapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 83}, page_content='73Summary Statistics from Training Set Naive Bayes (Accuracy):\\n       Mean       Max  Range      Mean          SDev\\n0  0.964781   0.964781     0.0  0.964781   3.330669e-16\\nTraining Data Confusion Matrix (Naive Bayes):\\n[[454   4]\\n [ 29 450]]\\nTest Data Confusion Matrix:\\n[[189  27]\\n [ 49 197]]\\nTest Data Confusion Matrix (Random Forest):\\n[[162  54]\\n [ 19 227]]\\nWhen evaluating the results, the neural network has a tendency to \\noverfit to training data, but its test performance is very similar to logistic \\nregression, although slightly less accurate. When assessing the results \\nof the naïve Bayes classifier and the random forest classifier, we observe \\nroughly similar AUC scores, with only a difference in false positives and \\ntrue positives as the trade-off that we must accept. In this instance, it is \\nimportant to consider our objective.\\nIf we are using these algorithms to label the reviews that users input, \\nand then perform analytics on top of these reviews, we want to maximize \\nthe accuracy rate, or seek models with the highest true positive rate and \\ntrue negative rate. In the instance of spam detection, we likely want the \\nmodel that has the best ability to properly classify spam from normal mail.\\nI have introduced and applied the bag-of-words schemes in both \\nthe logistic model and the naïve Bayes classifier. This brings us to the \\nfinal part of this section, in which I discuss their relative advantages and \\ndisadvantages. You should be aware of this so as to not waste time altering \\nsubpar solutions. The major advantage of BoW is that it is a relatively \\nstraightforward algorithm that allows you to quickly turn text into a Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 84}, page_content='74format interpretable by a machine learning algorithm, and to attack NLP \\nproblems directly.\\nThe largest disadvantage of BoW is its relative simplicity. BoW does \\nnot account for the context of words, and as such, it does not make it the \\nideal feature extraction method for more complex NLP tasks. For example, \\n“4” and “four” are considered semantically indistinguishable, but in BoW, \\nthey are considered two different words altogether. When we expand this \\nto phrases, “I went to college for four years, ” and “For 4 years, I attended a \\nuniversity, ” are treated as orthogonal vectors. Another example of a BoW \\nshortcoming is that it cannot distinguish the ordering of words. As such,  \\n“I am stupid” and “ Am I stupid” appear as the same vector.\\nBecause of these shortcomings, it is appropriate for us to utilize more \\nadvanced models, such as word embeddings, for these difficult problems, \\nwhich are discussed in detail in the next chapter.\\n Summary\\nThis brings us to the end of Chapter 3! This chapter tackled working with \\ntext data in document classification problems. You also became familiar \\nwith two BoW feature extraction methods.\\nLet’s take a moment to go over some of the most important lessons \\nfrom this chapter. Just as with traditional machine learning, you must \\ndefine the type of problem and analyze data. Is this simply document \\nclassification? Are we trying to find synonyms? We have to answer these \\nquestions before tackling any other steps.\\nThe removal of stop words, grammatical tokens, and frequent words \\nimproves the accuracy of our algorithms. Not every word in a document is \\ninformative, so you should know how to remove the noise. That said, over-  \\nsele cting features can be detrimental to our model’s success, so you should \\nbe aware of this too!Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 85}, page_content='75Whenever you are working with a machine learning problem, within \\nor outside the NLP domain, you must establish a baseline solution and \\nthen improve if necessary!  I suggest that you always start a deep learning \\nproblem by seeing how the solution appears, such as with a logistic \\nregression. Although it is my goal to teach you how to apply deep learning \\nto NLP-based problems, there is no reason to use overly complex methods \\nwhere less complex methods will do better or equally as well (unless you \\nlike to practice your deep learning skills).\\nFinally, while preprocessing methods are useful, BoW-based models \\nare best utilized with document classification. For more advanced NLP \\nproblems, such as sentiment analysis, understanding semantics, and \\nsimilarly abstract problems, BoW likely will not yield the best results.Chapter 3  Working With\\xa0raW text'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 86}, page_content='77© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_4CHAPTER 4\\nTopic Modeling and\\xa0 \\nWord Embeddings\\nNow that you have had an introduction to working with text data, let’s \\ndive into one of the more advanced feature extraction algorithms. To \\naccomplish some of the more difficult problems, it is reasonable for me \\nto introduce you to other techniques to approach NLP problems. We will \\nmove through Word2Vec, Doc2Vec, and GloVe.\\n Topic Model and\\xa0Latent Dirichlet  \\nAllocation (LDA)\\nTopic models are a method of extracting information from bodies of text \\nto see what “topics” occur across all the documents. The intuition is that \\nwe expect certain topics to appear more in relevant documents and not as \\nmuch in irrelevant documents. This might be useful when using the topics \\nwe associate with a document as keywords for better and more intuitive \\nsearch, or when using it for shorthand summarization. Before we apply \\nthis model, let’s talk about how we actually extract topics.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 87}, page_content='78Latent Dirichlet allocation (LDA) is a generative model developed in \\n2003 by David Blei, Andrew Ng, and Michael I.\\xa0Jordan. In their paper, they \\nhighlight the shortcomings of TFIDF . Most notably, TFIDF is unable to \\nunderstand the semantics of words, or the position of a word in text. This \\nled to the rise of LDA.\\xa0LDA is a generative model, meaning that it outputs \\nall the possible outcomes for a given phenomenon. Mathematically, we \\ncan describe the assumptions as follows:\\n 1. Choose N~Poisson (ξ) (a sequence of N words within \\na document have a Poisson distribution)\\n 2. Choose θ~Dir(α) (a parameter θ has a Dirichlet \\ndistribution)\\n 3. For each of the N words ( wn):\\n• Choose topic zn~Multinomial (θ) (Each topic zn has \\na multinomial distribution.)\\n• Choose wn from p (wn |\\xa0zn, β), a multinomial \\nprobability conditional on topic zn. (Each topic is \\nrepresented as a distribution over words, where a \\nprobability is generated from the probability of the \\nnth word, conditional upon the topic as well as β \\nwhere βij\\xa0=\\xa0p(wj\\xa0=\\xa01| zi\\xa0=\\xa01) with dimensions k\\xa0x\\xa0V . )\\n β = probability of a given word, V = number of \\nwords in the vocabulary, k = the dimensionality of \\nthe Dirichlet distribution, θ = the random variable \\nsampled from the probability simplex.\\nLet’s discuss some of the distributions utilized in these assumptions. \\nThe Poisson distribution represents events that occur in a fixed time or \\nspace and at a constant rate, independently of the time since the last event. \\nAn example of this distribution is a model of the number of people who \\ncall a pizzeria for delivery during a given period of time. The multinomial Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 88}, page_content='79distribution is the k-outcome generalization of the binomial distribution; \\nin other words, the same concept as the binomial distribution but \\nexpanded to cases where there are more than two outcomes.\\nFinally, the Dirichlet distribution is a generalization of the beta \\ndistribution, but expanded to handle multivariate data. The beta \\ndistribution is a distribution of probabilities.\\nLDA assumes that (1) words are generated from topics, which have \\nfixed conditional distributions, and (2) that the topics within a document \\nare infinitely exchangeable, which means that the joint probability \\ndistribution of these topics is not affected  by the order in which they are \\nrepresented. Reviewing statements 1 and 2 allows us to state that words \\nwithin a topic are not  infinitely exchangeable.\\nLet’s discuss parameter θ (drawn from the Dirichlet distribution), \\nwhich the dimensionality of the distribution, k, is utilized. We assume that \\nk is known and fixed, and that k-dimensional Dirichlet random variable θ \\ncan take any values in the ( k\\xa0−\\xa01) probability simplex. Here, we define the \\nprobability simplex as the area of the distribution that we draw the random \\nvariable from, graphically represented as a multidimensional triangle with \\nk + 1 vertices. The probability distribution on the simplex itself can be \\nrepresented as follows:\\n pik\\ni\\nik\\nik qaa\\naqqaa|() =()\\n()æ\\nèç\\nççö\\nø÷\\n÷÷¼=\\n=-- å\\nÕG1\\n111111\\nG,,\\n (4.1)\\nα = k-vector of positive real valued numbers. Γ(x) = gamma function.\\nSubsequently, we define the joint distribution of a mixture of topics,  \\nas follows:\\n pp pz pw z\\nnN\\nnn n (, ,| (| || qa bq aq b zw ,) ), = () ()\\n=Õ\\n1  (4.2)Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 89}, page_content='80Therefore, a given sequence of words and topics must have the \\nfollowing form:\\n pp pz pw z\\nnN\\nnn n wz,( |( | () =ò()æ\\nèçö\\nø÷\\n=Õ qq\\n1)\\n (4.3)\\nIn the LDA paper, the authors provide a useful illustration for \\nEquations 4.1, 4.2, and 4.3, as shown in Figure\\xa0 4-1.\\nThe example given in the LDA paper describes Figure\\xa0 4-1 as an \\nillustration of a topic simplex inside a word simplex comprised of three \\nwords. Each of the simplex points represent a given word and topic, \\nrespectively.\\nFigure 4-1.  Topic and word simplexesChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 90}, page_content=\"81Before we complete our discussion on the theory behind LDA, let’s \\nre-create all the work in Python. Thankfully, scikit-learn provides an \\nimplementation of LDA that we will utilize in the upcoming example.\\n Topic Modeling with\\xa0LDA on\\xa0Movie Review Data\\nNext, we look at the same movie review data that we used in our document \\nclassification example. The following is an example of some of the code \\nthat we will utilize to first create out topic model. We’ll start with an \\nimplementation in sklearn.\\ndef create_topic_model(model, n_topics=10, max_iter=5, min_\\ndf=10, max_df=300, stop_words='english', token_pattern=r'\\\\w+'):\\n    print(model + ' topic model: \\\\n')\\n    data = load_data()[0]\\n    if model == 'tf':\\n         feature_extractor = CountVectorizer(min_df=min_df, max_\\ndf=max_df, stop_words=stop_words, token_pattern=r'\\\\w+')\\n    else:\\n         feature_extractor = TfidfVectorizer(min_df=min_df, max_\\ndf=max_df, stop_words=stop_words, token_pattern=r'\\\\w+')\\n    processed_data = feature_extractor.fit_transform(data)\\nWe load the movie reviews that we used in Chapter 3 for the \\nclassification problem. In this example, we will imagine that we want to \\nmake a topic model for a given number of movie reviews.\\nNote  We are importing the load_data()  function from a previous \\nfile. to execute the lda_demo.py file , use a relative import \\nfrom the code_applied_nlp_python  directory, and execute the \\nfollowing command: 'python –m chapter4.topic_modeling'Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 91}, page_content=\"82We load the data in with our function and then prepare it for input to \\nthe LDA fit_transform()  method. Like other NLP problems, we cannot \\nput raw text into any of our algorithms; we must always preprocess it in \\nsome form. However, for producing a topic model, we will utilize both the \\nterm frequency and the TFIDF algorithms, but mainly to compare results.\\nLet’s move through the rest of the function.\\n lda_model = LatentDirichletAllocation(n_topics=n_topics, \\nlearning_method='online', learning_offset=50., max_iter=max_\\niter, verbose=1)\\nlda_model.fit(processed_data)\\ntf_features = feature_extractor.get_feature_names()\\n print_topics(model=lda_model, feature_names=tf_features, n_top_\\nwords=n_top_words)\\nWhen we execute the following function, we get this as our output:\\ntf topic model:\\nTopic #0: libby fugitive douglas sarah jones lee detective \\ndouble innocent talk\\nTopic #1: beatty joe hanks ryan crystal niro fox mail  \\nkathleen shop\\nTopic #2: wars phantom lucas effects menace neeson jedi anakin \\nspecial computer\\nTopic #3: willis mercury simon rising jackal bruce ray lynch \\nbaseball hughes\\nTopic #4: godzilla broderick redman bvoice kim michael \\nbloomington mission space york\\nTopic #5: planet apes joe sci fi space ape alien gorilla newman\\nTopic #6: d american fun guy family woman day ll james bit\\nTopic #7: bond brosnan bottle message blake theresa pierce \\ntomorrow dies crownChapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 92}, page_content='83Topic #8: van spielberg amistad gibson en home american kevin \\nending sense\\nTopic #9: scream 2 wild williamson horror smith kevin arquette \\nsidney finn\\nBeing that this is movie data, we can see that the topics refer to both the \\nmovie and the context surrounding it. For example, topic #4 lists “Godzilla” \\n(ostensibly a character) and “Broderick” (ostensibly an actor). We can also \\nproduce topic models utilizing other feature extraction methods.\\nNow let’s look at the results of the topic model when we use the TFIDF \\nfeature extractor.\\ntfidf topic model:\\nTopic #0: libby driver jacket attending terrorists tends finn \\ndoom tough parodies\\nTopic #1: godzilla beatty douglas arthur christ technology \\nburns jesus york cases\\nTopic #2: wars lucas episode menace jar niro jedi darth anakin \\nphantom\\nTopic #3: sub theron genre keaton cooper victor rita irene \\ndating rules\\nTopic #4: midnight kim stiller mulan spice newman disney junkie \\ntroopers strange\\nTopic #5: clooney palma kevin pacino snake home toy woody \\npfeiffer space\\nTopic #6: anna disney jude carpenter men wrong siege lee king \\nfamily\\nTopic #7: scream got mail bond hanks book performances summer \\ncute dewey\\nTopic #8: en van z n er met reese die fallen lou\\nTopic #9: family american effects home guy woman michael \\noriginal 10 jamesChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 93}, page_content=\"84There are similar results, although we get slightly different results \\nfor some of the topics. In some ways, the TFIDF model can be less \\ninterpretable than the term-frequency model.\\nBefore we move forward, let’s discuss how to utilize the LDA model \\nwith a new package. Gensim is a machine learning library that is heavily \\nfocused on applying machine learning and deep learning to NLP tasks. \\nThe following is code that utilizes this package in the gensim_topic_\\nmodel()  function:\\ndef gensim_topic_model():\\n    def remove_stop_words(text): (1)\\n        word_tokens = word_tokenize(text.lower())\\n         word_tokens = [word for word in word_tokens if word not \\nin stop_words and re.match('[a-zA-Z\\\\-][a-zA-Z\\\\-]{2,}', \\nword)]\\n        return word_tokens\\n    data = load_data()[0]\\n     cleaned_data = [remove_stop_words(data[i]) for i in \\nrange(0, len(data))]\\nWhen us ing this package, the Gensim LDA implementation expects a \\ndifferent input than the gensim implementation, although it still requires \\npreprocessing. When looking at function, we have to remove stop words \\nusing a proprietary function, as we did earlier in Chapter 3. In addition to \\nthis, we should be mindful to remove words that appear too frequently, \\nand not frequently enough. Thankfully, Gensim provides a method within \\nthe corpora.Dictionary()  function to do this, as shown here:\\ndictionary = gensim.corpora.Dictionary(cleaned_data)\\ndictionary.filter_extremes(no_below=100, no_above=300)\\ncorpus = [dictionary.doc2bow(text) for text in  cleaned_data]\\n lda_model = models.LdaModel(corpus=corpus, num_topics=n_topics, \\nid2word=dictionary, verbose=1)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 94}, page_content='85Similar to the scikit-learn method, we can filter objects based on \\ndocument frequency. The preprocessing steps we are taking here are \\nslightly different than those present in the sklearn_topic_model()  \\nfunction, which will become central to our discussion at the end of this \\nsection. Similar to what you saw before, what seems like a minor change in \\npreprocessing steps can lead to a drastically different outcome.\\nWe execute the gensim_topic_model()  function and get the following \\nresult:\\nGensim LDA implemenation:\\nTopic #0: 0.116*\"movie\" + 0.057*\"people\" + 0.051*\"like\" + \\n0.049*\"good\" + 0.041*\"well\" + 0.038*\"film\" + 0.037*\"one\" + \\n0.037*\"story\" + 0.033*\"great\" + 0.028*\"new\"\\nTopic #1: 0.106*\"one\" + 0.063*\"movie\" + 0.044*\"like\" + \\n0.043*\"see\" + 0.041*\"much\" + 0.038*\"story\" + 0.033*\"little\" + \\n0.032*\"good\" + 0.032*\"way\" + 0.032*\"get\"\\nTopic #2: 0.154*\"film\" + 0.060*\"one\" + 0.047*\"like\" + \\n0.039*\"movie\" + 0.037*\"time\" + 0.032*\"characters\" + \\n0.031*\"scene\" + 0.028*\"good\" + 0.028*\"make\" + 0.027*\"little\"\\nTopic #3: 0.096*\"film\" + 0.076*\"one\" + 0.060*\"even\" + \\n0.053*\"like\" + 0.051*\"movie\" + 0.040*\"good\" + 0.036*\"time\" + \\n0.033*\"get\" + 0.030*\"would\" + 0.028*\"way\"\\nTopic #4: 0.079*\"film\" + 0.068*\"plot\" + 0.058*\"one\" + \\n0.057*\"would\" + 0.049*\"like\" + 0.039*\"two\" + 0.038*\"movie\" + \\n0.036*\"story\" + 0.035*\"scenes\" + 0.033*\"much\"\\nTopic #5: 0.136*\"film\" + 0.067*\"movie\" + 0.064*\"one\" + \\n0.039*\"first\" + 0.037*\"even\" + 0.037*\"would\" + 0.036*\"time\" + \\n0.035*\"also\" + 0.029*\"good\" + 0.027*\"like\"\\nTopic #6: 0.082*\"movie\" + 0.072*\"get\" + 0.068*\"film\" + \\n0.059*\"one\" + 0.046*\"like\" + 0.036*\"even\" + 0.035*\"know\" + \\n0.027*\"much\" + 0.027*\"way\" + 0.026*\"story\"Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 95}, page_content='86Topic #7: 0.131*\"movie\" + 0.097*\"film\" + 0.061*\"like\" + \\n0.045*\"one\" + 0.032*\"good\" + 0.029*\"films\" + 0.027*\"see\" + \\n0.027*\"bad\" + 0.025*\"would\" + 0.025*\"even\"\\nTopic #8: 0.139*\"film\" + 0.060*\"movie\" + 0.052*\"like\" + \\n0.044*\"story\" + 0.043*\"life\" + 0.043*\"could\" + 0.041*\"much\" + \\n0.032*\"well\" + 0.031*\"also\" + 0.030*\"time\"\\nTopic #9: 0.116*\"film\" + 0.091*\"one\" + 0.059*\"movie\" + \\n0.035*\"two\" + 0.029*\"character\" + 0.029*\"great\" + 0.027*\"like\" \\n+ 0.026*\"also\" + 0.026*\"story\" + 0.026*\"life\"\\nSo far, the results from the scikit-learn implementation of LDA using \\nterm frequency as our feature extractor has given the most interpretable \\nresults. Most of the results are homogenous, which might not lead to much \\ndifferentiation, making the results from this less useful.\\nUsing this same data set, let’s utilize another topic extraction model.\\n Non-Negative Matrix Factorization (NMF)\\nNon-negative matrix factorization (NMF) is an algorithm that takes a \\nmatrix and returns two matrices that have no non-negative elements. NMF \\nis closely related to matrix factorization, except NMF only receives non-  \\nnegative values (0 and anything above 0).\\nWe want to utilize NMF rather than another type of matrix factorization \\nbecause we need positive coefficients, as is the case when using LDA.\\xa0We \\ncan describe the process with the following mathematical formula:\\n VWH= \\nThe matrix, V , is the original matrix that we input to the data. The two \\nmatrices that we output are W and H.\\xa0In this example, let’s assume matrix \\nV has 1000 rows and 200 columns. Each row represents a word and each \\ncolumn represents a document. Therefore, we have a 1000-word vocabulary \\nfeatured across 200 documents. As it relates to the preceding equation, V is an \\nm×n matrix, W is an m×p matrix, and H is a p ×n matrix. W is a features matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 96}, page_content=\"87Let’s say that we would like to find five features such that we generate \\nmatrix W with 1000 rows and 5 columns. Matrix H subsequently has a \\nshape equivalent to 5 rows and 200 columns. When we perform matrix \\nmultiplication on W and H, we yield matrix V with 1000 rows and 200 \\ncolumns, equivalent to the dimensionality described earlier. We consider \\nthat each document is built from a number of hidden features, which NMF \\nwould therefore generate. The following is the scikit-learn implementation \\nof NMF that we will utilize for this example:\\ndef nmf_topic_model():\\n     def create_topic_model(model, n_topics=10, max_iter=5,  \\nmin_df=10,\\n                            max_df=300, stop_words='english', \\ntoken_pattern=r'\\\\w+'):\\n        print(model + ' NMF topic model: ')\\n        data = load_data()[0]\\n        if model == 'tf':\\n             feature_extractor = CountVectorizer(min_df=min_df, \\nmax_df=max_df,\\n                                  stop_words=stop_words,  \\ntoken_pattern=token_pattern)\\n        else:\\n             feature_extractor = TfidfVectorizer(min_df=min_df, \\nmax_df=max_df,\\n                                  stop_words=stop_words,  \\ntoken_pattern=token_pattern)\\n        processed_data = feature_extractor.fit_transform(data)\\n         nmf_model = NMF(n_components=n_components,  \\nmax_iter=max_iter)\\n        nmf_model.fit(processed_data)\\n        tf_features = feature_extractor.get_feature_names()Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 97}, page_content=\"88         print_topics(model=nmf_model, feature_names=tf_\\nfeatures, n_top_words=n_topics)\\n    create_topic_model(model='tf')\\nWe invoke the NMF topic extraction in virtually the same manner that \\nwe invoke the LDA topic extraction model. Let’s look at the output of both \\nthe term frequency preprocessed data and the TFIDF preprocessed data.\\ntf NMF topic model:\\nTopic #0: family guy original michael sex wife woman r men play\\nTopic #1: jackie tarantino brown ordell robert grier fiction \\npulp jackson michael\\nTopic #2: jackie hong drunken master fu kung chan arts martial ii\\nTopic #3: scream 2 williamson horror sequel mr killer sidney \\nkevin slasher\\nTopic #4: webb jack girl gives woman ll male killed sir talking\\nTopic #5: musical musicals jesus death parker singing woman \\nnation rise alan\\nTopic #6: bulworth beatty jack political stanton black warren \\nprimary instead american\\nTopic #7: godzilla effects special emmerich star york computer \\nmonster city nick\\nTopic #8: rock kiss city detroit got music tickets band \\nsoundtrack trying\\nTopic #9: frank chicken run shannon ca mun html sullivan \\nparticularly history\\nThe following is the TFIDF NMF topic model:\\nTopic #0: 10 woman sense james sex wife guy school day ending\\nTopic #1: scream horror williamson 2 sidney craven stab killer \\narquette 3\\nTopic #2: wars phantom jedi lucas menace anakin jar effects \\ndarth gonChapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 98}, page_content='89Topic #3: space deep alien ship armageddon harry effects \\ngodzilla impact aliens\\nTopic #4: disney mulan animated joe voice toy animation apes \\nmermaid gorilla\\nTopic #5: van amistad spielberg beatty cinque political slavery \\nen slave hopkins\\nTopic #6: carpenter ott ejohnsonott nuvo subscribe reviews \\njohnson net mail e\\nTopic #7: hanks joe ryan kathleen mail shop online fox tom meg\\nTopic #8: simon sandler mercury adam rising willis wedding \\nvincent kevin julian\\nTopic #9: murphy lawrence martin eddie ricky kit robbins miles \\nclaude police\\nBefore we evaluate the results and have a more thorough discussion \\non both methods, let’s focus on visualizing the results. In the preceding \\nexample, we’ve reasonably reduced the complexity so that users can \\nassess the different topics within the analyzed documents. However, this \\nisn’t as helpful when we want to look at larger amounts of data and make \\ninferences from this topic model relatively quickly.\\nWe’ll begin with what I believe is a useful plot, supplied by pyLDAvis. \\nThis software is extremely useful and works relatively easily when used \\nwith a Jupyter notebook, which are excellent for code visualization and \\nresults presentation. It is common to utilize a Jupyter notebook when using \\na virtual machine instance from either Amazon Web Services (AWS) or \\nGoogle Cloud.\\nNote  For those of you who have not worked with google Cloud   \\nor aWs, i recommend these tutorials:  google Compute engine:   \\nwww.youtube.com/watch?v=zzMCKv1g5z0  aWs: www.youtube.\\ncom/watch?v=q1vVedHbkAYChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 99}, page_content='90Set up an instance and start a Jupyter notebook. We will make some \\nminor adjustments for running this on your local machine to running it \\nin the cloud. In this example, the scikit-learn implementations—given the \\npreprocessing algorithms provided—make gleaning interpretable topics \\nmuch easier than the Gensim model. Although it gives more flexibility and \\nhas a lot of features, Gensim requires you to fine-tune the preprocessing steps \\nfrom scratch. If you have the time to build results from scratch, this is not a \\nproblem; however, keep this in mind when building your own application, \\nand consider the difficulties of having to use this method in Gensim.\\nIn this demo, NMF and LDA typically give similar results; however, the \\nchoice of one model vs. the other is often relative to the way we conceive \\nof the data. LDA assumes that topics are infinitely exchangeable, but the \\nwords within a topic are not. As such, if we are not concerned about the \\ntopic probability per document remaining fixed (it assumedly would not \\nbe, as not all documents contain the same topics across large corpuses), \\nLDA is a better choice. NMF might be a better choice if we have a heavy \\ndegree of certainty with respect to fixed topic probability and the data \\nset is considerably smaller. Again, these statements should be taken in \\nconsideration when evaluating the results of the respective topic models, \\nas with all machine learning problems.\\nLet’s discuss a more advanced modeling technique that plays a role \\nin sentiment analysis (in addition to more advanced NLP tasks): word \\nembeddings. We begin by discussing a body of algorithms: Word2Vec.\\n Word2Vec\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean \\nare credited with creating Word2Vec in 2014 while working at Google. \\nWord2Vec represents a significant step forward in NLP-related tasks, as it \\nprovides a method for finding vector representations of words and phrases, \\nand it can be expanded as much as the documents.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 100}, page_content='91First, let’s examine the Skip-Gram model, which is a shallow neural \\nnetwork whose objective is to predict a word in a sequence based on the \\nwords around it. Let’s take the following sequence of training words:\\n ww wwT =¼()12,, ,  \\nThe objective function is the average log probability, represented by \\nthe following:\\n 1\\n10Tpw w\\ntT\\ncj cjtj t\\n=- ££ ¹+ åå\\n,log)(|\\n \\n pw wp wwvv\\nvvtj tO IwT\\nwi\\nwW\\nwT\\nwiO(| (|+\\n===¢()\\n¢() å))exp\\nexp1  \\nc = size of the training context, T = the total number of training words,  \\nt = index position of the current word, j = the window that determines \\nwhich word in the sequence we are looking, wt = center word of the \\nsequence, and W= number of words in the vocabulary.\\nBefore we move on, it’s important that you understand the formula and \\nhow it explains what the model does. An n-gram is a continuous grouping \\nof n words. A Skip-Gram is a generalization of an n-gram, such that we \\nhave groupings of words, but they no longer need to be continuous; that \\nis, we can skip words to create Skip-Grams. Mathematically, we typically \\ndefine k-skip-n-grams as follows:\\n {, ,, | ww wi ikii i\\njn\\njjn 12\\n11 ¼- <\\n=-å }\\n \\nLet’s assume the following is an input to the k-skip-n-gram model:\\n“The cat down the street”\\nLet’s also assume that we are seeking to create a 2-skip-bi-gram model. \\nAs such, the training examples are as follows:\\n• “The, cat” , “The, down” , “The, the” , “The, street”\\n• “cat, the” , “cat, down” , “cat, the” , “cat, street”Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 101}, page_content='92• “down, the” , “down, cat” , “down, the” , “down, street”\\n• “the, The” , “the, cat” , “the, down” , “the, street”\\n• “street, The” , “street, cat” , “street, down” , “street, the”\\nSo now you understand how the input data is represented as words.\\nLet’s discuss how we represent these words with respect to a neural \\nnetwork. The input layer for the Skip-Gram model is a one-hot encoded \\nvector with W components. In other words, every element of the vector is \\nrepresentative of a word in the vocabulary. The Skip-Gram architecture is \\ngraphically represented in Figure\\xa0 4-2.\\nFigure 4-2.  Skip-Gram model architectureChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 102}, page_content='93The goal of the neural network is to predict the word that has the \\nhighest probability of coming next in the input sequence. This is precisely \\nwhy we want to use softmax, and ultimately how you intuitively understand \\nthe formula. We want to predict the word that is most probable given the \\ninput of the words, and we are calculating this probability based on the \\nentirety of the input and output sequences observed by the neural network.\\nBe that as it may, we have a minor problem. Softmax computation \\nscales proportionally to the input size, which bodes poorly for this \\nproblem because accurate results will likely require large vocabulary sizes \\nfor training data. Hence, it is often suggested that we use an alternative \\nmethod. One of the methods often referenced is negative sampling. \\nNegative sampling is defined in the following equation:\\n log~ log ss¢() + () -() éëùû\\n=å vv Pw vvwoT\\nwi\\nik\\nwi nw iT\\nwi\\n1\\uf045 \\nNegative sampling achieves a cheaper computation than the softmax \\nactivation function by approximating its output. More precisely, we are \\nonly going to change K number of weights in the word embedding rather \\nthan computing them all. The Word2Vec paper suggests to sample  \\nwith 5 to 20 words in smaller data sets, but 2 to 5 words in larger data sets \\ncan achieve positive results.\\nBeyond the training of the word embedding, what are we actually \\ngoing to use it for? Unlike many neural networks, the main objective is not \\nnecessarily to use it for the purpose of prediction, but rather to obtain the \\ntrained hidden layer weight matrix. The hidden layer weight matrix is our \\ntrained word embedding . Once this hidden layer is trained, certain words \\ncluster in areas of vector space, where they share similar contexts.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 103}, page_content='94 Example Problem 4.2: Training a\\xa0Word \\nEmbedding (Skip-Gram)\\nLet’s display the power of Word2Vec by working through a demo example, \\nin Gensim and in TensorFlow. The following is some of the code that begins \\nour implementation of the TensorFlow Word2Vec Skip-Gram model:\\ndef remove_non_ascii(text):\\n    return \".join([word for word in text if ord(word) < 128])\\ndef load_data(max_pages=100):\\n    return_string = StringIO()\\n     device = TextConverter(PDFResourceManager(), return_string, \\ncodec=\\'utf-8\\', laparams=LAParams())\\n     interpreter = PDFPageInterpreter(PDFResourceManager(), \\ndevice=device)\\n     filepath = file(\\'/Users/tawehbeysolow/Desktop/applied_nlp_\\npython/datasets/economics_textbook.pdf\\', \\'rb\\')\\n     for page in PDFPage.get_pages(filepath, set(), \\nmaxpages=max_pages, caching=True, check_extractable=True):\\n        interpreter.process_page(page)\\n    text_data = return_string.getvalue()\\n    filepath.close(), device.close(), return_string.close()\\n    return remove_non_ascii(text_data)\\nFor our example problem, we will utilize the PDFMiner Python module. \\nFor those of you who often parse data in different forms, this package is \\nhighly recommended. PDF data is notorious in parsing, as it is often filled \\nwith images and metadata that makes preprocessing the data a hassle. \\nThankfully, PDFMiner takes care of most of the heavy lifting, making our \\nprimary concerns only cleaning out stop words, grammatical characters, \\nand other preprocessing steps, which are relatively straightforward. For this \\nproblem, we will read data from an economics textbook.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 104}, page_content=\"95def gensim_preprocess_data():\\n    data = load_data()\\n    sentences = sent_tokenize(data)\\n     tokenized_sentences = list([word_tokenize(sentence) for \\nsentence in sentences])\\n    for i in range(0, len(tokenized_sentences)):\\n         tokenized_sentences[i] = [word for word in tokenized_\\nsentences[i] if word not in punctuation]\\n    return tokenized_sentences\\nWe now move to tokenizing the data based on sentences. Do not \\nremove punctuation before this step . The NLTK sentence tokenizer relies on \\npunctuation to determine where to split data based on sentences. If this is \\nremoved, it can cause you to debug something rather trivial. Regardless, \\nthe next format the data should take is that of a list, where every entry is a \\nsentence whose words are tokenized, such that the words appear as follows:\\n [['This', 'text', 'adapted', 'The', 'Saylor', 'Foundation', \\n'Creative', 'Commons', 'Attribution-NonCommercial-ShareAlike', \\n'3.0', 'License', 'without', 'attribution', 'requested', \\n'works', 'original', 'creator', 'licensee'], ['Saylor', \\n'URL', 'http', '//www.saylor.org/books', 'Saylor.org', '1', \\n'Preface', 'We', 'written', 'fundamentally', 'different', \\n'text', 'principles', 'economics', 'based', 'two', 'premises', \\n'1'], ['Students', 'motivated', 'study', 'economics', 'see', \\n'relates', 'lives'], ['2'], ['Students', 'learn', 'best', \\n'inductive', 'approach', 'first', 'confronted', 'question', \\n'led', 'process', 'answer', 'question'], ['The', 'intended', \\n'audience', 'textbook', 'first-year', 'undergraduates', \\n'taking', 'courses', 'principles', 'macroeconomics', \\n'microeconomics'], ['Many', 'may', 'never', 'take', \\n'another', 'economics', 'course'], ['We', 'aim', 'increase', \\n'economic', 'literacy', 'developing', 'aptitude', 'economic', Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 105}, page_content=\"96'thinking', 'presenting', 'key', 'insights', 'economics', \\n'every', 'educated', 'individual', 'know'], ['Applications', \\n'ahead', 'Theory', 'We', 'present', 'theory', 'standard', \\n'books', 'principles', 'economics'], ['But', 'beginning', \\n'applications', 'also', 'show', 'students', 'theory', \\n'needed'], ['We', 'take', 'kind', 'material', 'authors', 'put', \\n'applications', 'boxes', 'place', 'heart', 'book'], ['Each', \\n'chapter', 'built', 'around', 'particular', 'business', \\n'policy', 'application', 'microeconomics', 'minimum', 'wages', \\n'stock', 'exchanges', 'auctions', 'macroeconomics', 'social', \\n'security', 'globalization', 'wealth', 'poverty', 'nations']\\nNow that we have finished the preprocessing of our data, we can work \\nwith the Gensim implementation of the Skip-Gram model.\\ndef gensim_skip_gram():\\n    sentences = gensim_preprocess_data()\\n     skip_gram = Word2Vec(sentences=sentences, window=1,  \\nmin_count=10, sg=1)\\n    word_embedding = skip_gram[skip_gram.wv.vocab] (1)\\nInvoking the Skip-Gram model is relatively straightforward, and the \\ntraining of the model is taken care of for us as well. The training process \\nof a Skip-Gram model mimics that of all neural networks, in that we pass \\nan input through all the layers and then backpropagate the error through \\neach of the respective weights in each layer, updating them until we have \\nreached a loss tolerance threshold or until the maximum number of \\nepochs has been reached. Once the word embedding has been trained, \\nwe obtain the weight matrix by indexing the model with the wv.vocab  \\nattribute of the model itself.\\nNow, let’s discuss visualizing the words as vectors.\\n    pca = PCA(n_components=2)\\n    word_embedding = pca.fit_transform(word_embedding)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 106}, page_content='97    #Plotting results from trained word embedding\\n    plt.scatter(word_embedding[:, 0], word_embedding[:, 1])\\n    word_list = list(skip_gram.wv.vocab)\\n    for i, word in enumerate(word_list):\\n         plt.annotate(word, xy=(word_embedding[i, 0],  \\nword_embedding[i, 1]))\\nWord embeddings are output in dimensions that are difficult to \\nvisualize in their raw formats. As such, we need to find a way to reduce \\nthe dimensionality of this matrix, while also retaining all the variance and \\nattributes of the original data set. A preprocessing method that does this \\nis principal components analysis (PCA). Briefly, PCA transforms a matrix \\nso that it returns an eigen-decomposition called eigenvectors, in addition \\nto eigenvalues. For the sake of showing a two-dimensional plot, we want \\nto create a transformation that yields two  principal components. It is \\nimportant to remember that these principal components are not exactly \\nthe same as the original matrix, but an orthogonal transformation of the \\nword embedding that is related to it. Figure\\xa0 4-3 illustrates the matrix.\\nFigure 4-3.  Skip-Gram word embeddings generated via GensimChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 107}, page_content='98In the vector space, words that are closer to one another appear in \\nsimilar contexts, and words that are further away from each other are more \\ndissimilar in respect to the contexts in which they appear. Cosine similarity \\nis a common method for measuring this. Mathematically, cosine distance \\nis described as follows:\\n cosq()=*AB\\nAB  \\nWe intuitively describe cosine similarity as the sum of the product of \\nall the respective elements of two given vectors, divided by the product of \\ntheir Euclidean norms. Two vectors that have a 0-degree difference yield \\na cosine similarity of 1; whereas two vectors with a 90-degree difference \\nyield a cosine similarity of 0. The following is an example of some of the \\ncosine distances between different word vectors:\\nCosine distance for people   and Saylor\\n -0.10727774727479297\\nCosine distance for URL   and people\\n -0.137377917173043\\nCosine distance for money   and URL\\n -0.03124461706797222\\nCosine distance for What   and money\\n -0.007384979727807199\\nCosine distance for one   and What\\n 0.022940581092187863\\nCosine distance for see   and one\\n 0.05983065381073224\\nCosine distance for economic   and see\\n -0.0530102968258333\\nGensim takes care of some of the uglier aspects of preprocessing the \\ndata. However, it is useful to know how to perform some of these things \\nfrom scratch, so let’s try implementing a word embedding utilizing the \\nsame data, except this time we will do it in TensorFlow.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 108}, page_content='99Let’s walk through a toy implementation to ensure that you are aware \\nof what the model is doing, and then walk through an implementation that \\nis easier to deploy.\\ndef tf_preprocess_data(window_size=window_size):\\n    def one_hot_encoder(index, vocab_size):\\n        vector = np.zeros(vocab_size)\\n        vector[index] = 1\\n        return vector\\n    text_data = load_data()\\n    vocab_size = len(word_tokenize(text_data))\\n    word_dictionary = {}\\n    for index, word in enumerate(word_tokenize(text_data)):\\n        word_dictionary[word] = index\\n    sentences = sent_tokenize(text_data)\\n    tokenized_sentences = list([word_tokenize(sentence) for \\nsentence in sentences])\\n    n_gram_data = []\\nWe must prepare the data slightly differently for TensorFlow than we \\ndid for Gensim. The Gensim Word2Vec method takes care of most of the \\nback-end things for us, but it is worthwhile to implement a simple proof of \\nconcept from scratch and walk through the algorithm.\\nWe begin by making a dictionary that matches a word with an index \\nnumber. This index number forms the position in our one-hot encoded \\ninput and output vectors.\\nLet’s continue preprocessing the data.\\n#Creating word pairs for word2vec model\\n    for sentence in tokenized_sentences:\\n        for index, word in enumerate(sentence):\\n            if word not in punctuation:Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 109}, page_content='100                for _word in sentence[max(index  - window_size, 0):\\n                                       min(index + window_size, \\nlen(sentence)) + 1]:\\n                    if _word != word:\\n                        n_gram_data.append([word, _word])\\nThe preceding section of code effectively creates our n-grams, and \\nultimately simulates how the Skip-Gram model convolves over a sentence \\nin such a way that it can predict the following word with the highest \\nprobability. We then create an m×n matrix, where m is the number of words \\nin our input sequences, and n is the number words in the vocabulary.\\n #One-hot encoding data and creating dataset intrepretable by \\nskip-gram model\\n x, y = np.zeros([len(n_gram_data), vocab_size]), \\nnp.zeros([len(n_gram_data), vocab_size])\\nfor i in range(0, len(n_gram_data)):\\n     x[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i]\\n[0]], vocab_size=vocab_size)\\n     y[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i]\\n[1]], vocab_size=vocab_size)\\nreturn x, y, vocab_size, word_dictionary\\nMoving forward to the function that we will use to construct our Skip-  \\nGram mo del, we begin by loading the data and the vocabulary size and \\nword dictionary. As with other neural network models, we instantiate the \\nplaceholders, variables, and weights. Per the Skip-Gram model diagram \\nshown in Figure\\xa0 4-2, we only need to contain a hidden and an output \\nweight matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 110}, page_content=\"101def tensorflow_word_embedding(learning_rate=learning_rate, \\nembedding_dim=embedding_dim):\\n    x, y, vocab_size, word_dictionary = tf_preprocess_data()\\n    #Defining tensorflow variables and placeholder\\n    X = tf.placeholder(tf.float32, shape=(None, vocab_size))\\n    Y = tf.placeholder(tf.float32, shape=(None, vocab_size))\\n    weights = {'hidden': tf.Variable(tf.random_normal([vocab_\\nsize, embedding_dim])),\\n                'output': tf.Variable(tf.random_\\nnormal([embedding_dim, vocab_size]))}\\n     biases = {'hidden': tf.Variable(tf.random_\\nnormal([embedding_dim])),\\n               'output':  tf.Variable(tf.random_normal([vocab_\\nsize]))}\\n     input_layer = tf.add(tf.matmul(X, weights['hidden']), \\nbiases['hidden'])\\n     output_layer = tf.add(tf.matmul(input_layer, \\nweights['output']), biases['output'])\\nIn C hapter 5, we walk through implementing negative sampling. \\nHowever, because the number of examples that we are using here is relatively \\nminiscule, we can get away with utilizing the regular implementation of \\nsoftmax as provided by TensorFlow. Finally, we execute our graph, as with \\nother TensorFlow models, and observe the results shown in Figure\\xa0 4-4.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 111}, page_content='102Cosine distance for dynamic   and limited\\n 0.4128825113896724\\nCosine distance for four   and dynamic\\n 0.2833843609582811\\nCosine distance for controversial   and four\\n 0.3266445485300576\\nCosine distance for hanging   and controversial\\n 0.37105348488163503\\nCosine distance for worked   and hanging\\n 0.44684699747383416\\nCosine distance for Foundation   and worked\\n 0.3751656692569623\\nFigure 4-4.  Word vectors from toy implementation of Skip-  GramChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 112}, page_content='103Again, the implementations provided here are not final examples of \\nwhat well-trained word embeddings necessarily looks like. We will tackle \\nthat task more specifically in Chapter 5, as data collection is largely the \\nissue that we must discuss in greater detail. However, the Skip-Gram \\nmodel is only one of the word embeddings that we will likely encounter.\\nWe now will continue our discussion by tackling the continuous  \\nbag-  of- words model.\\n Continuous Bag-of-Words (CBoW)\\nSimilar to a Skip-Gram model, a continuous bag-of-words model (CBoW) \\nis training on the objective of predicting a word. Unlike the Skip-Gram \\nmodel, however, we are not trying to predict the next word in a given \\nsequence. Instead, we are trying to predict some center word based on \\nthe context around the target label. Let’s imagine the following input data \\nsentence:\\n“The boy walked to the red house”\\nIn the context of the CBoW model, we could imagine that we would \\nhave an input vector that appeared as follows:\\n“The, boy, to, the, red, house”\\nHere, “walked” is the target that we are trying to predict. Visually, the \\nCBoW model looks like Figure\\xa0 4-5.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 113}, page_content='104Each word in the input is represented in a single one-hot encoded vector . \\nSimilar to the Skip-Gram model, the length of the input vector is equal to \\nthe number of words in the vocabulary. When evaluating our input data, a \\nvalue of “1” is for the words that are present and a “0” is for the words that \\nare not present. In Figure\\xa0 4-5, we are predicting a target word, w_t, based \\non the words w_t-2, w_ t-1, w_ t+1, and w_t+2.\\nWe then perform a weighted sum operation on this input vector with \\nweight and bias matrices that pass these values to the projection layer, \\nwhich is similar to the projection layer featured in the Skip-Gram model. \\nFinally, we predict the class label with another weighted sum operation \\nwith the output weight and bias matrices in addition to utilizing a softmax \\nclassifier. The training method is the same as the one used in the Skip-  \\nGram model.\\nNext, let’s work with a short example utilizing Gensim.\\nFigure 4-5.  CBoW model representationChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 114}, page_content='105 Example Problem 4.2: Training a\\xa0Word \\nEmbedding (CBoW)\\nThe Gensim implementation of CBoW requires that only a single \\nparameter is changed, as shown here:\\ncbow = Word2Vec(sentences=sentences, window=skip_gram_window_\\nsize, min_count=10, sg=0)\\nWe invoke this method and observe the results in the same manner \\nthat we did for the Skip-Gram model. Figure\\xa0 4-6 shows the results.\\nFigure 4-6.  CBoW wor d embedding visualizationChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 115}, page_content='106 Global Vectors for\\xa0Word  \\nRepresentation (GloVe)\\nGloVe is a contemporary and advanced method of vector representation \\nof words. In 2014, Jeffrey Pennington, Richard Socher, and Christopher \\nManning wrote a paper in which they describe GloVe. This type of \\nword embedding is an improvement over both matrix factorization–\\nbased representations of words, as well as the Skip-gram model. Matrix \\nfactorization–based methods of word representation are not particularly \\ngood at representing words with respect to their analogous nature. \\nHowever, Skip-Gram and CBoW train on isolated windows of text and do \\nnot utilize the same information that a matrix-based factorization method \\ndoes. Specifically, when we use LDA to create a topic model, we have \\nto preprocess the text in a way that encodes each word with statistical \\ninformation that represents the word in the context of the whole text. With \\nSkip-Gram and CBoW, the one-hot encoded vector doesn’t capture that \\nsame type of complexity.\\nGloVe specifically trains on “global word-to-word co-occurrence \\ncounts. ” Co-occurrence is the instance of two words appearing in a specific \\norder alongside one another. By global , I mean the co-occurrence counts \\nwith respect to all documents in the corpus that we are analyzing. In this \\nsense, GloVe is utilizing a bit of the intuition behind both models to try and \\novercome the respective shortcomings of the aforementioned alternatives.\\nLet’s begin by defining a co-occurrence matrix, X.\\xa0Each entry in the \\nmatrix represents the co-occurrence count of two specific words. More \\nspecifically, X i, j represents the number of times word j appears in the \\ncontext of word i. The following notation is also worth noting:\\n Xi=å\\nkikX, (4.4)\\n PP jiX\\nXijij\\ni,,=() = |\\n (4.5)Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 116}, page_content='107Equation 4.4 is defined as the number of times any word appears in the \\ncontext of the word I.\\xa0Equation 4.5 is the probability of a word j given word \\ni. We define this probability as the co-occurrence account of word j appears \\nin the context of the word “I” with the total co-occurrence counts of word i.\\nI suggest that the model should evaluate the ratios of co-occurrence \\nprobabilities, which we define as follows:\\n Fw wwP\\nPij kik\\njk,,\\uf025 () =\\n (4.6)\\nw\\xa0∈\\xa0ℝd= word vectors and \\uf025wkdÎ\\uf052 = context vectors, F = exp( x)∗\\nYou should observe that our definition of F has an asterisk above \\nit, particularly to indicate the fact that the value of F can be a multitude \\nof things; however, we often derive it to be the preceding definition. \\nThe purpose of F is to encode the value yielded from the co-occurrence \\nprobabilities into the word embedding.\\nThe following functions derive the target label and the error function \\nwe use to train the GloVe word embedding:\\n Fw wPiT\\nki k\\uf025() =, (4.7)\\n ww bb XiT\\nji ji j\\uf025\\uf025 ++ -log, (4.8)\\n Jf Xw wb bX\\nijV\\nij iT\\nji ji j = () ++ - ()\\n=å\\n,,, log\\n12\\uf025\\uf025\\n (4.9)\\nWhere f\\xa0(Xij) = weighting function\\nAs detailed in the GloVe paper, the weighting function should obey a \\nfew rules. Foremost, if f is a continuous function, it should vanish as x\\xa0→\\xa00, \\nf\\xa0(x) should be non-decreasing, and f(x) should be relatively small for large \\nvalues of x. These rules are to ensure that rare or frequent co-occurrence \\nvalues are not overweighted in the training of the word embedding. Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 117}, page_content='108Although the weighting function can be altered, the GloVe paper suggests \\nthe following equation:\\n fxx\\nxifxx\\notherwis emm()=æ\\nèçö\\nø÷ <ì\\níï\\nîïa\\n1  \\nxm = maximum value of x, fixed to 100. The weighting function yields \\nthe values shown in Figure\\xa0 4-7 with respect to the x value.\\nNow that we have reviewed the model, it is useful for you to \\nunderstand how to use pretrained word embeddings, particularly since not \\neveryone will have the time or the ability to train these embeddings from \\nscratch due to the difficult nature of acquiring all of this data. Although \\nthere is not necessarily one predetermined place to get a word embedding \\nfrom, you should be aware of the following GitHub repository that contains \\nFigure 4-7.  Weighting function for GloVeChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 118}, page_content=\"109the files for multitudes of word embeddings:  https://github.com/3Top/\\nword2vec-api#where-to-get-a-pretrained-models . You can feel free to \\nexperiment and deploy these word embeddings for different tasks where \\nthey see fit.\\nFor this example, we will use a GloVe word embedding that contains \\n6 billion words and 50 features. This word embedding was trained from \\ndata taken from Wikipedia and has a vocabulary containing 400,000 words. \\nNow, let’s begin with the code, shown here:\\ndef load_embedding(embedding_path='/path/to/glove.6B.50D.txt'):\\n    vocabulary, embedding = [], []\\n    for line in open(embedding_path, 'rb').readlines():\\n        row = line.strip().split(' ')\\n        vocabulary.append(row[0]), embedding.append(row[1:])\\n     vocabulary_length, embedding_dim = len(vocabulary), \\nlen(embedding[0])\\n    return vocabulary, np.asmatrix(embedding), vocabulary_\\nlength, embedding_dim\\nWe begin this problem by loading the word embeddings using the \\nnative open()  function to read the file line by line. Each line in the file \\nstarts with a word in the vocabulary, and the subsequent entries in that \\nline represent the values within each of that word’s vector. We iterate \\nthrough all the lines in the file, appending the word and the word vector to \\ntheir respective arrays. As such, we are able to create a list of words within \\na vocabulary and reconstruct the word embeddings from a .txt  file. This \\ntrained embedding should look like Figure\\xa0 4-8.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 119}, page_content='110Figure 4-8 shows the representation of the first 50 words in the \\nvocabulary when we look at the two principal components yielded from \\nthe transformation of our word embedding. Examples of words that seem \\nto appear in similar contexts are had  and has, and , and as, in addition to \\nhis and he. When comparing the cosine similarities of other words in the \\nvocabulary, we observe the following.\\nCosine Similarity Between so and u.s.: 0.5606769548631282\\nCosine Similarity Between them and so: 0.8815159254335486\\nCosine Similarity Between what and them: 0.8077565084355354\\nCosine Similarity Between him and what: 0.7972281857691554\\nCosine Similarity Between united and him: 0.5374600664967559\\nCosine Similarity Between during and united: 0.6205250403136882\\nCosine Similarity Between before and during: 0.8565694276984954\\nCosine Similarity Between may and before: 0.7855322363492923\\nCosine Similarity Between since and may: 0.7821437532357596\\nFigure 4-8.  GloV e pretrained embeddingChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 120}, page_content='111 Example Problem 4.4: Using Trained Word \\nEmbeddings with\\xa0LSTMs\\nNow that we have visually inspected the word embedding, let’s focus \\non how to use trained embeddings with a deep learning algorithm. \\nLet’s imagine that we would like to include the following paragraph as \\nadditional training data for our word embedding.\\nsample_text = \"\\'Living in different places has been the\\ngreatest experience that I have had in my life. It has allowed\\nme to understand people from different walks of life, as well \\nas to question some of my own biases I have had with respect\\nto people who did not grow up as I did. If possible, everyone\\nshould take an opportunity to travel somewhere separate from \\nwhere they grew up.\"\\'.replace(\\'\\\\n\\', \")\\nWith our sample data assigned to a variable, let’s begin by performing \\nsome of the same preprocessing steps that we have familiarized ourselves \\nwith, exemplified by the following body of code:\\ndef sample_text_dictionary(data=_sample_text):\\n     count, dictionary = collections.Counter(data).most_\\ncommon(), {} #creates list of word/count pairs;\\n    for word, _ in count:\\n         dictionary[word] = len(dictionary) #len(dictionary) \\nincreases each iteration\\n         reverse_dictionary = dict(zip(dictionary.values(), \\ndictionary.keys()))\\n     dictionary_list = sorted(dictionary.items(),  \\nkey = lambda x : x[1])\\n    return dictionary, reverse_dictionary, dictionary_listChapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 121}, page_content='112We start by using a remove_stop_words()  function, a redefinition of \\na sample preprocessing text algorithm defined in Chapter 3 that removes \\nstop words from relatively straightforward sample data. When you are \\nusing data that isn’t as clean as this sample data, I recommend that \\nyou preprocess the data in a manner similar to what you did using the \\neconomics textbook or War and Peace .\\nMoving to the sample_text_dictionary()  function, we create a \\nterm frequency dictionary, and then return these variables. This process \\nis important for you to understand, because this is an example of how \\nwe deal with words that are not in the vocabulary of a trained word \\nembedding:\\nfor i in range(len(dictionary)):\\n    word = dictionary_list[i][0]\\n    if word in vocabulary:\\n        _embedding_array.append(embedding_dictionary[word])\\n    else:\\n         _embedding_array.append(np.random.uniform(low=-0.2, \\nhigh=0.2, size=embedding_dim))\\nWe b egin by creating a variable title: _embedding_array . This variable \\nactually contains the word embedding representations of our sample \\ntext. To handle words that are not in the vocabulary, we will create a \\nrandomized distribution of numbers to simulate a word embedding, which \\nwe then feed as inputs to the neural network.\\nMoving forward, we make the final transformations to the embedding \\ndata before we create our computation graph.\\nembedding_array = np.asarray(_embedding_array)\\n decision_tree = spatial.KDTree(embedding_array, leafsize=100)Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 122}, page_content=\"113We will use a k-nearest neighbors tree to find the embedding that is \\nclosest to the array that our neural network outputs. From this, we use \\nreverse_dictionary  to find the word that matches the predicted embedding.\\nLet’s build our computational graph, as follows:\\n#Initializing placeholders and other variables\\nX = tf.placeholder(tf.int32, shape=(None, None, n_input))\\nY = tf.placeholder(tf.float32, shape=(None, embedding_dim))\\n weights = {'output': tf.Variable(tf.random_normal([n_hidden, \\nembedding_dim]))}\\n biases = {'output': tf.Variable(tf.random_normal([embedding_\\ndim]))}\\n _weights = tf.Variable(tf.constant(0.0, shape=[vocabulary_\\nlength, embedding_dim]), trainable=True)\\n _embedding = tf.placeholder(tf.float32, [vocabulary_length, \\nembedding_dim])\\nembedding_initializer = _weights.assign(_embedding)\\nembedding_characters = tf.nn.embedding_lookup(_weights, X)\\n input_series = tf.reshape(embedding_characters, [-1, n_input])\\ninput_series = tf.split(input_series, n_input, 1)\\nYou will find most of this similar to the LSTM tutorial in Chapter 2, \\nbut direct your attention to the second grouping of code, specifically \\nwhere we create the _weights  and _embedding  variables. When we are \\nloading a trained word embedding, or have an embedding layer in our \\ncomputational graph, the data must pass through this layer before it can \\nget to the neural network. The dimension of the network is the number of \\nwords in the vocabulary by the number of features. Although the number \\nof features when training one’s own embedding can be altered, this is a \\npredetermined value when we load a word embedding.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 123}, page_content=\"114We assign the weights variable to the _embedding  placeholder, which \\nis ultimately the weights our optimizer is tuning, whereupon we create an \\nembedding characters variable.  The tf.nn.embedding_lookup()  function \\nspecifically retrieves the index numbers of the _weights  variable. Finally, \\nwe transform the embedding_characters  variable into the input_series  \\nvariable, which is actually directly fed into the LSTM layer.\\nFrom this point forward, the passage of data from the LSTM layer \\nthrough the rest of the graph should be familiar from the tutorial. When \\nexecuting the code, you should see output such as the following:\\nInput Sequence: ['me', 'to', 'understand', 'people']\\nActual Label: from\\nPredicted Label: an\\nEpoch: 210\\nError: 45.62042\\nInput Sequence: ['different', 'walks', 'of', 'life,']\\nActual Label: as\\nPredicted Label: with\\nEpoch: 220\\nError: 64.55679\\nInput Sequence: ['well', 'as', 'to', 'question']\\nActual Label: some\\nPredicted Label: has\\nEpoch: 230\\nError: 75.29771\\nAn immediate suggestion for improving the error rate is to load \\ndifferent sample texts, perhaps from an actual corpus of data to train on, \\nas the limited amount of data does not allow the accuracy to improve \\nsignificantly much.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 124}, page_content='115Another suggestion is to use the load_data()  function that is \\ncommented out loading your own PDF file and experimenting from that \\npoint forward.\\nNow that we have reviewed the methods in which we can represent \\nwords as vectors, let’s discuss other textual representations. Thankfully, \\nsince most of these are Word2Vec abstractions , it will not require nearly as \\nmuch explanation this time around.\\n Paragraph2Vec: Distributed Memory \\nof\\xa0Paragraph Vectors (PV-DM)\\nParagraph2Vec is an algorithm that allows us to represent objects of \\nvarying length, from sentences to whole documents, for the same purposes \\nthat we represented words as vectors in the previous examples. This \\ntechnique was developed by Quoc Le and Tomas Mikolov, and largely is \\nbased off the Word2Vec algorithm.\\nIn Paragraph2Vec, we represent each paragraph as a unique vector \\nin a matrix, D. Every word is also mapped to a unique vector, represented \\nby a column in matrix W. We subsequently construct a matrix, h, which \\nis formed by concatenating matrices W and D. We think of this paragraph \\ntoken as an analog to the cell state from the LSTM, in that it is providing \\nmemory to the current context in the form of the topic of the paragraph. \\nIntuitively, that means that matrix W is the same across all paragraphs, \\nsuch that we observe the same representation of a given word. Training \\noccurs as it does in Word2Vec, and the negative sampling can occur in this \\ninstance by sampling from a fixed-length context in a random paragraph.\\nTo ensure that you understand how this works functionally, let’s look at \\none final example in this chapter.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 125}, page_content=\"116 Example Problem 4.5: Paragraph2Vec Example \\nwith\\xa0Movie Review Data\\nOnce again, Gensim thankfully has a Doc2Vec method that makes \\nimplementation of this algorithm relatively straightforward. In this \\nexample, we will keep things relatively simple and represent sentences in \\na vector space, rather than create or approximate a paragraph tokenizer, \\nwhich we would likely want to be more precise than a heuristic that \\nwould be relatively quick to draw up (i.e., paragraphs comprised of four \\nsentences each). In the doc2vec_example.py  file, there are only slight \\ndifferences in the Doc2Vec model and the Word2Vec model, specifically \\nthe preprocessing.\\ndef gensim_preprocess_data(max_pages):\\n    sentences = namedtuple('sentence', 'words tags')\\n    _sentences = sent_tokenize(load_data(max_pages=max_pages))\\n    documents = []\\n    for i, text in enumerate(_sentences):\\n        words, tags = text.lower().split(), [i]\\n        documents.append(sentences(words, tags))\\n    return documents\\nThe Doc2Vec implementation expects what is known as a named \\ntuple object . This tuple contains a list of tokenized words contained \\nin the sentence, as well as an integer that indexes this document. In \\nonline documentation, some people utilize a class object entitled \\nLabledLineSentence() ; however, this performs the necessary \\npreprocessing the same way. When we run our script, we iterate through \\nall the sentences that we are analyzing, and view their associated cosine \\nsimilarities. The following is an example of some of them:\\nDocument sentence(words=['this', 'text', 'adapted', 'the', \\n'saylor', 'foundation', 'creative', 'commons', 'attribution-  Chapter 4  topiC Modeling and\\xa0Word eMbeddings\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 126}, page_content='117noncommercial-  sharealike\\', \\'3.0\\', \\'license\\', \\'without\\', \\n\\'attribution\\', \\'requested\\', \\'works\\', \\'original\\', \\'creator\\', \\n\\'licensee\\', \\'.\\'], tags=[0])\\nDocument sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\n\\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\n\\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\n\\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], \\ntags=[1])\\nCosine Similarity Between Documents: -0.025641936104727547\\nDocument sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\n\\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\n\\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\n\\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], \\ntags=[1])\\nDocument sentence(words=[\\'students\\', \\'motivated\\', \\'study\\', \\n\\'economics\\', \\'see\\', \\'relates\\', \\'lives\\', \\'.\\'], tags=[2])\\nCosine Similarity Between Documents:\\n0.06511943195883922\\nBeyond this, Gensim also allows us to infer vectors without having \\nto retrain our models on these vectors. This is particularly important in \\nChapter 5, where we apply word embeddings in a practical setting. You \\ncan see this functionality when we execute the code with the training_\\nexample  parameter set to False. We have two sample documents, which we \\ndefine at the beginning of the file:\\nsample_text1 = \"\\'I love italian food. My favorite items are\\npizza and pasta, especially garlic bread. The best italian food\\nI have had has been in New  York. Little Italy was very fun\"\\'Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 127}, page_content='118sample_text2 = \"\\'My favorite time of italian food is pasta with\\nalfredo sauce. It is very creamy but the cheese is the best\\npart. Whenevr I go to an italian restaurant, I am always \\ncertain to get a plate.\"\\'\\nThese two examples are fairly similar. When we train our model—more \\nthan 300 pages worth of data from an economics textbook, we get the \\nfollowing results:\\n Cosine Similarity Between Sample Texts:\\n0.9911814256706748\\nAgain, you should be aware that they will likely need significantly \\nlarger amounts of data to get reasonable results across unseen data. \\nThese examples show them how to train and infer vectors using various \\nframeworks. For those who are dedicated to training their own word \\nembeddings, the path forward should be fairly clear.\\n Summary\\nBefore we move on to work on natural language processing tasks, let’s \\nrecap some of the most important things learned in this chapter. As \\nyou saw in Chapter 3, preprocessing data correctly is the majority of the \\nwork that we need to perform when applying deep learning to natural \\nlanguage processing. Beyond cleaning out stop words, punctuation, and \\nstatistical noise, you should be prepared to wrangle data and organize \\nit in an interpretable format for the neural network. Well-trained word \\nembeddings often require the collection of billions of tokens.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 128}, page_content='119Making sure that you aggregate the right data is extremely important, \\nas a couple billion tokens from radically different data sources can leave \\nyou with an embedding that doesn’t yield much of anything useful. \\nAlthough some of our examples yielded positive results, it does not \\nmean these applications would work in a production environment. You \\nmust (responsibly) collect large amounts of text data from sources while \\nmaintaining homogeneity in vocabulary and context.\\nIn the following chapter, we conclude the book by working on \\napplications of recurrent neural networks.Chapter 4  topiC Modeling and\\xa0Word eMbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 129}, page_content='121© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_5CHAPTER 5\\nText Generation, \\nMachine Translation, \\nand\\xa0Other Recurrent \\nLanguage Modeling \\nTasks\\nIn Chapter 4, I introduced you to some of the more advanced deep \\nlearning and NLP techniques, and I discussed how to implement these \\nmodels in some basic problems, such as mapping word vectors. Before \\nwe conclude this book, I will discuss a handful of other NLP tasks that are \\nmore domain-specific, but nonetheless useful to go through.\\nBy this point, you should be relatively comfortable with preprocessing \\ntext data in various formats, and you should understand a few NLP tasks, \\nsuch as document classification, well enough to perform them. As such, \\nthis chapter focuses on combining many of the skills we have worked with \\nby tackling a couple of problems. All solutions provided in this chapter are \\nfeasible. You are more than welcome to present or complete new solutions \\nthat outperform them.'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 130}, page_content='122 Text Generation with\\xa0LSTMs\\nText generation is increasingly an important feature in AI-based tools. \\nParticularly when working with large amounts of data, it is useful \\nfor systems to be able to communicate with users to provide a more \\nimmersive and informative experience. For text generation, the main goal \\nis to create a generative model that provides some sort of insight with \\nrespect to the data. You should be aware that text generation should not \\nnecessarily create a summary of the document, but generate an output \\nthat is descriptive of the input text. Let’s start by inspecting the problem.\\nInitially, for such a task, we need a data source. From that, our data \\nsource changes the results. For this task, we start by working with Harry \\nPotter and the Sorcerer’s Stone . I chose this book since the context should \\nprovide some fairly notable results with respect to the topics that are \\ncontained within the generated text.\\nLet’s go through the steps that we’ve become accustomed to. We will \\nutilize the load_data()  preprocessing function that we used in word_\\nembeddings.py ; however, the only change that we will make is loading \\nharry_potter.pdf  instead of economics_textbook.pdf .\\nThat said, this function allows you to easily utilize the preprocessing \\nfunction for whatever purpose, so long as the directory and other \\narguments are changed. Being that this is a text generation example, we \\nshould not clean the data beyond removing non-ASCII characters.\\nThe following is an example of how the data appears:\\n\"Harry Potter Sorcerer\\'s Stone CHAPTER ONE THE BOY WHO LIVED \\nMr. Mrs. Dursley, number four, Privet Drive, proud say \\nperfectly normal, thank much. They last people \\'d expect \\ninvolved anything strange mysterious, n\\'t hold nonsense. Mr. \\nDursley director firm called Grunnings, made drills. He big, \\nbeefy man hardly neck, although large mustache. Mrs. Dursley \\nthin blonde nearly twice usual amount neck, came useful spent \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 131}, page_content='123much time craning garden fences, spying neighbors. The Dursleys \\nsmall son called Dudley opinion finer boy anywhere. The \\nDursleys everything wanted, also secret, greatest fear somebody \\nwould discover. They think could bear anyone found Potters. \\nMrs. Potter Mrs. Dursley\\'s sister, n\\'t met several years; fact, \\nMrs. Dursley pretended n\\'t sister, sister good-for-nothing \\nhusband unDursleyish possible. The Dursleys shuddered think \\nneighbors would say Potters arrived street. The Dursleys knew \\nPotters small son,, never even seen. This boy another good \\nreason keeping Potters away; n\\'t want Dudley mixing child like. \\nWhen Mr. Mrs. Dursley woke dull, gray Tuesday story starts, \\nnothing cloudy sky outside suggest strange mysterious things \\nwould soon happening country. Mr. Dursley hummed picked boring \\ntie work, Mrs. Dursley gossiped away happily wrestled screaming \\nDudley high chair. None noticed large, tawny owl flutter past \\nwindow. At half past eight, Mr. Dursley picked briefcase, \\npecked Mrs. Dursley cheek, tried kiss Dudley good-bye missed, 1 \\nDudley tantrum throwing cereal walls. `` Little tyke, \"chortled \\nMr. Dursley left house. He got car backed number four\\'s drive. \\nIt corner street noticed first sign something peculiar -- cat \\nreading map. For second, Mr. Dursley n\\'t realize seen -- jerked \\nhead around look. There tabby cat standing corner Privet Drive, \\nn\\'t map sight. What could thinking ? It must trick light. Mr. \\nDursley blinked stared cat. It stared back. As Mr. Dursley \\ndrove around corner road, watched cat mirror. It reading sign \\nsaid Privet Drive --, looking sign; cats...\"\\nLet’s inspect our preprocessing function.\\ndef preprocess_data(sequence_length=sequence_length, max_\\npages=max_pages, pdf_file=pdf_file):\\n    text_data = load_data(max_pages=max_pages, pdf_file=pdf_file)\\n    characters = list(set(text_data.lower()))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 132}, page_content=\"124     character_dict = dict((character, i) for i, character in \\nenumerate(characters))\\n     int_dictionary = dict((i, character) for i, character in \\nenumerate(characters))\\n    num_chars, vocab_size = len(text_data), len(characters)\\n    x, y = [], []\\n    for i in range(0, num_chars  - sequence_length, 1):\\n        input_sequence = text_data[i: i+sequence_length]\\n        output_sequence = text_data[i+sequence_length]\\n         x.append([character_dict[character.lower()] for \\ncharacter in input_sequence])\\n        y.append(character_dict[output_sequence.lower()])\\n    \\n    for k in range(0, len(x)): x[i] = [_x for _x in x[i]]\\n    x = np.reshape(x, (len(x), sequence_length, 1))\\n    x, y = x/float(vocab_size), np_utils.to_categorical(y)\\n     return x, y, num_chars, vocab_size, character_dict,  \\nint_dictionary\\nWhen inspecting the function, we use methods similar to the tf_\\npreprocess_data()  function in the toy example of a Skip-Gram model. \\nOur input and output sequences are fixed lengths, and we will transform \\nthe y variable to a one-hot encoded vector, where each entry in the vector \\nrepresents a possible character. We represent the sequence of characters \\nas a matrix, where each row represents the entire observation and each \\ncolumn represents a character.\\nLet’s look at the first example of Keras code used in the book.\\n    def create_rnn(num_units=num_units, activation=activation):\\n        model = Sequential()\\n         model.add(LSTM(num_units, activation=activation,  \\ninput_shape=(None, x.shape[1])))\\n        model.add(Dense(y.shape[1], activation='softmax'))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 133}, page_content=\"125         model.compile(loss='categorical_crossentropy', \\noptimizer='adam')\\n        model.summary()\\n        return model\\nKeras, unlike TensorFlow, is considerably less verbose. As such, this \\nmakes changing the architecture of a model relatively easy. We instantiate \\na model by assigning it to a variable, and then simply add layer types with \\nthe Sequential().add()  function.\\nAfter running the network with 200 epochs, we get the following result:\\n  driv, proud say perfecdly normal, thanp much. they last \\npeople 'd expect involved anytsing strange mysterious, s't \\nhold donsense. mr. dursley director firm called grunnings, \\nmade drills. he big, berfy man, ardly neck, althougl larte \\nmustache. mrs. dursley thic -londe. early twece uiual amount \\nnecd, came ueeful spent much time craning geddon fences, \\nspying neighbors. the dursleys small son called dudley \\nopinion finer boy anyw   rd. the dursleys everything wanted, \\nslso secret, greatest fear somebody would discover. they \\nthinn could bear antone found potters. mrs. potterimrs. \\ndursley's sister, n't met several years; fact, mrs. dursley \\npretended n't sister, sister good-sur-notding husband \\nundursleyir   pousible. the dursleys suuddered think auigybors \\nwould say potters arrived strett. the dursleys knew potters \\nsmall. on,   ever even seen. thit boy another good reason \\nkeeping potters away; n'e want dudley mixing child like. \\nwten mr. mrs. dursley woke dull, gray tuesday story startss, \\nnothing cloudy skycoutside suggest strange mytter ous taings \\nwould soon darpening codntry. mr. dursley tummed picked \\nboring tie work, mrs. dursley gosudaed away happily wrestled \\nscreaming dudley aigh cuair. noneoloticed large, tawny owl \\nflutter past wincow. at ialf past, ight, mr. dursley picked \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 134}, page_content='126briefcase, pecked mrs. dursley cheek, tried kiss dudley good-\\nbye missed, 1 dudley tantrum,hrowigg cereal walls. `` lwttle \\ntykp, \"chortled mr. dursley left house. he got car backel \\nnumber four\\'s drive. it corner street noticed fir t sign \\nsomathing pcculilr -- cat feading,ap. for sicond, mr. dursley \\nr\\'t realize scen -- jerked head around look. thereytab y \\ncat standing corneraprivet drive, n\\'tamap sight. what sould \\nthinking ? it muse trick light. mr. dursley blinked stared \\ncat. it stayed back. as mr. dursley drove around corner road, \\nwatched catcmirror. it reading sign saidsprivet druve --, \\nlookingtsign; cats could n\\'t read maps signs. mr. durs\\nNote  Some of the text is interpretable, but obviously not ever ything \\nis as good as it could be. In this instance, I suggest that you allow the \\nneural network to train longer and to add more data. Also consider \\nusing different models and model architectures. Beyond this example, \\nit would be useful to present a more advanced version of the LSTM \\nthat is also useful for speech modeling.\\n Bidirectional RNNs (BRNN)\\nBRNNs were created in 1997 by Mike Schuster and Kukdip Paliwal, who \\nintroduced the technique to a signal-processing academic journal. The \\npurpose of the model was to utilize information moving in both a “positive \\nand negative time direction. ” Specifically, they wanted to utilize both \\nthe information moving up to the prediction, as well as the same stream \\nof inputs moving in the opposite direction. Figure\\xa0 5-1 illustrates the \\narchitecture of a BRNN.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 135}, page_content=\"127Let’s imagine that that we have a sequence of words, such as the \\nfollowing: The man walks down the boardwalk.\\nIn a regular RNN, assuming that we wanted to predict the word \\nboardwalk , the input data would be The , man , walks , down , the. If we \\ninput bigrams, it would be The , man , man , walks , and so forth. We keep \\nmoving through the input data, predicting the word that is most likely \\nto come next at each time step, culminating in our final target label, a \\nprobability that corresponds to the one-hot encoded vector that is most \\nlikely to be present given the input data. The only difference in a BRNN \\nis that while we are predicting the sequence left-to-right, we also are \\npredicting the sequence right-to-left.\\nBRNNs have been particularly useful for NLP tasks. The following is \\nthe code for building a BRNN:\\ndef create_lstm(input_shape=(1, x.shape[1])):\\n        model = Sequential()\\n        model.add(Bidirectional(LSTM(unites=n_units,\\n                                     activation=activation),\\n                                     input_shape=input_shape))\\n        model.add(Dense(train_y.shape[1]), activation=out_act)\\n         model.compile(loss='categorical_crossentropy', \\nmetrics=['accuracy'])\\n        return model...AA AA\\nX0 X1 X2 Xis 0\\nsiy0 y1 y2 yi\\ns0\\nFigure 5-1.  Bidirectional RNN\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 136}, page_content='128The structure of the bidirectional RNN is nearly identical in that we are \\nonly adding a Bidirectional()  cast over our layer. This often increases \\nthe time it takes to train neural networks, but in general, it outperforms \\ntraditional RNN architectures in many tasks. With this in mind, let’s apply \\nour model.\\n Creating a\\xa0Name Entity Recognition Tagger\\nPeople who have worked with NLTK or similar packages have likely come \\nacross the name entity recognition  (NER) tagger. NER taggers typically \\noutput a label that identifies the entity within larger categories (person, \\norganization, location, etc.). Creating an NER tagger requires a large \\namount of annotated data.\\nFor this task, we will use a data set from Kaggle. When we unzip the \\ndata, we see that it comes in the following format:\\n played    on    Monday    (    home    team  in    CAPS )    :\\nVBD        IN    NNP       (    NN      NN    IN    NNP  )    :\\nO          O     O         O    O       O     O     O    0    O\\nAmerican    League\\nNNP        NNP\\nB-MISC     I-MISC\\nCleveland   2     DETROIT   1\\nNNP        CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nBALTIMORE   12    Oakland   11   (       10    innings         )\\nVB         CD    NNP       CD   (       CD    NN         )\\nB-ORG      O     B-ORG     O    O       O     O     O\\nTORONTO    5     Minnesota  3\\nTO         CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nMilwaukee   3     CHICAGO   2\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 137}, page_content=\"129NNP        CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nBoston     4     CALIFORNIA 1\\nThe data is tab-delimited but also in .txt  format. This requires some \\ndata wrangling before we get to training the BRNN.\\nLet’s start by turning the text data into an interpretable format, as \\nfollows:\\ndef load_data():\\n     text_data = open('/Users/tawehbeysolow/Downloads/train.\\ntxt', 'rb').readlines()\\n     text_data = [text_data[k].replace('\\\\t', ' ').split() for k \\nin range(0, len(text_data))]\\n    index = range(0, len(text_data), 3)\\n    #Transforming data to matrix format for neural network\\n    input_data =   list()\\n    for i in range(1, len(index)-1):\\n        rows = text_data[index[i-1]:index[i]]\\n         sentence_no = np.array([i for i in np.repeat(i, \\nlen(rows[0]))], dtype=str)\\n        rows.append(np.array(sentence_no))\\n        rows = np.array(rows).T\\n        input_data.append(rows)\\nWe must first iterate through each line of the .txt  file. Notice that \\nthe data is organized in groups of three. A typical grouping looks like the \\nfollowing:\\ntext_data[0]\\n['played', 'on', 'Monday', '(', 'home', 'team', 'in', 'CAPS', \\n')', ':']\\n text_data[1]\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 138}, page_content=\"130['VBD', 'IN', 'NNP', '(', 'NN', 'NN', 'IN', 'NNP', ')', ':']\\ntext_data[2]\\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\\nThe first set of observations contains the text itself, the second set of \\nobservations contains the name entity tag, and the final set contains the \\nspecific tag. Back to the preprocessing function, we take the groupings of \\nsentences and append an array that contains a sentence number label, \\nwhich I will discuss the importance of shortly.\\nWhen looking at a snapshot of the input_data  variable, we see the \\nfollowing:\\ninput_data[0:1]\\n[array([['played', 'VBD', 'O', '1'],\\n       ['on', 'IN', 'O', '1'],\\n       ['Monday', 'NNP', 'O', '1'],\\n       ['(', '(', 'O', '1'],\\n       ['home', 'NN', 'O', '1'],\\n       ['team', 'NN', 'O', '1'],\\n       ['in', 'IN', 'O', '1'],\\n       ['CAPS', 'NNP', 'O', '1'],\\n       [')', ')', 'O', '1'],\\n       [':', ':', 'O', '1']], dtype='|S6')]\\nWe need to remove the sentence label while observing the data in \\nsuch a fashion that the neural network implicitly understands how these \\nsentences are grouped. The reason we want to remove this label is that \\nneural networks read categorical labels (which the sentence number is \\nan analog for) in such a way that higher-numbered sentences explicitly \\nhave a greater importance than lower-numbered sentences. For this task, I \\nassume most you understand we do not want to bake this into the training \\nprocess. As such, we move to the following body of code:\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 139}, page_content=\"131    input_data = pan.DataFrame(np.concatenate([input_data[j] for \\nj in range(0,len(input_data))]),\\n                        columns=['word', 'pos', 'tag', 'sent_no'])\\n    labels, vocabulary = list(set(input_data['tag'].values)), \\nlist(set(input_data['word'].values))\\n    vocabulary.append('endpad'); vocab_size = len(vocabulary); \\nlabel_size = len(labels)\\n  aggregate_function = lambda input: [(word, pos, label) for \\nword, pos, label in zip(input['word'].values.tolist(),\\n   input['pos'].values.tolist(),\\n   input['tag'].values.tolist())]\\nWe organize the input_data  into a data frame, and then create a \\ncouple of other variables that we will use in the function later, as well as \\nthe train_brnn_keras()  function. Some of these variables are familiar to \\nothers present in the scripts from the prior chapter ( vocab_size  represents \\nthe number of words in the vocabulary, for example). However, the \\nimportant parts are mainly the last two variables, which is what you should \\nfocus on to solve this problem.\\nThe lambda function, aggregate_function , takes a data frame as \\nan input, and then returns a three-tuple for each observation within a \\ngrouping. This is precisely how we will group all the observations within \\none sentence. A snapshot of our data after this transformation yields the \\nfollowing:\\n sentences[0]\\n[('played', 'VBD', 'O'), ('on', 'IN', 'O'), ('Monday', 'NNP', \\n'O'), ('(', '(', 'O'), ('home', 'NN', 'O'), ('team', 'NN', \\n'O'), ('in', 'IN', 'O'), ('CAPS', 'NNP', 'O'), (')', ')', 'O'), \\n(':', ':', 'O')]\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 140}, page_content=\"132We have nearly finished all the necessary preprocessing; however, \\nthere is a key step that you should be aware of.\\n     x = [[word_dictionary[word[0]] for word in sent] for sent \\nin sentences]\\n     x = pad_sequences(maxlen=input_shape, sequences=x, \\npadding='post', value=0)\\n     y = [[label_dictionary[word[2]] for word in sent] for sent \\nin sentences]\\n     y = pad_sequences(maxlen=input_shape, sequences=y, \\npadding='post', value=0)\\n      = [np_utils.to_categorical(label, num_classes=label_size) \\nfor label in y]\\nIn the preceding lines of code, we are transforming our words to their \\ninteger labels as we did in many other examples, and creating a one-  \\nhot encoded matrix. This is similar to the previous chapter, however, we \\nshould specifically not use the pad_sequences()  function.\\nWhen working with sentence data, we do not always get sentences of \\nequal length; however, the input matrix for the neural network has to have \\nan equal number of features across all observations. Zero padding  is used \\nto add the extra features that normalize the size of all observations.\\nWith this step done, we are now ready to move to training our neural \\nnetwork. Our model is as follows:\\ndef create_brnn():\\n        model = Sequential()\\n         model.add(Embedding(input_dim=vocab_size+1,  \\noutput_dim=output_dim,\\n                             input_length=input_shape,  \\nmask_zero=True))\\n         model.add(Bidirectional(LSTM(units=n_units, \\nactivation=activation,\\n                                     return_sequences=True)))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 141}, page_content=\"133         model.add(TimeDistributed(Dense(label_size, \\nactivation=out_act)))\\n         model.compile(optimizer='adam', loss='categorical_\\ncrossentropy', metrics=['accuracy'])\\n        model.summary()\\n        return model\\nMost of our model is similar to the prior Keras models built in this \\nchapter; however, we have an embedding layer (analogous to a word \\nembedding) that is stacked on top of the bidirectional LSTM, which is \\nsubsequently staked on top of a fully connected output layer.\\nWe train our network on roughly 90% of the data we have, and then \\nsubsequently evaluate the results. We find that our tagger on the training \\ndata yields an accuracy of 90% and higher, depending on the number of \\nepochs we train it for.\\nNow that we have dealt with this classification task and sufficiently \\nworked with BRNNs, let’s move on to another neural network model and \\ndiscuss how it can be effectively applied to another NLP task.\\n Sequence-to-Sequence Models (Seq2Seq)\\nSequence-to-sequence models (seq2seq) are notable because they take \\nin an input sequence and return an output sequence, both of variable \\nlength. This makes this model particularly powerful, and it is predisposed \\nto perform well on language modeling tasks. The particular model that \\nwe will utilize is best summarized in a paper by Sutskever et\\xa0al. Figure\\xa0 5-2 \\nillustrates the model.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 142}, page_content='134The model is generally comprised of two parts: an encoder and a \\ndecoder. Both the encoder and the decoder are RNNs. The encoder reads \\nthe input sequence and outputs a fixed-length vector in addition to the \\nhidden and cell states from the LSTM unit. Subsequently, the decoder \\ntakes this fixed-length vector, in addition to the output hidden and cell \\nstates, and uses them as inputs to the first of its LSTM units. The decoder \\noutputs a fixed-length vector, which we will evaluate as a target label. We \\nwill perform prediction one character at a time, which easily allows us to \\nevaluate sequences of varying length from one observation to the next. \\nNext, you see this model in action.\\n Question and\\xa0Answer with\\xa0Neural Network \\nModels\\nOne popular application of deep learning to NLP is the chatbot. Many \\ncompanies use chatbots to handle generic customer service requests, \\nwhich require them to be flexible in translating questions into answers. \\nWhile the test case that we look at is a microcosm of questions and howa re you ?Ia m goodDECODER ENCODER\\n<GO>\\nEmbedding\\nFigure 5-2.  Encoder-decoder model\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 143}, page_content=\"135answers, it is an example of how we can train a neural network to properly \\nanswer a question. We will use the Stanford Question Answering Dataset. \\nAlthough it is more representative of general knowledge, you would do \\nwell to recognize the way in which these problems are structured.\\nLet’s begin by examining how we will preprocess the data by utilizing \\nthe following function:\\n     dataset = json.load(open('/Users/tawehbeysolow/Downloads/\\nqadataset.json', 'rb'))['data']\\n    questions, answers = [], []\\n    for j in range(0, len(dataset)):\\n        for k in range(0, len(dataset[j])):\\n             for i in range(0, len(dataset[j]['paragraphs'][k]\\n['qas'])):\\n                 questions.append(remove_non_ascii(dataset[j]\\n['paragraphs'][k]['qas'][i]['question']))\\n               answers.append(remove_non_ascii(dataset[j]\\n['paragraphs'][k]['qas'][i]['answers'][0]\\n['text']))\\nWhen we look at a snapshot of the data, we observe the following structure:\\n[{u'paragraphs': [{u'qas': [{u'question': u'To whom did the \\nVirgin Mary allegedly appear in 1858  in Lourdes France?', \\nu'id': u'5733be284776f41900661182', u'answers': [{u'text': \\nu'Saint Bernadette Soubirous', u'answer_start': 515}]}, \\n{u'question': u'What is in front of the Notre Dame Main \\nBuilding?', u'id': u'5733be284776f4190066117f', u'answers': \\n[{u'text': u'a copper statue of Christ', u'answer_start': \\n188}]}, {u'question': u'The Basilica of the Sacred heart \\nat Notre Dame is beside to which structure?', u'id': \\nu'5733be284776f41900661180', u'answers': [{u'text': u'the Main \\nBuilding', u'answer_start': 279}]}, {u'question': u'What is the \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 144}, page_content='136Grotto at Notre Dame?\\', u\\'id\\': u\\'5733be284776f41900661181\\', \\nu\\'answers\\': [{u\\'text\\': u\\'a Marian place of prayer and \\nreflection\\', u\\'answer_start\\': 381}]}, {u\\'question\\': u\\'What \\nsits on top of the Main Building at Notre Dame?\\', u\\'id\\': \\nu\\'5733be284776f4190066117e\\', u\\'answers\\': [{u\\'text\\': u\\'a \\ngolden statue of the Virgin Mary\\', u\\'answer_start\\': 92}]}], \\nu\\'context\\': u\\'Architecturally, the school has a Catholic \\ncharacter. Atop the Main Building\\\\\\'s gold dome is a golden \\nstatue of the Virgin Mary. Immediately in front of the Main \\nBuilding and facing it, is a copper statue of Christ with arms \\nupraised with the legend \"Venite Ad Me Omnes\". Next to the \\nMain Building is the Basilica of the Sacred Heart. Immediately \\nbehind the basilica is the Grotto, a Marian place of prayer and \\nreflection. It is a replica of the grotto at Lourdes, France \\nwhere the Virgin Mary reputedly appeared to Saint Bernadette \\nSoubirous in 1858. At the end of the main drive (and in a \\ndirect line that connects through 3 statues and the Gold \\nDome), is a simple, modern stone statue of Mary.\\'}, {u\\'qas\\': \\n[{u\\'question\\': u\\'When did the Scholastic Magazine of Notre \\ndame begin publishing?\\', u\\'id\\': u\\'5733bf84d058e614000b61be\\', \\nu\\'answers\\'\\nWe have a JSON file with question and answers. Similar to the name \\nentity recognition task, we need to preprocess our data into a matrix \\nformat that we can input into a neural network. We must first collect the \\nquestions that correspond to the proper answers. Then we iterate through \\nthe JSON file, and append each of the questions and answers to the \\ncorresponding arrays.\\nNow let’s discuss how we are actually going to frame the problem for \\nthe neural network. Rather than have the neural network predict each \\nword, we are going to have the neural network predict each character given \\nan input sequence of characters. Since this is a multilabel classification \\nproblem, we will output a softmax probability for each element of the \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 145}, page_content='137output vector, and then choose the vector with the highest probability. \\nThis represents the character that is most likely to proceed given the prior \\ninput sequence.\\nAfter we have done this for the entire output sequence, we will \\nconcatenate this array of outputted characters so that we get a human-  \\nreadable message. As such, we move forward to the following part of the code:\\n    input_chars, output_chars = set(), set()\\n    for i in range(0, len(questions)):\\n        for char in questions[i]:\\n             if char not in input_chars: input_chars.add(char.\\nlower())\\n    for i in range(0, len(answers)):\\n        for char in answers[i]:\\n             if char not in output_chars:  output_chars.add(char.\\nlower())\\n     input_chars, output_chars = sorted(list(input_chars)), \\nsorted(list(output_chars))\\n     n_encoder_tokens, n_decoder_tokens = len(input_chars), \\nlen(output_chars)\\nWe iterated through each of the questions and answers, and collected \\nall the unique individual characters in both the output and input \\nsequences. This yields the following sets, which represent the input and \\noutput characters, respectively.\\ninput_chars; output_chars\\n[u\\' \\', u\\'\"\\', u\\'#\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\',\\', u\\'-\\', \\nu\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', u\\'6\\', u\\'7\\', \\nu\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'>\\', u\\'?\\', u\\'_\\', u\\'a\\', u\\'b\\', u\\'c\\', \\nu\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', u\\'l\\', u\\'m\\', \\nu\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', u\\'v\\', u\\'w\\', \\nu\\'x\\', u\\'y\\', u\\'z\\']\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 146}, page_content='138[u\\' \\', u\\'!\\', u\\'\"\\', u\\'$\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\'+\\', \\nu\\',\\', u\\'-\\', u\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', \\nu\\'6\\', u\\'7\\', u\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'?\\', u\\'[\\', u\\']\\', u\\'a\\', \\nu\\'b\\', u\\'c\\', u\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', \\nu\\'l\\', u\\'m\\', u\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', \\nu\\'v\\', u\\'w\\', u\\'x\\', u\\'y\\', u\\'z\\']\\nThe two lists contain 53 and 55 characters, respectively; however, they \\nare virtually homogenous and contain all the letters of the alphabet, plus \\nsome grammatical and numerical characters.\\nWe move to the most important part of the preprocessing, in which \\nwe transform our input sequences to one-hot encoded vectors that are \\ninterpretable by the neural network.\\n(code redacted, please see github)\\n     x_encoder = np.zeros((len(questions), max_encoder_len,  \\nn_encoder_tokens))\\n     x_decoder = np.zeros((len(questions), max_decoder_len,  \\nn_decoder_tokens))\\n     y_decoder = np.zeros((len(questions), max_decoder_len,  \\nn_decoder_tokens))\\n     for i, (input, output) in enumerate(zip(questions, \\nanswers)):\\n        for _character, character in enumerate(input):\\n             x_encoder[i, _character, input_\\ndictionary[character.lower()]] = 1.\\n        for _character, character in enumerate(output):\\n             x_decoder[i, _character, output_\\ndictionary[character.lower()]] = 1.\\n             if i > 0: y_decoder[i, _character,  \\noutput_dictionary[character.lower()]] = 1.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 147}, page_content=\"139We start by instantiating two input vectors and an output vector, \\ndenoted by x_encoder , x_decoder , and y_encoder . Sequentially, this \\nrepresents the order in which the data passes through the neural network \\nand validated against the target label. While the one-hot encoding that \\nwe chose to create here is similar, we make a minor change by creating a \\nthree-dimensional array to evaluate each question and answer. Each row \\nrepresents a question, each time step represents a character, and each \\ncolumn represents the type of character within our set of characters. We \\nrepeat this process for each question-and-answer pair until we have an \\narray with the entire data set, which yields 4980 observations of data.\\nThe last step defines the model, as given by the encoder_decoder()  \\nfunction.\\ndef encoder_decoder(n_encoder_tokens, n_decoder_tokens):\\n    encoder_input = Input(shape=(None, n_encoder_tokens))\\n    encoder = LSTM(n_units, return_state=True)\\n     encoder_output, hidden_state, cell_state = encoder(encoder_\\ninput)\\n    encoder_states = [hidden_state, cell_state]\\n    decoder_input = Input(shape=(None, n_decoder_tokens))\\n     decoder = LSTM(n_units, return_state=True,  \\nreturn_sequences=True)\\n     decoder_output, _, _ = decoder(decoder_input,  \\ninitial_state=encoder_states)\\n     decoder = Dense(n_decoder_tokens, activation='softmax')\\n(decoder_output)\\n    model = Model([encoder_input, decoder_input], decoder)\\n     model.compile(optimizer='adam', loss='categorical_\\ncrossentropy',   metrics=['accuracy'])\\n    model.summary()\\n    return model\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\"),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 148}, page_content='140We instantiated our model slightly differently than other Keras models. \\nThis method of creating a model is done through using the Functional \\nAPI, rather than relying on the sequential model, as we have often done. \\nSpecifically, this method is useful when creating more complex models, \\nsuch as seq2seq models, and is relatively straightforward once you have \\nlearned how to use the sequential model. Rather than adding layers to \\nthe sequential model, we instantiate different layers as variables and \\nthen pass the data by calling the tensor we created. We see this when \\nobserving the encoder_output  variable when we instantiate it by calling \\nencoder(encoder_input). We keep doing this through the encoder-decoder \\nphase until we reach an output vector, which we define as a dense/fully \\nconnected layer  with a softmax activation function.\\nFinally, we move to training, and observe the following results:\\nModel Prediction: saint bernadette soubiroust\\nActual Output: saint bernadette soubirous\\nModel Prediction: a copper statue of christ\\nActual Output: a copper statue of christ\\nModel Prediction: the main building\\nActual Output: the main building\\nModel Prediction: a marian place of prayer and reflection\\nActual Output: a marian place of prayer and reflection\\nModel Prediction: a golden statue of the virgin mary\\nActual Output: a golden statue of the virgin mary\\nModel Prediction: september 18760\\nActual Output: september 1876\\nModel Prediction: twice\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 149}, page_content='141Actual Output: twice\\nModel Prediction: the observer\\nActual Output: the observer\\nModel Prediction: three\\nActual Output: three\\nModel Prediction: 19877\\nActual Output: 1987\\nAs you can see, this model performs considerably well, with only three \\nepochs. Although there are some problems with the spelling from adding \\nextra characters, the messages themselves are correct in most instances. \\nFeel free to keep experimenting with this problem, particularly by altering \\nthe model architecture to see if there is one that yields better accuracy.\\n Summary\\nWith the chapter coming to a close, we should review the concepts that are \\nmost important in helping us successfully train our algorithms. Primarily, \\nyou should take note of the model types that are appropriate for different \\nproblems. The encoder-decoder model architecture introduces the “many-  \\nto- many” input-output scheme and shows where it is appropriate to apply it.\\nSecondarily, you should take note of where preprocessing techniques \\ncan be applied to seemingly different but related problems. The translation \\nof data from one language to another uses the same preprocessing steps \\nas creating a neural network that answered questions based on different \\nresponses. Paying attention to these modeling steps and how they relate \\nto the underlying structure of the data can save you time on seemingly \\ninnocuous tasks.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 150}, page_content='142 Conclusion and\\xa0Final Statements\\nWe have reached the end of this book. We solved a wide array of NLP \\nproblems of varying complexities and domains. There are many \\nconcepts that are constant across all problem types, most specifically \\ndata preprocessing. The vast majority of what makes machine learning \\ndifficult is preprocessing data. You saw that similar problem types share \\npreprocessing steps, as we often reused parts of solutions as we moved to \\nmore advanced problems.\\nThere are some final principles that are worth remembering from \\nthis point forward. NLP with deep learning can require large amounts of \\ntext data. Collect it carefully and responsibly, and consider your options \\nwhen dealing with large data sets with respect to choice of language for \\noptimized run time (C/C++ vs. Python, etc.).\\nNeural networks, by and large, are fairly straightforward models to \\nwork with. The difficulty is finding good data that has predictive power, in \\naddition to structuring it in such a way that our neural network can find \\npatterns to exploit.\\nStudy carefully the preprocessing steps to take for document \\nclassification, creating a word embedding, or creating an NER tagger, for \\nexample. Each of these represents feature extraction schemes that can be \\napplied to different problems and illuminate a path forward during your \\nresearch.\\nAlthough intelligent preprocessing of data spoken about fairly often \\nin the machine learning community, it is particularly true of the NLP \\nparadigm of deep learning and data science. The models that we have \\ntrained give you a roadmap on how to work with similar data sets in \\nprofessional or academic environments. However, this does not mean that \\nthe models we have deployed could be used in production and work well.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 151}, page_content='143There are a considerable number of variables that I did not discuss, \\nbeing that they are problems of maintaining production systems rather \\nthan the theory behind a model. Examples include unknown words \\nin vocabularies that appear over time, when to retrain models, how to \\nevaluate multiple models’ outputs simultaneously, and so forth.\\nIn my experience, finding out when to retrain models has best been \\nsolved by collecting large amounts of live performance data. See when \\nsignals deprecate, if they do at all, and track the effect of retraining, as well \\nas the persistence in retraining of models. Even if your model is accurate, it \\ndoes not mean that it will be easy to use in practice.\\nThink carefully about how to handle false classifications, particularly \\nif the penalty for misclassification could cause the loss of money and/or \\nother resources. Do not be afraid to utilize multiple models for multiple \\nproblem types. When experimenting, start simple and gradually add \\ncomplexity as needed. This is significantly easier than trying to design \\nsomething very complex in the beginning and then trying to debug a \\nsystem that you do not understand.\\nYou are encouraged to reread this book at your leisure, as well as for \\nreference, in addition to utilizing the code on my GitHub page to tackle the \\nproblems in their own unique fashion. While reading this book provides \\na start, the only way to become proficient in data science is to practice the \\nproblems on your own.\\nI hope you have enjoyed learning about natural language processing \\nand deep learning as much as I have enjoyed explaining it.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 152}, page_content='145© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5Index\\nA, B\\nBackpropagation through time \\n(BPTT), 36\\nBag-of-words (BoW) model\\nadvantages, 73\\nCountVectorizer, 51–52\\ndefinition, 50\\ndisadvantages, 74\\nfeature extraction algorithm, 50\\nmachine learning algorithm, 50\\nmovie reviews ( see IMDB  \\nmovie review data set)\\nscikit-learn library, 51\\nspam detection\\naccuracy and  \\nAUC scores, 55–56\\nCountVectorizer(), 54\\ndata set, 54\\nfit() method, 55\\ninbox, 53\\nload_spam_data(), 54\\nlogistic regression, 54–56\\nnp.random.seed()  \\nfunction, 56\\nridge regression, 55\\nROC curve, 57\\nSMS message length \\nhistogram, 53text_classifiction_demo.py \\nfile, 54\\nunwanted advertisements/\\nmalicious files, 53\\nTFIDF , 57\\nBidirectional RNNs  \\n(BRNNs), 126–128,  133\\nC\\nContinuous bag-of-words (CBoW) \\nmodel, 103–105\\nD, E, F\\nDeep learning\\napplications\\nlanguage modeling  \\ntasks, 11\\nNLP techniques and \\ndocument  \\nclassification, 10\\nRNNs, 11\\ntopic modeling, 10\\nword embeddings, 10–11\\nKeras, 7–8\\nmodels, 4\\nTensorFlow, 4–7\\nTheano, 8–9'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 153}, page_content='146G, H\\nGlobal Vectors for Word \\nRepresentation (GloVe)\\nco-occurrence, 106–107\\ncosine similarities, 110\\ndescription, 106\\nerror function, 107\\nGitHub repository, 108\\nmatrix-based factorization \\nmethod, 106\\nopen() function, 109\\npretrained embeddings, 108–110\\nweighting function, 107–108\\nWikipedia, 109\\nI\\nIMDB movie review data set\\n” .join() function, 63\\nL1 and L2 norm visualization, 69\\nload data, 64\\nlogistic regression, 65–66\\nmachine learning packages, 62\\nmin_df and max_df, 65\\nmlp_movie_classification_\\nmodel.py file, 68\\nopen() function, 64\\nord() function, 63\\nos.listdir() function, 64\\npositive and negative rate, 73\\nremove_non_ascii() function, 64\\nROC curve\\nL1 and L2 logistic regression \\ntest set, 67–68multilayer perceptron, 70\\nnaïve Bayes classifier, 71–73\\nrandom forest, 71\\nTfidfVectorizer() method, 63\\ntrain_logistic_model()  \\nfunction, 65\\nJ\\nJupyter notebook, 89\\nK\\nKeras, 7–8\\nL\\nLatent Dirichlet allocation (LDA)\\nassumptions, 78\\nbeta distribution, 79\\njoint probability  \\ndistribution, 79\\nmovie review data\\ndocument classification, 81\\nfit_transform() method, 82\\nGensim, 84–85\\nscikit-learn  \\nimplementation, 86\\nsklearn_topic_model() \\nfunction, 85\\nTFIDF model, 82–84\\ntopics, 82–83\\nmultinomial distribution, 78\\nPoisson distribution, 78\\nprobability distribution, 79Index'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 154}, page_content='147TFIDF , 78\\ntopics and words simplexes, 80\\nLong short-term memory (LSTM)\\nBasicLSTMCell() function, 41\\nformulae, 38\\ngates, 39\\nplaceholder variables, 40–41\\nsigmoid activation  \\nfunction, 38–39\\ntanh activation function, 38\\nunits/blocks, 37–38\\nword embeddings\\ncomputation graph, 112\\n_embedding_array, 112\\nerror rate, 114\\nexecuting code, 114\\nload_data() function, 115\\npreprocessing steps, 111\\nremove_stop_words() \\nfunction, 112\\nreverse_dictionary, 113\\nsample data, 111\\nsample_text_dictionary() \\nfunction, 112\\ntf.nn.embedding_lookup() \\nfunction, 114\\ntraining data, 111\\n_weights and _embedding \\nvariables, 113\\nM\\nMean squared error (MSE), 29–30\\nModeling stock returnsLSTM, 40\\nMLPs, 15\\nRNNs, 32\\nMultilayer perceptron  \\nmodels (MLPs)\\ncross entropy, 30\\nerror function, 18–19\\nFord Motor Company (F), 15\\nlearning rate\\nactivation function, 16, 24–25\\nAdam optimization \\nalgorithm, 20–21\\nepochs, 22\\nfloating-point value, 20\\noptimal solution, 20\\nparameter, 20, 22\\nplaceholder variables, 23\\nReLU activation function, 26\\nsigmoid activation  \\nfunction, 24–25\\nTensorFlow, 22–23\\ntraining and test sets, 22\\nvanishing gradient, 26\\nweights, neural network, 24\\nMSE and RMSE loss  \\nfunction, 29–30\\nneural networks, 17\\nnormal distribution, 17\\nsentiment analysis, 30\\nSLPs, 13\\nstandard normal distribution, 14\\nTensorFlow, 15\\ntf.random_normal(), 17\\ntrain_data, 15Index'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 155}, page_content='148vanishing gradients and  \\nReLU, 27–28\\nvisualization, 14\\nweight and bias units, 17–18\\nN, O\\nName entity recognition (NER) \\ntagger\\naggregate_function, 131\\ncategories, 128\\ndata set, Kaggle, 128\\nembedding layer, 133\\nfeature extraction, 142\\ninput_data variable, 130–131\\ninteger labels, 132\\nneural network, 130,  132\\ntext data, 129\\ntrain_brnn_keras() function, 131\\nzero padding, 132\\nNatural language processing (NLP)\\nBayesian statistics, 3\\nbifurcation, 3\\ncomplexities and domains, 142\\ncomputational linguistics, 2\\ncomputing power, 3\\ndeep learning, Python ( see  \\nDeep learning)\\ndefinition, 1\\nformal language theory, 2\\nmachine learning concepts, 4\\nprinciples, 142\\nSLP , 2–3spell-check, sentences, 31\\nNatural Language Toolkit (NLTK) \\nmodule, 45–46\\nNatural language understanding \\n(NLU), 3\\nNeural networks\\ncharacters, 136–138\\nchatbots, 134\\ndense/fully connected layer, 140\\nencoder_decoder() function, \\n139–140\\nJSON file, 136\\nKeras models, 140\\none-hot encoded  \\nvectors, 138–139\\nseq2seq models, 140\\nStanford Question Answering \\nDataset, 135–136\\nNon-negative matrix factorization \\n(NMF)\\nfeatures, 87\\nGensim model, 90\\nJupyter notebook, 89–90\\nand LDA, 90\\nmathematical formula, 86\\nscikit-learn implementation, \\n87–88,  90\\ntopic extraction, 88\\nP,  Q\\nParagraph2Vec algorithm, 115\\nmovie review data, 116–118\\nPrincipal components analysis \\n(PCA), 97Multilayer perceptron models \\n(MLPs) ( cont. )Index'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 156}, page_content='149R\\nRecurrent neural networks (RNNs)\\nactivation function, 35\\nBPTT , 36\\nbuild_rnn() function, 32\\nchain rule, 36\\ndata set, 33, 35\\nfloating-point decimal, 35\\ngradient descent algorithm, 35\\nhidden state, 32, 33\\nLSTM ( see Long short-term \\nmemory (LSTM))\\nplaceholder variables, 34\\nsigmoid activation function, 37\\nstate_size, 32–33\\nstructure, 31–32\\ntanh activation and derivative \\nfunction, 36–37\\nTensorFlow, 32\\nvanishing gradient, 36–37\\nRoot mean squared error  \\n(RMSE), 29–30\\nS\\nSequence-to-sequence models \\n(seq2seq), 133–134\\nSigmoid activation  \\nfunction, 24–25\\nSingle-layer perceptron (SLP),  \\n2–3,  13\\nSkip-Gram model\\narchitecture, 92\\nk-skip-n-grams, 91negative sampling, 93\\nneural network, 93\\nn-gram, 91\\nobjective function, 91\\none-hot encoded vector, 92\\n2-skip-bi-gram model, 91\\ntraining words, 91\\nword embedding\\ncosine similarity, 98\\nGensim, 96–99\\nhidden layer weight  \\nmatrix, 93\\nindex number, 99\\nnegative sampling, 101\\nneural networks, 96\\none-hot encoding data, 100\\nPCA, 97\\nPDFMiner, 94\\nTensorFlow, 94, 101–102\\ntokenizing data, 95\\nvisualizing, 96–97\\nvocabulary size and word \\ndictionary, 100\\nT , U, V\\nTensorFlow, 4–7\\nTerm frequency–inverse document \\nfrequency (TFIDF), 57\\nText generation, LSTMs\\nAI-based tools, 122\\nBRNNs, 126–128\\ndata, 122\\nepochs, 125–126Index'),\n",
       " Document(metadata={'source': 'C:\\\\Projects\\\\Langchain\\\\Rag_doc\\\\Applied Natural Language Processing.pdf', 'page': 157}, page_content='150Harry Potter and the Sorcerer’s \\nStone, 122\\nKeras code, 124\\nload_data(), 122\\npreprocessing function, 123–124\\nSequential().add()  \\nfunction, 125\\nSkip-Gram model, 124\\ntf_preprocess_data(), 124\\nTheano, 8–9\\nTokenization and stop words\\nBoolean variable, 47\\ndata set, 44\\nfeature extraction  \\nalgorithms, 48–49\\nfunction words, 46\\ngrammatical characters, 48\\nlowercase, 47\\nmistake() and advised_\\npreprocessing()  \\nfunctions, 47–48\\nNLTK module, 45–46\\nsample_sent_tokens, 46\\nsample text, 44sample_word_tokens, 45, 49\\nsingle string objects, 44\\nuppercase, 48\\nTopic models, 10\\ndescription, 77\\nLDA ( see Latent Dirichlet \\nallocation (LDA))\\nNMF ( see Non-negative matrix \\nfactorization (NMF))\\nWord2Vec, 90–93\\nW, X, Y\\nWord embeddings, 10–11\\nCBoW, 103–105\\nGloVe, 106–110\\nLSTM ( see Long short-term \\nmemory (LSTM))\\nParagraph2Vec, 115–118\\nSkip-Gram model ( see Skip-  \\nGram mo del)\\nZ\\nZero padding, 132Text generation, LSTMs ( cont. )Index')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap =100) # How to determine the size of chunking and overlap\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applied Natural \\nLanguage Processing \\nwith Python\\nImplementing Machine Learning  \\nand Deep Learning Algorithms for  \\nNatural Language Processing\\n—\\nTaweh Beysolow II\\n\\nApplied Natural \\nLanguage Processing \\nwith Python\\nImplementing Machine \\nLearning and Deep Learning \\nAlgorithms for Natural \\nLanguage Processing\\nTaweh  Beysolow  II\\n\\nApplied Natural Language Processing with Python\\nISBN-13 (pbk): 978-1-4842-3732-8     ISBN-13 (electronic): 978-1-4842-3733-5\\nhttps://doi.org/10.1007/978-1-4842-3733-5\\nLibrary of Congress Control Number: 2018956300\\nCopyright © 2018 by Taweh Beysolow II \\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or \\npart of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, \\nand transmission or information storage and retrieval, electronic adaptation, computer software, \\nor by similar or dissimilar methodology now known or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark \\nsymbol with every occurrence of a trademarked name, logo, or image we use the names, logos, \\nand images only in an editorial fashion and to the benefit of the trademark owner, with no \\nintention of infringement of the trademark. \\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if \\nthey are not identified as such, is not to be taken as an expression of opinion as to whether or not \\nthey are subject to proprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the date of \\npublication, neither the authors nor the editors nor the publisher can accept any legal \\nresponsibility for any errors or omissions that may be made. The publisher makes no warranty, \\nexpress or implied, with respect to the material contained herein.\\nManaging Director, Apress Media LLC: Welmoed Spahr\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Siddhi Chavan\\nCoordinating Editor: Divya Modi\\nCover designed by eStudioCalamar\\nCover image designed by Freepik (www.freepik.com)\\nDistributed to the book trade worldwide by Springer Science+Business Media New\\xa0York,  \\n233 Spring Street, 6th Floor, New\\xa0York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505,  \\ne-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a \\nCalifornia LLC and the sole member (owner) is Springer Science + Business Media Finance Inc \\n(SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.\\nFor information on translations, please e-mail rights@apress.com, or visit http://www.apress.\\ncom/rights-permissions.\\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook \\nversions and licenses are also available for most titles. For more information, reference our Print \\nand eBook Bulk Sales web page at http://www.apress.com/bulk-sales.\\nAny source code or other supplementary material referenced by the author in this book is \\navailable to readers on GitHub via the book’s product page, located at www.apress.com/ \\n978-1-4842-3732-8. For more detailed information, please visit http://www.apress.com/\\nsource-code.\\nPrinted on acid-free paperTaweh\\xa0Beysolow\\xa0II\\nSan Francisco, California, USA\\n\\nTo my family, friends, and colleagues for their continued \\nsupport and encouragement to do more with myself than  \\nI often can conceive of doing\\n\\nvTable of Contents\\nChapter 1 :  What Is Natural Language Pr ocessing?  ������������������������������ 1\\nThe History of  Natural Language Processing  �������������������������������������������������������� 2\\nA Review of  Machine Learning and  Deep Lear ning ���������������������������������������������� 4\\nNLP, Machine Learning, and  Deep Lear ning Packages with  Python  ���������������� 4\\nApplications of  Deep Lear ning to  NLP ����������������������������������������������������������� 10\\nSummar y������������������������������������������������������������������������������������������������������������� 12\\nChapter 2 :  Review of\\xa0Deep Learning ��������������������������������������������������� 13\\nMultilayer Perceptrons and  Recurrent Neural Networks  ������������������������������������ 13\\nToy Example 1: Modeling Stock Returns with  the MLP Model  ����������������������� 15\\nVanishing Gradients and  Why ReLU Helps to  Prevent Them  �������������������������� 27\\nLoss Functions and  Backpropagation  ������������������������������������������������������������ 29\\nRecurr ent Neural Networks and  Long Short-T erm Memory  �������������������������� 30\\nToy Example 2: Modeling Stock Returns with  the RNN Model  ����������������������� 32\\nToy Example 3: Modeling Stock Returns with  the LSTM Model  ��������������������� 40\\nSummary ������������������������������������������������������������������������������������������������������������� 41About the Author  ��������������������������������������������������������������������������������� ix\\nAbout the Technical Review er ������������������������������������������������������������� xi\\nAcknowledgments  ����������������������������������������������������������������������������� xiii\\nIntroduction  ���������������������������������������������������������������������������������������� xv\\n\\nviChapter 3 :  Working with\\xa0 Ra w Text  ���������������������������������������������������� 43\\nTokenization and  Stop Words  ������������������������������������������������������������������������������ 44\\nThe Bag-of-Wor ds Model (BoW)  �������������������������������������������������������������������������� 50\\nCountV ectorizer  ��������������������������������������������������������������������������������������������� 51\\nExample Problem 1:  Spam Detection  ������������������������������������������������������������ 53\\nTerm Frequency Inverse Document Frequency  ��������������������������������������������� 57\\nExample Problem 2:  Classifying Movie Reviews  ������������������������������������������� 62\\nSummar y������������������������������������������������������������������������������������������������������������� 74\\nChapter 4 :  Topic Modeling and\\xa0 Word Embeddings  ���������������������������� 77\\nTopic Model and  Latent Dirichlet Allocation (LDA)  ���������������������������������������������� 77\\nTopic Modeling with  LDA on  Movie Review Data  ������������������������������������������� 81\\nNon-Negative Matrix Factorization (NMF)  ����������������������������������������������������������� 86\\nWord2Vec  ������������������������������������������������������������������������������������������������������������ 90\\nExample Problem 4 �2: Training a  Word Embedding (Skip-Gram)  ������������������� 94\\nContinuous Bag-of-Wor ds (CBoW)  �������������������������������������������������������������������� 103\\nExample Problem 4 �2: Training a  Word Embedding (CBoW)  ������������������������� 105\\nGlobal Vectors for  Word Representation (GloVe)  ����������������������������������������������� 106\\nExample Problem 4 �4: Using Trained Word Embeddings with  LSTMs  ���������� 111\\nParagraph2Vec: Distributed Memory of  Paragraph Vectors (PV-DM)  ���������������� 115\\nExample Problem 4 �5: Paragraph2Vec Example with  Movie  \\nReview Data  ������������������������������������������������������������������������������������������������ 116\\nSummar y����������������������������������������������������������������������������������������������������������� 118\\nChapter 5 :  Text Generation, Machine Translation, and\\xa0Other  \\nRecurr ent Language Modeling Tasks  ������������������������������ 121\\nText Generation with  LSTMs  ����������������������������������������������������������������������������� 122\\nBidirectional RNNs (BRNN)  �������������������������������������������������������������������������� 126Table of Con TenTs Table of Con TenTs\\n\\nviiCreating a  Name Entity Recognition Tagger  ������������������������������������������������������ 128\\nSequence-to-Sequence Models (Seq2Seq)  ������������������������������������������������������ 133\\nQuestion and  Answer with  Neur al Network Models  ������������������������������������������ 134\\nSummar y����������������������������������������������������������������������������������������������������������� 141\\nConclusion and  Final Statements  ��������������������������������������������������������������������� 142\\n Index  ������������������������������������������������������������������������������������������������� 145Table of Con TenTs Table of Con TenTs\\n\\nixAbout the Author\\nTaweh\\xa0Beysolow  II\\xa0is a data scientist and \\nauthor currently based in San Francisco, \\nCalifornia. He has a bachelor’s degree in \\neconomics from St. Johns University and a \\nmaster’s degree in applied statistics from \\nFordham University. His professional \\nexperience has included working at Booz \\nAllen Hamilton, as a consultant and in various \\nstartups as a data scientist, specifically \\nfocusing on machine learning. He has applied machine learning to federal \\nconsulting, financial services, and agricultural sectors.\\n\\nxiAbout the Technical Reviewer\\nSantanu\\xa0Pattanayak \\xa0currently works at GE \\nDigital as a staff data scientist and is the author \\nof the deep learning book  Pro Deep Learning \\nwith TensorFlow: A Mathematical Approach \\nto Advanced Artificial Intelligence in Python  \\n(Apress, 2017). He has more than eight years of \\nexperience in the data analytics/data science \\nfield and a background in development and \\ndatabase technologies. Prior to joining GE, \\nSantanu worked at companies such as RBS, \\nCapgemini, and IBM.\\xa0He graduated with a degree in electrical engineering \\nfrom Jadavpur University, Kolkata, and is an avid math enthusiast. Santanu \\nis currently pursuing a master’s degree in data science from the Indian \\nInstitute of Technology (IIT), Hyderabad. He also devotes his time to data \\nscience hackathons and Kaggle competitions, where he ranks within the \\ntop 500 across the globe. Santanu was born and brought up in West Bengal, \\nIndia, and currently resides in Bangalore, India, with his wife.\\n\\nxiiiAcknowledgments\\nA special thanks to Santanu Pattanayak, Divya Modi, Celestin Suresh \\nJohn, and everyone at Apress for the wonderful experience. It has been a \\npleasure to work with you all on this text. I couldn’t have asked for a better \\nteam.\\n\\nxvIntroduction\\nThank you for choosing Applied Natural Language Processing with Python  \\nfor your journey into natural language processing (NLP). Readers should \\nbe aware that this text should not be considered a comprehensive study \\nof machine learning, deep learning, or computer programming. As such, \\nit is assumed that you are familiar with these techniques to some degree. \\nRegardless, a brief review of the concepts necessary to understand the \\ntasks that you will perform in the book is provided.\\nAfter the brief review, we begin by examining how to work with raw \\ntext data, slowly working our way through how to present data to machine \\nlearning and deep learning algorithms. After you are familiar with some \\nbasic preprocessing algorithms, we will make our way into some of the \\nmore advanced NLP tasks, such as training and working with trained \\nword embeddings, spell-check, text generation, and question-and-answer \\ngeneration.\\nAll of the examples utilize the Python programming language and \\npopular deep learning and machine learning frameworks, such as scikit-  \\nlearn, Keras, and TensorFlow. Readers can feel free to access the source \\ncode utilized in this book on the corresponding GitHub page and/or try \\ntheir own methods for solving the various problems tackled in this book \\nwith the datasets provided.\\n\\n1© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_1CHAPTER 1\\nWhat Is Natural \\nLanguage \\nProcessing?\\nDeep learning and machine learning continues to proliferate throughout \\nvarious industries, and has revolutionized the topic that I wish to discuss \\nin this book: natural language processing (NLP). NLP is a subfield of \\ncomputer science that is focused on allowing computers to understand \\nlanguage in a “natural” way, as humans do. Typically, this would refer to \\ntasks such as understanding the sentiment of text, speech recognition, and \\ngenerating responses to questions.\\nNLP has become a rapidly evolving field, and one whose applications \\nhave represented a large portion of artificial intelligence (AI) \\nbreakthroughs. Some examples of implementations using deep learning \\nare chatbots that handle customer service requests, auto-spellcheck on cell \\nphones, and AI assistants, such as Cortana and Siri, on smartphones. For \\nthose who have experience in machine learning and deep learning, natural \\nlanguage processing is one of the most exciting areas for individuals to \\napply their skills. To provide context for broader discussions, however, let’s \\ndiscuss the development of natural language processing as a field.\\n\\n2 The History of\\xa0Natural Language Processing\\nNatural language processing can be classified as a subset of the broader \\nfield of speech and language processing. Because of this, NLP shares \\nsimilarities with parallel disciplines such as computational linguistics, \\nwhich is concerned with modeling language using rule-based models. \\nNLP’s inception can be traced back to the development of computer science \\nin the 1940s, moving forward along with advances in linguistics that led to \\nthe construction of formal language theory. Briefly, formal language theory \\nmodels language on increasingly complex structures and rules to these \\nstructures. For example, the alphabet is the simplest structure, in that it is \\na collection of letters that can form strings called words . A formal language \\nis one that has a regular, context-free, and formal grammar. In addition to \\nthe development of computer sciences as a whole, artificial intelligence’s \\nadvancements also played a role in our continuing understanding of NLP .\\nIn some sense, the single-layer perceptron (SLP) is considered to be the \\ninception of machine learning/AI.\\xa0Figure\\xa0 1-1 shows a photo of this model.\\nThe SLP was designed by neurophysiologist Warren McCulloch and \\nlogician Walter Pitt. It is the foundation of more advanced neural network \\nmodels that are heavily utilized today, such as multilayer perceptrons.  \\nFigure 1-1.  Single-layer perceptronChapter 1  What Is Natural laNguage proCessINg?\\n\\n3The SLP model is seen to be in part due to Alan Turing’s research in the \\nlate 1930s on computation, which inspired other scientists and researchers \\nto develop different concepts, such as formal language theory.\\nMoving forward to the second half of the twentieth century, NLP starts \\nto bifurcate into two distinct groups of thought: (1) those who support a \\nsymbolic approach to language modelling, and (2) those who support a \\nstochastic approach. The former group was populated largely by linguists \\nwho used simple algorithms to solve NLP problems, often utilizing pattern \\nrecognition. The latter group was primarily composed of statisticians \\nand electrical engineers. Among the many approaches that were popular \\nwith the second group was Bayesian statistics. As the twentieth century \\nprogressed, NLP broadened as a field, including natural language \\nunderstanding (NLU) to the problem space (allowing computers to react \\naccurately to commands). For example, if someone spoke to a chatbot and \\nasked it to “find food near me, ” the chatbot would use NLU to translate this \\nsentence into tangible actions to yield a desirable outcome.\\nSkip closer to the present day, and we find that NLP has experienced \\na surge of interest alongside machine learning’s explosion in usage over \\nthe past 20 years. Part of this is due to the fact that large repositories of \\nlabeled data sets have become more available, in addition to an increase in \\ncomputing power. This increase in computing power is largely attributed \\nto the development of GPUs; nonetheless, it has proven vital to AI’s \\ndevelopment as a field. Accordingly, demand for materials to instruct \\ndata scientists and engineers on how to utilize various AI algorithms has \\nincreased, in part the reason for this book.\\nNow that you are aware of the history of NLP as it relates to the present \\nday, I will give a brief overview of what you should expect to learn. The \\nfocus, however, is primarily to discuss how deep learning has impacted \\nNLP , and how to utilize deep learning and machine learning techniques to \\nsolve NLP problems.Chapter 1  What Is Natural laNguage proCessINg?\\n\\n4 A Review of\\xa0Machine Learning and\\xa0Deep \\nLearning\\nYou will be refreshed on important machine learning concepts, \\nparticularly deep learning models such as multilayer perceptrons  (MLPs), \\nrecurrent neural networks  (RNNs), and long short-term memory  (LSTM) \\nnetworks. You will be shown in-depth models utilizing toy examples before \\nyou tackle any specific NLP problems.\\n NLP , Machine Learning, and\\xa0Deep Learning \\nPackages with\\xa0Python\\nEqually important as understanding NLP theory is the ability to apply it in \\na practical context. This book utilizes the Python programming language, \\nas well as packages written in Python. Python has become the lingua \\nfranca for data scientists, and support of NLP , machine learning, and \\ndeep learning libraries is plentiful. I refer to many of these packages when \\nsolving the example problems and discussing general concepts.\\nIt is assumed that all readers of this book have a general understanding \\nof Python, such that you have the ability to write software in this language. \\nIf you are not familiar with this language, but you are familiar with others, \\nthe concepts in this book will be portable with respect to the methodology \\nused to solve problems, given the same data sets. Be that as it may, this \\nbook is not intended to instruct users on Python. Now, let’s discuss some of \\nthe technologies that are most important to understanding deep learning.\\n TensorFlow\\nOne of the groundbreaking releases in open source software, in addition \\nto machine learning at large, has undoubtedly been Google’s TensorFlow. \\nIt is an open source library for deep learning that is a successor to Theano, \\na similar machine learning library. Both utilize data flow graphs for Chapter 1  What Is Natural laNguage proCessINg?\\n\\n5computational processes. Specifically, we can think of computations as \\ndependent on specific individual operations. TensorFlow functionally \\noperates by the user first defining a graph/model, which is then operated \\nby a TensorFlow session that the user also creates.\\nThe reasoning behind using a data flow graph rather than another \\ncomputational format computation is multifaceted, however one of the \\nmore simple benefits is the ability to port models from one language to \\nanother. Figure\\xa0 1-2 illustrates a data flow graph.\\nFor example, you may be working on a project where Java is the \\nlanguage that is most optimal for production software due to latency \\nreasons (high-frequency trading, for example); however, you would like to \\nutilize a neural network to make predictions in your production system. \\nRather than dealing with the time-consuming task of setting up a training \\nframework in Java for TensorFlow graphs, something could be written in \\nPython relatively quickly, and then the graph/model could be restored by \\nloading the weights in the production system by utilizing Java. TensorFlow \\ncode is similar to Theano code, as follows.\\n    #Creating weights and biases dictionaries\\n     weights = {\\'input\\': tf.Variable(tf.random_normal([state_\\nsize+1, state_size])),biases\\nweights\\ninputs\\ntargetsMatMulAdd Softmax\\nXentGraph of Nodes , also called operations (ops)\\nFigure 1-2.  Data flow graph diagramChapter 1  What Is Natural laNguage proCessINg?\\n\\n6         \\'output\\': tf.Variable(tf.random_normal([state_size, \\nn_classes]))}\\n     biases = {\\'input\\': tf.Variable(tf.random_normal([1, state_\\nsize])),\\n         \\'output\\': tf.Variable(tf.random_normal([1, n_classes]))}\\n    #Defining placeholders and variables\\n    X = tf.placeholder(tf.float32, [batch_size, bprop_len])\\n    Y = tf.placeholder(tf.int32, [batch_size, bprop_len])\\n     init_state = tf.placeholder(tf.float32, [batch_size, state_\\nsize])\\n    input_series = tf.unstack(X, axis=1)\\n    labels = tf.unstack(Y, axis=1)\\n    current_state = init_state\\n    hidden_states = []\\n    #Passing values from one hidden state to the next\\n     for input in input_series: #Evaluating each input within \\nthe series of inputs\\n         input = tf.reshape(input, [batch_size, 1]) #Reshaping \\ninput into MxN tensor\\n         input_state = tf.concat([input, current_state], axis=1) \\n#Concatenating input and current state tensors\\n         _hidden_state = tf.tanh(tf.add(tf.matmul(input_\\nstate, weights[\\'input\\']), biases[\\'input\\'])) #Tanh \\ntransformation\\n         hidden_states.append(_hidden_state) #Appending the next \\nstate\\n        current_state = _hidden_state #Updating the current state\\nTensorFlow is not always the easiest library to use, however, as there \\noften serious gaps between documentation for toy examples vs.  real-  \\nworld examples that reasonably walk the reader through the complexity of \\nimplementing a deep learning model.Chapter 1  What Is Natural laNguage proCessINg?\\n\\n7In some ways, TensorFlow can be thought of as a language inside of \\nPython, in that there are syntactical nuances that readers must become \\naware of before they can write applications seamlessly (if ever). These \\nconcerns, in some sense, were answered by Keras.\\n Keras\\nDue to the slow development process of applications in TensorFlow, \\nTheano, and similar deep learning frameworks, Keras was developed for \\nprototyping applications, but it is also utilized in production engineering \\nfor various problems. It is a wrapper for TensorFlow, Theano, MXNet, and \\nDeepLearning4j. Unlike these frameworks, defining a computational graph \\nis relatively easy, as shown in the following Keras demo code.\\ndef create_model():\\n    model = Sequential()\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       input_shape=(None, 40, 40, 1),\\n                       padding=\\'same\\', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding=\\'same\\', return_sequences=True))\\n    model.add(BatchNormalization())\\n    \\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding=\\'same\\', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\\n                       padding=\\'same\\', return_sequences=True))\\n    model.add(BatchNormalization())\\n    model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),Chapter 1  What Is Natural laNguage proCessINg?\\n\\n8                   activation=\\'sigmoid\\',\\n                   padding=\\'same\\', data_format=\\'channels_last\\'))\\n    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adadelta\\')\\n    return model\\nAlthough having the added benefit of ease of use and speed with \\nrespect to implementing solutions, Keras has relative drawbacks when \\ncompared to TensorFlow. The broadest explanation is that Keras \\nusers have considerably less control over their computational graph \\nthan TensorFlow users. You work within the confines of a sandbox \\nwhen using Keras. TensorFlow is better at natively supporting more \\ncomplex operations, and providing access to the most cutting-edge \\nimplementations of various algorithms.\\n Theano\\nAlthough it is not covered in this book, it is important in the progression \\nof deep learning to discuss Theano. The library is similar to TensorFlow \\nin that it provides developers with various computational functions (add, \\nmatrix multiplication, subtract, etc.) that are embedded in tensors when \\nbuilding deep learning and machine learning models. For example, the \\nfollowing is a sample Theano code.\\n(code redacted please see github)\\nX, Y = T.fmatrix(), T.vector(dtype=theano.config.floatX)\\n    weights = init_weights(weight_shape)\\n    biases = init_biases(bias_shape)\\n    predicted_y = T.argmax(model(X, weights, biases), axis=1)\\n    cost = T.mean(T.nnet.categorical_crossentropy(predicted_y, Y))\\n    gradient = T.grad(cost=cost, wrt=weights)\\n    update = [[weights, weights  - gradient * 0.05]]Chapter 1  What Is Natural laNguage proCessINg?\\n\\n9    train = theano.function(inputs=[X, Y], outputs=cost, \\nupdates=update, allow_input_downcast=True)\\n    predict = theano.function(inputs=[X], outputs=predicted_y, \\nallow_input_downcast=True)\\n    for i in range(0, 10):\\n        print(predict(test_x_data[i:i+1]))\\nif __name__ == \\'__main__\\':\\n    model_predict()\\nWhen looking at the functions defined in this sample, notice that T is \\nthe variable defined for a tensor, an important concept that you should \\nbe familiar with. Tensors can be thought of as objects that are similar \\nto vectors; however, they are distinct in that they are often represented \\nby arrays of numbers, or functions, which are governed by specific \\ntransformation rules unique unto themselves. Tensors can specifically be \\na single point or a collection of points in space-time (any function/model \\nthat combines x, y, and z axes plus a dimension of time), or they may be a \\ndefined over a continuum, which is a tensor field . Theano and TensorFlow \\nuse tensors to perform most of the mathematical operations as data is \\npassed through a computational graph, colloquially known as a model .\\nIt is generally suggested that if you do not know Theano, you should \\nfocus on mastering TensorFlow and Keras. Those that are familiar with \\nthe Theano framework, however, may feel free to rewrite the existing \\nTensorFlow code in Theano.Chapter 1  What Is Natural laNguage proCessINg?\\n\\n10 Applications of\\xa0Deep Learning to\\xa0NLP\\nThis section discusses the applications of deep learning to NLP .\\n Introduction to\\xa0NLP Techniques and\\xa0Document \\nClassification\\nIn Chapter 3, we walk through some introductory techniques, such as \\nword tokenization, cleaning text data, term frequency, inverse document \\nfrequency, and more. We will apply these techniques during the course \\nof our data preprocessing as we prepare our data sets for some of the \\nalgorithms reviewed in Chapter 2. Specifically, we focus on classification \\ntasks and review the relative benefits of different feature extraction \\ntechniques when applied to document classification tasks.\\n Topic Modeling\\nIn Chapter 4, we discuss more advanced uses of deep learning, machine \\nlearning, and NLP .\\xa0We start with topic modeling and how to perform it via \\nlatent Dirichlet allocation, as well as non-negative matrix factorization. \\nTopic modeling is simply the process of extracting topics from documents. \\nYou can use these topics for exploratory purposes via data visualization or \\nas a preprocessing step when labeling data.\\n Word Embeddings\\nWord embeddings are a collection of models/techniques for mapping \\nwords (or phrases) to vector space, such that they appear in a high-  \\ndimension al field. From this, you can determine the degree of similarity, \\nor dissimilarity, between one word (or phrase, or document) and another. \\nWhen we project the word vectors into a high-dimensional space, we can \\nenvision that it appears as something like what’s shown in Figure\\xa0 1-3.Chapter 1  What Is Natural laNguage proCessINg?\\n\\n11Ultimately, how you utilize word embeddings is up to your own \\ninterpretation. They can be modified for applications such as spell check, \\nbut can also be used for sentiment analysis, specifically when assessing \\nlarger entities, such as sentences or documents in respect to one another. \\nWe focus simply on how to train the algorithms and how to prepare data to \\ntrain the embeddings themselves.\\n Language Modeling Tasks Involving RNNs\\nIn Chapter 5, we end the book by tackling some of the more advanced NLP \\napplications, which is after you have been familiarized with preprocessing \\ntext data from various format and training different algorithms. \\nSpecifically, we focus on training RNNs to perform tasks such as name \\nentity recognition, answering questions, language generation, and \\ntranslating phrases from one language to another.walked\\nwalkingswam\\nswimming\\nVerb tense\\nFigure 1-3.  Visualization of word embeddingsChapter 1  What Is Natural laNguage proCessINg?\\n\\n12 Summary\\nThe purpose of this book is to familiarize you with the field of natural \\nlanguage processing and then progress to examples in which you \\ncan apply this knowledge. This book covers machine learning where \\nnecessary, although it is assumed that you have already used machine \\nlearning models in a practical setting prior.\\nWhile this book is not intended to be exhaustive nor overly academic, \\nit is my intention to sufficiently cover the material such that readers are \\nable to process more advanced texts more easily than prior to reading \\nit. For those who are more interested in the tangible applications of NLP \\nas the field stands today, it is the vast majority of what is discussed and \\nshown in examples. Without further ado, let’s begin our review of machine \\nlearning, specifically as it relates to the models used in this book.Chapter 1  What Is Natural laNguage proCessINg?\\n\\n13© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_2CHAPTER 2\\nReview of\\xa0Deep \\nLearning\\nYou should be aware that we use deep learning and machine learning \\nmethods throughout this chapter. Although the chapter does not provide \\na comprehensive review of ML/DL, it is critical to discuss a few neural \\nnetwork models because we will be applying them later. This chapter also \\nbriefly familiarizes you with TensorFlow, which is one of the frameworks \\nutilized during the course of the book. All examples in this chapter use toy \\nnumerical data sets, as it would be difficult to both review neural networks \\nand learn to work with text data at the same time.\\nAgain, the purpose of these toy problems is to focus on learning how \\nto create a TensorFlow model, not to create a deployable solution. Moving \\nforward from this chapter, all examples focus on these models with text data.\\n Multilayer Perceptrons and\\xa0Recurrent \\nNeural Networks\\nTraditional neural network models, often referred to as multilayer \\nperceptron models  (MLPs), succeed single-layer perceptron models  (SLPs). \\nMLPs were created to overcome the largest shortcoming of the SLP model, \\nwhich was the inability to effectively handle data that is not linearly \\nseparable. In practical cases, we often observe that multivariate data is\\n\\n14non-linear, rendering the SLP null and void. MLPs are able to overcome \\nthis shortcoming—specifically because MLPs have multiple layers. We’ll \\ngo over this detail and more in depth while walking through some code to \\nmake the example more intuitive. However, let’s begin by looking at the \\nMLP visualization shown in Figure\\xa0 2-1.\\nEach layer of an MLP model is connected by weights, all of which are \\ninitialized randomly from a standard normal distribution. The input layer \\nhas a set number of nodes, each representative of a feature within a neural \\nnetwork. The number of hidden layers can vary, but each of them typically \\nhas the same number of nodes, which the user specifies. In regression, the \\noutput layer has one node. In classification, it has K nodes, where K is the \\nnumber of classes.\\nNext, let’s have an in-depth discussion on how an MLP works and \\ncomplete an example in TensorFlow.\\nFigure 2-1.  Multilayer perceptronChapter 2  review of\\xa0Deep Learning\\n\\n15 Toy Example 1: Modeling Stock Returns \\nwith\\xa0the\\xa0MLP Model\\nLet’s imagine that we are trying to predict Ford Motor Company (F) \\nstock returns given the returns of other stocks on the same day. This is a \\nregression problem, given that we are trying to predict a continuous value. \\nLet’s begin by defining an mlp_model  function with arguments that will be \\nused later, as follows:\\ndef mlp_model(train_data=train_data, learning_rate=0.01, \\niters=100, num_hidden1=256):\\nThis Python function contains all the TensorFlow code that forms the \\nbody of the neural network. In addition to defining the graph, this function \\ninvokes the TensorFlow session that trains the network and makes \\npredictions. We’ll begin by walking through the function, line by line, while \\ntying the code back to the theory behind the model.\\nFirst, let’s address the arguments in our function: train_data  is the \\nvariable that contains our training data; in this example; it is the returns of \\nspecific stocks over a given period of time. The following is the header of \\nour data set:\\n0  0.002647 -0.001609   0.012800   0.000323   0.016132 -0.004664 \\n-0.018598\\n1  0.000704   0.000664   0.023697 -0.006137 -0.004840   0.003555 \\n-0.006664\\n2  0.004221   0.003600   0.002469 -0.010400 -0.008755 -0.002737    \\n0.025367\\n3  0.003328   0.001605   0.050493   0.006897   0.010206   0.002260 \\n-0.007156\\n4  0.001397   0.004052 -0.009965 -0.012720 -0.019235 -0.002255    \\n0.017916Chapter 2  review of\\xa0Deep Learning\\n\\n165 -0.009326 -0.003754 -0.014506 -0.006607 -0.034865   0.011463    \\n0.003844\\n6  0.008446   0.005747   0.022830   0.009312   0.021757 -0.000319    \\n0.023982\\n7  0.002705   0.002623   0.007636   0.020099 -0.007433 -0.008303  \\n-0.004330\\n8 -0.011224 -0.009530 -0.008161 -0.003230 -0.015381 -0.003381  \\n-0.010674\\n9  0.012496   0.010942   0.016750   0.007777   0.001233   0.008724    \\n0.033367\\nEach of the columns r epresent the percentage return of a stock on a \\ngiven day, with our training set containing 1180 observations and our test \\nset containing 582 observations.\\nMoving forward, we come to the learning rate and activation function. \\nIn machine learning literature, the learning rate is often represented by the \\nsymbol η (eta). The learning rate is a scalar value that controls the degree \\nto which the gradient is updated to the parameter that we wish to change. \\nWe can exemplify this technique when referring to the gradient descent \\nupdate method. Let’s first look at the equation, and then we can break it \\ndown iteratively.\\n qq hq tt iN i i\\nNhx y+==- ()- () 112 1S (2.1)\\n qq hqq q tt ii i i\\nNhx yh x+==- ()- () Ñ()1112S \\nIn Equation 2.1, we are updating some parameter, θ, at a given \\ntime step, t. hθ(x)i is equal to the hypothesized label/value, y being the \\nactual label/value, in addition to N being equal to the total number of \\nobservations in the data set we are training on.\\n∇θhθ(x)i is the gradient of the output with respect to the parameters of \\nthe model.Chapter 2  review of\\xa0Deep Learning\\n\\n17Each unit in a neural network (with the exception of the input layer) \\nreceives the weighted sum of inputs multiplied by weights, all of which are \\nsummed with a bias. Mathematically, this can be described in Equation 2.2.\\n yf xw bT=() + , (2.2)\\nIn neural networks, the parameters are the weights and biases. When \\nreferring to Figure\\xa0 2-1, the weights are the lines that connect the units in \\na layer to one another and are typically initialized by randomly sampling \\nfrom a normal distribution. The following is the code where this occurs:\\n     weights = {\\'input\\': tf.Variable(tf.random_normal([train_x.\\nshape[1], num_hidden])),\\n             \\'hidden1\\': tf.Variable(tf.random_normal([num_\\nhidden, num_hidden])),\\n             \\'output\\': tf.Variable(tf.random_normal([num_hidden, \\n1]))}\\n     biases = {\\'input\\': tf.Variable(tf.random_normal([num_\\nhidden])),\\n             \\'hidden1\\': tf.Variable(tf.random_normal([num_\\nhidden])),\\n            \\'output\\': tf.Variable(tf.random_normal([1]))}\\nBecause they are part of the computational graph, weights and biases \\nin TensorFlow must be initialized as TensorFlow variables with the tf.\\nVariable() . TensorFlow thankfully has a function that generates numbers \\nrandomly from a normal distribution called tf.random_normal() , which \\ntakes an array as an argument that specifies the shape of the matrix that you \\nare creating. For people who are new to creating neural networks, choosing \\nthe proper dimensions for the weight and bias units is a typical source of \\nfrustration. The following are some quick pointers to keep in mind :\\n• When r eferring to the weights, the columns of a given \\nlayer need to match the rows of the next layer.Chapter 2  review of\\xa0Deep Learning\\n\\n18• The columns of the wei ghts for every layer must match \\nthe number of units for each layer in the biases.\\n• The o utput layer columns for the weights dictionary \\n(and array shape for the bias dictionary) should be \\nrepresentative of the problem that you are modeling.  \\n(If regression, 1; if classification, N, where N = the \\nnumber of classes).\\nYou might be curious as to why we initialize the weights and biases \\nrandomly. This leads us to one of the key components of neural networks’ \\nsuccess. The easiest explanation is to imagine the following two scenarios:\\n• All weig hts are initialized to 1 . If all the weights are \\ninitialized to 1, every neuron is passed the same value, \\nequal to the weighted sum, plus the bias, and then put \\ninto some activation function, whatever this value may be.\\n• All weig hts are initialized to 0 . Similar to the prior \\nscenario, all the neurons are passed the same value, \\nexcept that this time, the value is definitely zero.\\nThe more general problem associated with weights that are initialized \\nat the same location is that it makes the network susceptible to getting \\nstuck in\\xa0local minima. Let’s imagine an error function, such as the one \\nshown in Figure\\xa0 2-2.Chapter 2  review of\\xa0Deep Learning\\n\\n19Imagine that when we initialize the neural network weights at 0, and \\nsubsequently that when it calculates the error, it yields the value at the Y \\nvariable in Figure\\xa0 2-2. The gradient descent algorithm always gives the \\nsame update for the weights from the first iteration of the algorithm, and \\nit likely gives the same value moving forward. Because of that, we are not \\ntaking advantage of the ability of neural networks to start from any point in \\nthe solution space. This effectively removes the stochastic nature of neural \\nnetworks, and considerably reduces the probability of reaching the best \\npossible solution for the weight optimization problem. Let’s discuss the \\nlearning rate.\\nFigure 2-2.  Error plotChapter 2  review of\\xa0Deep Learning\\n\\n20 Learning Rate\\nThe learning rate is typically a static floating-point value that determines \\nthe degree to which the gradient, or error, affects the update to the \\nparameter that you seek to optimize. In example problems, it is common to \\nsee learning rates initialized anywhere from 0.01 to 1e–4. The initialization \\nof the learning rate parameter is another point worth mentioning, as it can \\naffect the speed at which the algorithm converges upon a solution. Briefly, \\nthe following are two scenarios that are problematic:\\n• The learning rate is too large.  When the learning rate \\nis too large, the error rate moves around in an erratic \\nfashion. Typically, we observe that the algorithm on one \\niteration seems to find a better solution than the prior \\none, only to get worse upon the next, and oscillating \\nbetween these two bounds. In a worst-case scenario, \\nwe eventually start to receive NaN values for error rates, \\nand all solutions effectively become defunct. This is the \\nexploding gradient problem, which I discuss later.\\n• The learning rate is too small.  Although, over time, \\nthis does not yield an incorrect, ultimately, we spend \\nan inordinate amount of time waiting for the solution \\nto converge upon an optimal solution.\\nThe optimal solution is to pick a learning rate that is large enough \\nto minimize the number of iterations needed to converge upon an \\noptimal solution, while not being so large that it passes this solution in \\nits optimization path. Some solutions, such as adaptive learning rate \\nalgorithms, solve the problem of having to grid search or iteratively \\nuse different learning rates. The mlp_model()  function uses the Adam \\n(ada ptive moment estimation) optimization algorithm, which updates the \\nlearning rate aw we learn. I briefly discuss how this algorithm works, and \\nwhy you should use it to expedite learning rate optimization.Chapter 2  review of\\xa0Deep Learning\\n\\n21Adam was first described in a paper that written by Diederik Kingma \\nand Jimmy Lei Ba. Adam specifically seeks to optimize learning rates by \\nestimating the first and second moments of the gradients. For those who \\nare unfamiliar, moments  are defined as specific measures of the shape of a \\nset of points. As it relates to statistics, these points are typically the values \\nwithin a probability distribution. We can define the zeroth moment as the \\ntotal probability; the first moment as the mean; and second moment as the \\nvariance. In this paper, they describe the optimal parameters for Adam, in \\naddition to some initial assumptions, as follows:\\n• α\\xa0=\\xa0Stepsize; α\\xa0≔\\xa00.001,  \\xa0ϵ\\xa0=\\xa010−8\\n• β1, β2\\xa0=\\xa0Exponential decay rates for\\xa01st and 2nd\\xa0moment \\nestimateions\\xa0 β1\\xa0=\\xa00.9,  β2\\xa0=\\xa00.999; β1, β2\\xa0∈\\xa0[0, 1)\\n• f(θ)\\xa0=\\xa0Stochastic objective function that \\nwe\\xa0are\\xa0optimizing with parameters\\xa0 θ\\n• m\\xa0=\\xa01st\\xa0moment vector,  v\\xa0=\\xa02nd\\xa0moment vector\\xa0(Both \\ninitialized\\xa0as\\xa00s)\\nWith this in mind, although we have not converged upon an optimal \\nsolution, the following is the algorithm that we use:\\n• gt\\xa0=\\xa0∇θft(θt\\xa0−\\xa01)\\n• ˆˆ,mv=-Bias corrected first and second moment estimates  \\nresppect ively;\\n• mm gv vgtt tt tt ::==bb bb11 12 12211 *+ -() ** +-() *--\\n• ˆ:, : mmvv\\ntt\\nt tt\\nt==1112--bb\\uf0b5\\n• qq attt\\ntm\\nv:= -*-1ˆ\\n() +\\uf0b7\\uf0f2\\nWhile the preceding formulae describe Adam when optimizing \\none parameter, we can extrapolate the formulae to adjust for multiple \\nparameters (as is the case with multivariate problems). In the paper, Adam Chapter 2  review of\\xa0Deep Learning\\n\\n22outperformed other standard optimization techniques and was seen as the \\ndefault learning rate optimization algorithm.\\nAs for the final parameters, num_hidden  refers to the number of units in \\nthe hidden layer(s). A commonly referenced rule of thumb is to make this \\nnumber equal to the number of inputs plus the number of outputs, and \\nthen multiplied by 2/3.\\nEpochs  refers to the number of times the algorithm should iterate \\nthrough the entirety of the training set. Given that this is situation \\ndependent, there is no general suggestible number of epochs that a neural \\nnetwork should be trained. However, a suggestible method is to pick an \\narbitrarily large number (1500, for example), plot the training error, and \\nthen observe which number of epochs is sufficient. If needed, you can \\nenlarge the upper limit to allow the model to optimize its solution further.\\nNow that I have finished discussing the parameters, let’s walk through \\nthe architecture, code, and mathematics of the multilayer perceptron, as \\nfollows:\\n#Creating training and test sets\\ntrain_x, train_y = train_data[0:int(len(train_data)*.67), \\n1:train_data.shape[1]], train_data[0:int(len(train_data)*.67), 0]\\ntest_x, test_y = train_data[int(len(train_data)*.67):, 1:train_\\ndata.shape[1]], train_data[int(len(train_data)*.67):, 0]\\nObserve that we are creating both a training set and a test set. The \\ntraining and test sets contain 67% and 33%, respectively, of the original data \\nset labeled train_data . It is suggested that machine learning problems \\nhave these two data sets, at a minimum. It is optional to create a validation \\nset as well, but this step is omitted for the sake of brevity in this example.\\nNext, let’s discuss the following important aspect of working with \\nTensorFlow:\\n#Creating placeholder values and instantiating weights and \\nbiases as dictionariesChapter 2  review of\\xa0Deep Learning\\n\\n23X = tf.placeholder(\\'float\\', shape = (None, 7))\\nY = tf.placeholder(\\'float\\', shape = (None, 1))\\nWhen working in TensorFlow, it is important to refer to machine \\nlearning models as graphs , since we are creating computational graphs \\nwith different tensor objects. Any typical deep learning or machine \\nlearning model expects an explanatory and response variable; however, \\nwe need to specify what these are. Since they are not a part of the graph, \\nbut are representational objects that we are passing data through, they are \\ndefined as placeholder variables , which we can access from TensorFlow \\n(imported as tf) by using tf.placeholder() . The three arguments for this \\nfunction are dtype (data type), shape, and name. dtype and shape are the \\nonly required arguments. The following are quick rules of thumb:\\n• Generally, the shape of the X and Y variables should \\nbe initialized as a tuple. When working with a two-  \\ndimensional data set, the shape of the X variable \\nshould be (none, number of features), and the shape \\nof the Y variable should be (none, [1 if regression, N if \\nclassification]), where N is the number of classes.\\n• The data type specified for these placeholders should \\nreflect the values that you are passing through them. \\nIn this instance, we are passing through a matrix of \\nfloating-point values and predicting a floating-point \\nvalue, so both placeholders for the response and \\nexplanatory variables have the float data type. In \\nthe instance that this was a classification problem, \\nassuming the same data passed through the \\nexplanatory variable, the response variable has the int \\ndata type since the labels for the classes are integers.Chapter 2  review of\\xa0Deep Learning\\n\\n24Since I discussed the weights in the neural network already, let’s get \\nto the heart of the neural network structure: the input through the output \\nlayers, as shown in the following code (inside mlp_model()  function):\\n#Passing data through input, hidden, and output layers\\ninput_layer = tf.add(tf.matmul(X, weights[\\'input\\']), \\nbiases[\\'input\\']) (1)\\ninput_layer = tf.nn.sigmoid(input_layer) (2)\\ninput_layer = tf.nn.dropout(input_layer, 0.20) (3)\\nhidden_layer = tf.add(tf.multiply(input_layer, \\nweights[\\'hidden1\\']), biases[\\'hidden1\\'])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nhidden_layer = tf.nn.dropout(hidden_layer, 0.20)\\noutput_layer = tf.add(tf.multiply(hidden_layer, weights \\n[\\'output\\']),biases[\\'output\\']) (4)\\nWhen looking at the first line of highlighted code (1), we see the input \\nlayer operation. Mathematically, operations from one neural network layer \\nto the next can be represented by the following equation:\\n layerf Xw biaskk kT\\nk =*() + ()  (2.2.1)\\nf(x) is equal to some activation function. The output from this \\noperation is passed to the next layer, where the same operation is run, \\nincluding any operations placed between layers. In TensorFlow, there are \\nbuilt-in mathematical operations to represent the preceding equation:  \\ntf.add()  and tf.matmul() .\\nAfter we create the output, which in this instance is a matrix of \\nshape (1, 256), we pass it to an activation function. In the second line of \\nhighlighted code (2), we first pass the weighted sum of the inputs and bias \\nto a sigmoid activation function, given in Equation 2.3.\\n s=+æ\\nèçö\\nø÷ -1\\n1ex (2.3)Chapter 2  review of\\xa0Deep Learning\\n\\n25e is the exponential function. Activation functions serve as a way to \\nscale the outputs from Equation 2.2, and are sometimes directly related to \\nhow we classify outputs. More importantly, this is the core component of \\nthe neural network that introduces non-linearity to the learning process. \\nSimply stated, if we use a linear activation function, where f(x) = x, we are \\nsimply repetitively passing the outputs of a linear function from the input \\nlayer to the output layer. Figure\\xa0 2-3 illustrates this activation function.\\nAlthough the range here is from –6 to 6, the function essentially looks \\nlike −∞  to ∞, in that there are asymptotes at 0 and 1 as X grows infinitely \\nlarger or infinitely smaller, respectively. This function is one of the more \\ncommon activation functions utilized in neural networks, which we use in \\nthe first layer.-6 -5 -4 -3 -2 -1 01 23 45 61.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0Activation Function Ouptut\\nX ValueSigmoid Activation Function\\nFigure 2-3.  Sigmoid activation functionChapter 2  review of\\xa0Deep Learning\\n\\n26Also, we defined the derivative of this function, which is important \\nin mathematically explaining the vanishing gradient problem (discussed \\nlater in the chapter). Although going through all the activation functions \\nin neural networks would be exhaustive, it is worth discussing the other \\nactivation function that this neural network utilizes. The hidden layer uses a \\nReLU activation function, which is mathematically defined in Equation 2.4.\\n ReLU xx() = () max0,  (2.4)\\nThe function is illustrated in Figure\\xa0 2-4.\\nFigure 2-4.  ReLU activation functionChapter 2  review of\\xa0Deep Learning\\n\\n27Both mathematically and visually, the ReLU activation function is \\nsimple. The output of a ReLU is a matrix of 0s, with some positive values. \\nOne of the major benefits of the ReLU activation function lies in the fact \\nthat it produces a sparse matrix as its output. This attribute is ultimately \\nwhy I have decided to include it as the activation function in the hidden \\nlayer, particularly as it relates to the vanishing gradient problem.\\n Vanishing Gradients and\\xa0Why ReLU Helps \\nto\\xa0Prevent Them\\nThe vanishing gradient problem is specific to the training of neural \\nnetworks, and it is part of the improvements that researchers sought to \\nmake with LSTM over RNN (both are discussed later in this chapter). The \\nvanishing gradient problem is a phenomenon observed when the gradient \\ngets so small that the updates to weights from one iteration to the next \\neither stops completely or is considerably negligible.\\nLogically, what proceeds is a situation in which the neural network \\neffectively stops training. In most cases, this results in poor weight \\noptimization, and ultimately, bad training and test set performance. Why \\nthis happens can be explained precisely by how the updates for each of the \\nweights are calculated:\\nWhen we look at Figure\\xa0 2-3, we see the derivative of the sigmoid \\nfunction. The majority of the function’s derivate falls in a narrow range, \\nwith most of the values being close to 0. When considering how to \\ncalculate the gradient of differing hidden layers, this is precisely what \\ncauses a problem as our network gets deeper. Mathematically, this is \\nrepresented by the following equation:\\n ¶\\n¶=¶\\n¶¶\\n¶¶\\n¶¶\\n¶=åE\\nWE\\nyy\\nss\\nss\\nWk kk 3\\n02\\n3\\n33\\n33\\n\\uf0b5 Chapter 2  review of\\xa0Deep Learning\\n\\n28As you can see, when we backpropagate the error to layer k, which in \\nthis example is 0 (the input layer), we are multiplying several derivatives of \\nthe activation function’s output several times. This is a brief explanation of \\nthe chain rule and underlies most of a neural networks’ backpropagation \\ntraining algorithm. The chain rule is a formula that specifies how to calculate \\na derivative that is composed of two or more functions. Assume that we have \\na two-layer neural network. Let’s also assume that our respective gradients \\nare 0.001 and 0.002. This yields 2 e–6 as a respective gradient of the output \\nlayer. Our update to the next gradient would be described as negligible.\\nYou should know that any activation function that yields non-  sparse \\noutputs, particularly when used for multiple layers in succession, \\ntypically causes vanishing gradients. We are able to substantially mitigate \\nthis problem by using a combination of sparse and non-sparse output \\nactivation functions, or exclusively utilize non-spare activation functions. \\nWe illustrate an example of such a neural network in the mlp_model()  \\nfunction. For now, however, let’s take a look at one last activation layer \\nbefore we finish analyzing this MLP .\\nObserve that after every activation layer, we use the dropout layer , invoked \\nby tf.nn.dropout() . Dropout layers have the same dimensions as the layer \\npreceding them; however, they arbitrarily set a random selection of weights’ \\nvalues to 0, effectively “shutting off” the neurons that are connected to them. \\nIn every iteration, there are a different set of random neurons that shut off. \\nThe benefit of using dropout is to prevent overfitting, which is the instance in \\nwhich a model performs well in training data but poorly in test data.\\nThere are a multitude of factors that can cause overfitting, including (but \\nnot limited to) not having enough training data or not cross-  validating data \\n(which induces a model to memorize idiosyncrasies of a given orientation of \\na data set rather than generalizing to the distribution underlying the data). \\nAlthough you should solve issues like these first, adding dropout is not a bad \\nidea. When you execute functions without dropout, you notice overfitting \\nrelative to the models that do contain dropout.\\nLet’s discuss some final MLP topics—specifically, the key components \\nto what causes the model to learn.Chapter 2  review of\\xa0Deep Learning\\n\\n29 Loss Functions and\\xa0Backpropagation\\nLoss functions are specifically how we define the degree to which our \\nmodel was incorrect. In regression, the most typical choices are mean \\nsquared error  (MSE) or root mean squared error  (RMSE). Mathematically, \\nthey are defined as follows:\\n MSENhx y\\niN\\nii= ()- ()\\n=å1\\n12\\nq (2.5)\\n RMSENhx y\\niN\\nii= ()- ()\\n=å1\\n12\\nq (2.6)\\nerror = tf.reduce_mean(tf.pow(output_layer\\xa0– Y,2)) (mean squared error \\nin code)\\nIntuitively, MSE (see Equation 2.5) provides a method for assessing \\nwhat was the average error over all predictions in a given epoch. RMSE \\n(see Equation 2.6) provides the same statistic, but takes the square root \\nof the MSE value. The benefit of RMSE is that it provides a statistic in \\nthe same unit as the predicted value, allowing the user to assess the \\nperformance of the model more precisely. MSE does not have this benefit, \\nand as such, it becomes less interpretable—except in the sense that a lower \\nMSE from one epoch to the next is desirable.\\nAs an example, if we are predicting money, what does it mean that our \\nprediction is $0.30 squared inaccurate? While we can tell that we have a \\nbetter solution if the next epoch yields an MSE of $0.10, it is much harder \\nto tell precisely what an MSE of $0.10 translates to in a given prediction. \\nWe compare the results of using RMSE vs. MSE in the final toy example in \\nthe chapter. In natural language processing, however, we more often deal \\nwith error functions reserved for classification tasks. With that in mind, \\nyou should be accustomed to the following formulas.Chapter 2  review of\\xa0Deep Learning\\n\\n30The binary cross entropy is\\n \\uf04cyh yp ypxi,q() () =- ()+-() -() logl og ) 11 (2.7)\\nThe multiclass cross entropy is\\n \\uf04cyh ssxijyi,,q() () =- + () max0 D (2.8)\\nCross entropy  is the number of bits needed to identify an event drawn \\nfrom a set. The same principles (with respect to training using an MSE \\nor RMSE loss function) are carried when using a cross-entropy-based \\nloss function. Our objective is to optimize the weights in a direction that \\nminimizes the error as much as possible.\\nAt this point, we have walked through the MLP from the initialization \\nof the parameters, what they mean, how the layer moves from each layer, \\nwhat the activation functions do to it, and how the error is calculated. Next, \\nlet’s dig into recurrent neural networks, long short-term memory, and their \\nrelative importance in the field of natural language processing.\\n Recurrent Neural Networks and\\xa0Long Short-Term \\nMemory\\nDespite the relative robustness of MLPs, they have their limitations. The \\nmodel assumes independence between inputs and outputs, making \\nit a suboptimal choice for problems in which the output of function is \\nstatistically dependent on the preceding inputs. As this relates to natural \\nlanguage processing (NLP), there are tasks that MLPs might be particularly \\nuseful for, such as sentiment analysis. In these problems, one body of text \\nbeing classified as negative is not dependent on assessing the sentiment of \\na separate body of text.\\nAs an example, I wouldn’t need to read multiple restaurant reviews to \\ndetermine whether an individual review is positive or negative. It can be \\ndetermined by the attributes of a given observation. However, this is not Chapter 2  review of\\xa0Deep Learning\\n\\n31always the type of NLP problem we encounter. For example, let’s assume \\nthat we are trying to spell-check on the following sentences:\\n“I am happy that we are going too the mall!”\\n“I am happy to. That class was excellent. ”\\nBoth sentences are incorrect in their usage of the words too and to, \\nrespectively, because of the context in which they appear. We must use the \\nsequence of words prior, and perhaps even the words after, to determine \\nwhat is incorrect. Another similar problem would be predicting words in a \\nsentence; for example, let’s look at the following sentence.\\n“I was born in Germany. I speak _______. ”\\nAlthough there isn’t necessarily one answer to complete this sentence, \\nas being born in Germany does not predetermine someone to speaking \\nonly German, there is a high probability that the missing word is German . \\nHowever, we can only say that because of the context that surrounds the \\nwords, and assuming that the neural network was trained on sentences (or \\nphrases) and has a similar structure. Regardless, these types of problems \\ncall for a model that can accommodate some sort of memory related to the \\nprior inputs, which brings us to recurrent neural network. Figure\\xa0 2-5 shows \\nthe structure of an RNN.\\no\\nVWW\\nW\\nUV\\nU\\nxs\\nUnfoldot-1\\nxt-1st-1\\nWW\\nUVot +1\\nxt +1st +1\\nUVot\\nxtst\\nFigure 2-5.  Recurrent neural networkChapter 2  review of\\xa0Deep Learning\\n\\n32It is important to examine the structure of the RNN as it relates to \\nresolving the statistical dependency problem. Similar to the prior example, \\nlet’s walk through some example code in TensorFlow to illustrate the \\nmodel structure using a toy problem. Similar to the MLP , we will work with \\na toy problem to create a function that loads and preprocesses our data for \\nthe neural network, and then make a function to build our neural network. \\nThe following is the beginning of the function:\\ndef build_rnn(learning_rate=0.02, epochs=100, state_size=4):\\nThe first two arguments should be familiar. They represent the same \\nconcepts as in the MLP example. However, we have a new argument called \\nstate_size . In a vanilla RNN, the model we are building here, we pass \\nwhat is called the hidden state  from a given time step forward. The hidden \\nstate is similar to the hidden layer of an MLP in that it is a function of the \\nhidden states at previous time steps. The following defines the hidden state \\nand output as\\n hf Wx Wh btx ht hh th =+ + ()-1  (2.9)\\n yW hbth ot o =+  (2.10)\\nht is the hidden state, W is the weight matrix, b is the bias array, y is the \\noutput of the function, and f(x) is the activation function of our choosing.\\n Toy Example 2: Modeling Stock Returns \\nwith\\xa0the\\xa0RNN Model\\nUsing the code in the build_rnn()  function, observe the following.\\n#Loading data\\n     x, y = load_data(); scaler = MinMaxScaler(feature_range=(0, 1))\\n    x, y = scaler.fit_transform(x), scaler.fit_transform(y)Chapter 2  review of\\xa0Deep Learning\\n\\n33     train_x, train_y = x[0:int(math.floor(len(x)*.67)),   :], \\ny[0:int(math.floor(len(y)*.67))]\\n    #Creating weights and biases dictionaries\\n     weights = {\\'input\\': tf.Variable(tf.random_normal([state_\\nsize+1, state_size])),\\n         \\'output\\': tf.Variable(tf.random_normal([state_size, \\ntrain_y.shape[1]]))}\\n     biases = {\\'input\\': tf.Variable(tf.random_normal([1, state_\\nsize])),\\n         \\'output\\': tf.Variable(tf.random_normal([1, train_y.\\nshape[1]]))}\\nWe b egin by loading the training and test data, performing a similar \\nsplit in the test set such that the first 67% of the complete data set becomes \\nthe training set and the remaining 33% becomes the test set. In this \\ninstance, we distinguish between two classes, 0 or 1, indicating whether \\nthe price went up or down. Moving forward, however, we must refer back \\nto the state size parameter to understand the shape of the matrices we \\nproduce, again as TensorFlow variables, for the weight and bias matrices.\\nTo crystallize your understanding of the state size parameter, refer to \\nFigure\\xa0 2-5, in which the center of the neural network represents a state. We \\nmultiply the given input, as well as the previous state, by a weight matrix, \\nand sum all of this with the bias. Similar to the MLP , the weighted sum \\nvalue forms the input for the activation function.\\nThe output of the activation function forms the hidden state at time \\nstep t, whose value becomes part of the weighted sum in Equation 2.10 . \\nThe value of this matrix application ultimately forms the output for the \\nRNN.\\xa0We repeat these operations for as many states that we have, which \\nis equal to the number of inputs that we pass through the neural network. \\nWhen referring back to the image, this is what is meant by the RNN being \\n“unfolded. ” The state_size  in our example is set to 4, meaning that we are \\ninputting four input sequences before we make a prediction.Chapter 2  review of\\xa0Deep Learning\\n\\n34Let’s now walk through the TensorFlow code associated with these \\noperations.\\n#Defining placeholders and variables\\n    X = tf.placeholder(tf.float32, [batch_size, train_x.shape[1]])\\n    Y = tf.placeholder(tf.int32, [batch_size, train_y.shape[1]])\\n     init_state = tf.placeholder(tf.float32, [batch_size, state_\\nsize])\\n    input_series = tf.unstack(X, axis=1)\\n    labels = tf.unstack(Y, axis=1)\\n    current_state = init_state\\n    hidden_states = []\\n    #Passing values from one hidden state to the next\\n     for input in input_series: #Evaluating each input within \\nthe series of inputs\\n         input = tf.reshape(input, [batch_size, 1]) #Reshaping \\ninput into MxN tensor\\n         input_state = tf.concat([input, current_state], axis=1) \\n#Concatenating input and current state tensors\\n         _hidden_state = tf.tanh(tf.add(tf.matmul(input_\\nstate, weights[\\'input\\']), biases[\\'input\\'])) #Tanh \\ntransformation\\n         hidden_states.append(_hidden_state) #Appending the next \\nstate\\n        current_state = _hidden_state #Updating the current state\\nSimilar to the MLP model, we need to define place holder variables for \\nboth the x and y tensors that our data will pass through. However, a new \\nplaceholder will be here, which is the init_state , representing the initial \\nstate matrix. Notice that the current state is the init_state  placeholder for \\nthe first iteration through the next. It also holds the same dimensions and \\nexpects the same data type.Chapter 2  review of\\xa0Deep Learning\\n\\n35Moving forward, we iterate through every input_sequence  in the data set, \\nwhere _hidden_state  is the Python definition of formula (see Equation 2.9). \\nFinally, we must come to the output state, given by the following:\\nlogits = [tf.add(tf.matmul(state, weights[\\'output\\']), \\nbiases[\\'output\\']) for state in hidden_states]\\nThe code here is representative of Equation 2.10 . However, this will \\nonly give us a floating-point decimal, which we need to convert into a label \\nsomehow. This brings us to an activation function which will be important \\nto remember for multiclass classification, and therefore for the remainder \\nof this text, the softmax activation function. Subsequently, we define this \\nactivation function as the following:\\n Sye\\neiy\\niNyi\\ni () =æ\\nèç\\nçö\\nø÷\\n÷\\n=å1  (2.11)\\nWhen you look at the formula, we are summing some value over all \\nthe possible values. As such, we define this as a probability score. When \\nrelating this back to classification, particularly with the RNN, we are \\noutputting the relative probability of an observation being of one class vs \\nanother (or others). The label we choose in this instance is the one with the \\nhighest relative score, meaning that we choose a given label k because it \\nhas the highest probability of being true based on the model’s prediction. \\nEquation 2.11  is subsequently represented in the code by the following line:\\npredicted_labels = [tf.nn.softmax(logit) for logit in logits] \\n#predictions for each logit within the series\\nBeing that this is a classification problem, we use a cross entropy–\\nbased loss function and for this toy example we will use the gradient \\ndescent algorithm, both of which were elaborated upon in the prior \\nsection MLPs. Invoking the TensorFlow session also is performed in the \\nsame fashion as it would be for the MLP graph (and furthermore for all \\nTensorFlow computational graphs). In a slight derivation from the MLP , Chapter 2  review of\\xa0Deep Learning\\n\\n36we calculate errors at each time step of an unrolled network and sum these \\nerrors. This is known as backpropagation through time  (BPTT), which is \\nutilized specifically because the same weight matrix is used for every time \\nstep. As such, the only changing variable besides the input is the hidden \\nstate matrix. As such, we can calculate each time step’s contribution to the \\nerror. We then sum these time step errors to get the error. Mathematically, \\nthis is represented by the following equation:\\n ¶\\n¶=¶\\n¶¶\\n¶¶\\n¶¶\\n¶=åE\\nWE\\nyy\\nss\\nss\\nWk kk 3\\n03\\n3\\n33\\n33\\n\\uf0b5 \\nThis is an application of the chain rule, as described briefly in the \\nsection on how we backpropagate the error from the output layer back to \\nthe input layer to update the weights with respect to their contribution to \\nthe total error. BPPT applies the same logic; instead, we treat the time steps \\nas the layers. However, although RNNs solved many problems of MLPs, \\nthey had relative limitations, which you should be aware of.\\nOne of the largest drawbacks of RNNs is that the vanishing gradient \\nproblem reappears. However, instead of it being due to having very \\ndeep neural network layers, it is caused by trying to evaluate arbitrarily \\nlong sequences. The activation function used in RNNs is often the tanh \\nactivation function. Mathematically, we define this as follows:\\n tanh x()=-\\n+-\\n-ee\\neexx\\nxx  \\nFigure 2-6 illustrates the activation function.Chapter 2  review of\\xa0Deep Learning\\n\\n37Similar to the problem with the sigmoid activation function, the \\nderivative of the tanh function can 0, such that when backpropagated \\nover large sequences results in a gradient that is equal to 0. Similar to the \\nMLP , this can cause problems with learning. Depending on the choice of \\nactivation function, we also might experience the opposite of the vanishing \\ngradient problem—the exploding gradient. Simply stated, this is the result \\nof the gradients appearing as NaN values. There are couple of solutions for \\nthe vanishing gradient function in RNNs. Among them are to try weight \\nregularization via an L1 or L2 norm, or to try different activation functions \\nas we did in the MLP , utilizing functions such as ReLU.\\xa0However, one of the \\nmore straightforward solutions is to use a model devised in the 1990s by \\nSepp Hochreiter and Jürgen Schmidhuber: the long short-term memory unit, \\nor LSTM. Let’s start with what this model looks like, as shown in Figure\\xa0 2-7.\\nFigure 2-6.  Tanh activation and derivative functionChapter 2  review of\\xa0Deep Learning\\n\\n38LSTMs are distinguished structurally by the fact that we observe them \\nas blocks, or units, rather than the traditional structure a neural network \\noften appears as. That said, the same principles are generally applied here. \\nHowever, we have an improvement over the hidden state from the vanilla \\nRNN.\\xa0I will walk through the formulae associated with the LSTM.\\n iW xW hW cbtx it hith ct i =+ ++ ()-- s11  (2.12)\\n fW xW hW cbtx ft hf th ft f =+ ++ () -- s11  (2.13)\\n cf ci Wx Wh btt tt xc th ct c =+ ++ ()--\\uf06f\\uf06f11 tanh  (2.14)\\n oW xW hW cbtx ot ho tc ot o =+ ++ ()- s1  (2.15)\\n ho ctt t = ()\\uf06ftanh  (2.16)\\nit is the input gate, ft is the forget gate, ct is the cell state, ot is the output \\ngate, htis the output vector, σ is the sigmoid activation function, and tanh is \\nthe tanh activation function. Both the hidden and cell states are initialized \\nat 0 upon initialization of the algorithm.\\nThe formulae from the LSTM is similar to that of the vanilla RNN, \\nhowever there is some slight complexity added. Initially, let’s draw our \\nattention to the diagram, specifically the LSTM unit in the center, and \\nunderstand the directional flow as they relate to the formulae. x\\nxx+\\ntanhtanhx\\nx+\\ntanhx\\nx+\\ntanhx\\nx+\\ntanhtanhx\\nx+\\ntanhtanhAA\\nXt-1ht-1 ht ht+1\\nXt+1 Xt\\nFigure 2-7.  LSTM units/blocksChapter 2  review of\\xa0Deep Learning\\n\\n39Preliminarily, let’s discuss the notation. Each block, denoted by \\n , \\nrepresents a neural network layer, through which we pass through values. \\nThe horizontal lines with arrows represent the vectors and direction in \\nwhich the data moves. After it moves through a neural network layer, the \\ndata often is passed to a pointwise operation object, represented by \\n .\\nNow that I have discussed how to read the diagram, let’s dive in deeper.\\nLSTMs are distinguished by having gates that regulate the information \\nthat passes through individual units, as well as what information passes to \\nthe next unit. Individually, these are the input gate, the output gate, and \\nthe forget gate. In addition to these three gates, an LSTM also contains a \\ncell, which is an important aspect of the unit.\\nOn the diagram, the cell is represented by the horizontal line, and it \\nis mathematically represented in Equation 2.14 . The cell state is similar \\nto the hidden state, featured here as well as in the RNN, except there is \\ndiscretion as to how much information we pass from one unit to the next. \\nWhen looking at the diagram, an input, xt, is passed through the input \\ngate. Here the neural network is put through a neural network layer, \\nwith a sigmoid activation function that passes the output to a pointwise \\nmultiplication operator. This operation is combined with the forget gate, ft, \\nwhich is the entirety of Equation 2.14 .\\nAbove all, what you should take away from this operation is that its \\noutput is a number between and including 0 and 1. The closer the number \\nis to 1, information is increasingly passed to the subsequent unit. In \\ncontrast, the closer the number is to 0, information is decreasingly passed \\nto the subsequent unit.\\nIn Equation 2.13 , the forget gate, is what regulates this acceptance of \\ninformation, which is represented by ct\\xa0−\\xa01.\\nMoving to Equation 2.15  and relating it to the diagram, this is the \\nneural network layer furthest to the right that is passed through another \\nsigmoid layer, in similar fashion in to the input layer. The output of this \\nsigmoid activated neural network layer is then multiplied with the tanh \\nactivated cell state vector, in Equation 2.16  Finally, we pass both the Chapter 2  review of\\xa0Deep Learning\\n\\n40cell state vector and the output vector to the next LSTM unit. While I \\ndo not draw out the LSTM in the same fashion as the RNN, I utilize the \\nTensorFlow API’s implementation of the LSTM.\\n Toy Example 3: Modeling Stock Returns \\nwith\\xa0the\\xa0LSTM Model\\nAs was the case in our prior neural network examples, we must still create \\nTensorFlow placeholders and variables. For this example, the LSTM \\nexpects sequences of data, which we facilitate by first creating a three-  \\ndimensional X placeholder variables. To avoid debugging issues when \\ndeploying this API with different data sets, you should be careful to read the \\nfollowing instructions carefully .\\n    X = tf.placeholder(tf.float32, (None, None, train_x.shape[1]))\\n    Y = tf.placeholder(tf.float32, (None, train_y.shape[1]))\\n     weights = {\\'output\\': tf.Variable(tf.random_normal([n_\\nhidden, train_y.shape[1]]))}\\n     biases = {\\'output\\':  tf.Variable(tf.random_normal([train_y.\\nshape[1]]))}\\n    input_series = tf.reshape(X, [-1, train_x.shape[1]])\\n    input_series = tf.split(input_series, train_x.shape[1], 1)\\n     lstm = rnn.core_rnn_cell.BasicLSTMCell(num_units=n_hidden, \\nforget_bias=1.0, reuse=None, state_is_tuple=True)\\n     _outputs, states = rnn.static_rnn(lstm, input_series, \\ndtype=tf.float32)\\n     predictions = tf.add(tf.matmul(_outputs[-1], \\nweights[\\'output\\']), biases[\\'output\\'])\\n     accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax \\n(tf.nn.softmax(predictions), 1)tf.argmax(Y, 1)), dtype=tf.\\n     float32)),Chapter 2  review of\\xa0Deep Learning\\n\\n41     error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_\\nlogits(labels=Y, logits=predictions))\\n     adam_optimizer = tf.train.AdamOptimizer(learning_rate).\\nminimize(error)\\nWhen creating a sequence of variables, we start by creating a three-  \\ndimensional placeholder named X, which is what we feed our data into. \\nWe transform this variable by creating a two-dimensional vector of the \\nobservations with the tf.reshape() .\\nNext, we create a tensor object for each of these observations with the tf.\\nsplit()  function, which are then stored as a list underneath  input_series .\\nThen, we can create an LSTM cell using the BasicLSTMCell()  function. \\nThe static_rnn()  function accepts any type of RNN cell, so you can \\nutilize other types of RNNs, such as GRUs or vanilla RNNs, and the inputs. \\nEverything else follows the same pattern as the prior examples, in that we \\ncreate TensorFlow variables to calculate accuracy, the error rate, and the \\nAdam optimizer.\\n Summary\\nWe have reached the end of our brief, but necessary review of machine \\nlearning before we dive deeply into tackling problems using these models \\non text data. However, it is important for us to review some key concepts:\\n• Model choice matters!  Understand the data that you \\nare analyzing. Is the label you are predicting dependent \\non other prior observed labels, or are these inputs \\nand outputs statistically independent of one another? \\nFailing to inspect these key properties of your data \\nbeforehand will waste time and provide you with \\nsuboptimal results. Do not skip these steps.Chapter 2  review of\\xa0Deep Learning\\n\\n42• Parameter choice matters!  Picking the right model \\nfor a problem is the first step, but you have to tune this \\nmodel properly to get optimal results. Inspect model \\nperformance when you alter the number of hidden \\nunits and epochs. I suggest utilizing algorithms such \\nas Adam to tune the learning rate while the network \\nis training. Where possible, grid search or use similar \\nreactive search methods to find better parameters.\\n• Activation functions matter!  Be mindful of how your \\nneural network behaves with respect to the vanishing \\ngradient problem, particularly if you are working with \\nlong sequences or have very deep neural networks.\\nWith these concepts in mind, there is one that we did not cover in this \\nchapter: data preprocessing. It is more appropriate to discuss with the \\nproblems we are facing.\\nLet’s move from this chapter and get into the weeds of natural \\nlanguage processing with a couple of example problems. In the next \\nchapter, we walk through a couple of methods for preprocessing text, \\ndiscuss their relative advantages and disadvantages, and compare model \\nperformance when using them.Chapter 2  review of\\xa0Deep Learning\\n\\n43© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_3CHAPTER 3\\nWorking with\\xa0 \\nRaw Text\\nThose who approach NLP with the intention of applying deep learning \\nare most likely immediately confronted with a simple question: How \\ndoes a machine learning algorithm learn to interpret text data? Similar \\nto the situations in which a feature set may have a categorical feature, we \\nmust perform some preprocessing. While the preprocessing we perform \\nin NLP often is more involved than simply converting a categorical \\nfeature using label encoding, the principle is the same. We need to find \\na way to represent individual observations of texts as a row, and encode \\na static number of features, represented as columns, across all of these \\nobservations. As such, feature extraction becomes the most important \\naspect of text preprocessing.\\nThankfully, there has been a considerable amount of work, \\nincluding ongoing work, to develop preprocessing algorithms of various \\ncomplexities. This chapter introduces these preprocessing methods, walks \\nthrough which situations they each work well with, and applies them to \\nexample NLP problems that focus on document classification. Let’s start \\nby discussing what you should be aware of prior to performing feature \\nextraction from text.\\n\\n44 Tokenization and\\xa0Stop Words\\nWhen you are working with raw text data, particularly if it uses a web \\ncrawler to pull information from a website, for example, you must assume \\nthat not all of the text will be useful to extract features from. In fact, it is \\nlikely that more noise will be introduced to the data set and make the \\ntraining of a given machine learning model less effective. As such, I suggest \\nthat you perform preliminary steps. Let’s walk through these steps using \\nthe following sample text.\\nsample_text = \"\\'I am a student from the University of Alabama. I\\nwas born in Ontario, Canada and I am a huge fan of the United \\nStates. I am going to get a degree in Philosophy to improve\\nmy chances of becoming a Philosophy professor. I have been\\nworking towards this goal for 4 years. I am currently enrolled\\nin a PhD program. It is very difficult, but I am confident that\\nit will be a good decision\"\\'\\nWhen the sample_text  variable prints, there is the following output:\\n\\'I am a student from the University of Alabama. I\\nwas born in Ontario, Canada and I am a huge fan of the United\\nStates. I am going to get a degree in Philosophy to improve my\\nchances of becoming a Philosophy professor. I have been working\\ntowards this goal for 4 years. I am currently enrolled in a PhD \\nprogram. It is very difficult, but I am confident that it will\\nbe a good decision\\'\\nYou should observe that the computer reads bodies of text, even if \\npunctuated, as single string objects. Because of this, we need to find a way \\nto separate this single body of text so that the computer evaluates each \\nword as an individual string object. This brings us to the concept of word \\ntokenization , which is simply the process of separating a single string Chapter 3  Working With\\xa0raW text\\n\\n45object, usually a body of text of varying length, into individual tokens \\nthat represent words or characters that we would like to evaluate further. \\nAlthough you can find ways to implement this from scratch, for brevity’s \\nsake, I suggest that you utilize the Natural Language Toolkit (NLTK) \\nmodule.\\nNLTK allows you to use some of the more basic NLP functionalities, \\nas well as pretrained models for different tasks. It is my goal to allow \\nyou to train your own models, so we will not be working with any of the \\npretrained models in NLTK.\\xa0However, you should read through the NLTK \\nmodule documentation to become familiar with certain functions and \\nalgorithms that expedite text preprocessing. Relating back to our example, \\nlet’s tokenize the sample data via the following code:\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nsample_word_tokens = word_tokenize(sample_text)\\nsample_sent_tokens = sent_tokenize(sample_text)\\nWhen you print the sample_word_tokens  variable, you should observe \\nthe following:\\n[\\'I\\', \\'am\\', \\'a\\', \\'student\\', \\'from\\', \\'the\\', \\'University\\', \\'of\\', \\n\\'Alabama\\', \\'.\\', \\'I\\', \\'was\\', \\'born\\', \\'in\\', \\'Ontario\\', \\',\\', \\n\\'Canada\\', \\'and\\', \\'I\\', \\'am\\', \\'a\\', \\'huge\\', \\'fan\\', \\'of\\', \\'the\\', \\n\\'United\\', \\'States\\', \\'.\\', \\'I\\', \\'am\\', \\'going\\', \\'to\\', \\'get\\', \\'a\\', \\n\\'degree\\', \\'in\\', \\'Philosophy\\', \\'to\\', \\'improve\\', \\'my\\', \\'chances\\', \\n\\'of\\', \\'becoming\\', \\'a\\', \\'Philosophy\\', \\'professor\\', \\'.\\', \\'I\\', \\n\\'have\\', \\'been\\', \\'working\\', \\'towards\\', \\'this\\', \\'goal\\', \\'for\\', \\n\\'4\\', \\'years\\', \\'.\\', \\'I\\', \\'am\\', \\'currently\\', \\'enrolled\\', \\'in\\', \\n\\'a\\', \\'PhD\\', \\'program\\', \\'.\\', \\'It\\', \\'is\\', \\'very\\', \\'difficult\\', \\n\\',\\', \\'but\\', \\'I\\', \\'am\\', \\'confident\\', \\'that\\', \\'it\\', \\'will\\', \\'be\\', \\n\\'a\\', \\'good\\', \\'decision\\']Chapter 3  Working With\\xa0raW text\\n\\n46You will also observe that we have defined another tokenized object, \\nsample_sent_tokens . The difference between word_tokenize()  and  \\nsent_tokenize()  is simply that the latter tokenizes text by sentence \\ndelimiters. This is observed in the following output:\\n  [\\'I am a student from the University of Alabama.\\', \\'I \\\\nwas \\nborn in Ontario, Canada and I am a huge fan of the United \\nStates.\\', \\'I am going to get a degree in Philosophy to improve \\nmy chances of \\\\nbecoming a Philosophy professor.\\', \\'I have \\nbeen working towards this goal\\\\nfor 4 years.\\', \\'I am currently \\nenrolled in a PhD program.\\', \\'It is very difficult, \\\\nbut I am \\nconfident that it will be a good decision\\']\\nNow we have individual tokens that we can preprocess! From this \\nstep forward, we can clean out some of the junk text that we would not \\nwant to extract features from. Typically, the first thing we want to get rid \\nof are stop words , which are usually defined as very common words in a \\ngiven language. Most often, lists of stop words that we build or utilize in \\nsoftware packages include function words , which are words that express \\na grammatical relationship (rather than having an intrinsic meaning). \\nExamples of function words include the, and , for, and of.\\nIn this example, we use the list of stop words from the NLTK package.\\n[u\\'i\\', u\\'me\\', u\\'my\\', u\\'myself\\', u\\'we\\', u\\'our\\', u\\'ours\\', \\nu\\'ourselves\\', u\\'you\\', u\"you\\'re\", u\"you\\'ve\", u\"you\\'ll\", \\nu\"you\\'d\", u\\'your\\', u\\'yours\\', u\\'yourself\\', u\\'yourselves\\', \\nu\\'he\\', u\\'him\\', u\\'his\\', u\\'himself\\', u\\'she\\', u\"she\\'s\", u\\'her\\', \\nu\\'hers\\', u\\'herself\\', u\\'it\\', u\"it\\'s\", u\\'its\\', u\\'itself\\', \\nu\\'they\\', u\\'them\\', u\\'their\\', u\\'theirs\\', u\\'themselves\\', u\\'what\\', \\nu\\'which\\', u\\'who\\', u\\'whom\\', u\\'this\\', u\\'that\\', u\"that\\'ll\", \\nu\\'these\\', u\\'those\\', u\\'am\\', u\\'is\\', u\\'are\\', u\\'was\\', u\\'were\\', Chapter 3  Working With\\xa0raW text\\n\\n47u\\'be\\', u\\'been\\', u\\'being\\', u\\'have\\', u\\'has\\', u\\'had\\', u\\'having\\', \\nu\\'do\\', u\\'does\\', u\\'did\\', u\\'doing\\', u\\'a\\', u\\'an\\', u\\'the\\', u\\'and\\', \\nu\\'but\\', u\\'if\\', u\\'or\\', u\\'because\\', u\\'as\\', u\\'until\\', u\\'while\\', \\nu\\'of\\', u\\'at\\', u\\'by\\', u\\'for\\', u\\'with\\', u\\'about\\', u\\'against\\', \\nu\\'between\\', u\\'into\\', u\\'through\\', u\\'during\\', u\\'before\\', \\nu\\'after\\', u\\'above\\', u\\'below\\', u\\'to\\', u\\'from\\', u\\'up\\', u\\'down\\', \\nu\\'in\\', u\\'out\\', u\\'on\\', u\\'off\\', u\\'over\\', u\\'under\\', u\\'again\\', \\nu\\'further\\', u\\'then\\', u\\'once\\', u\\'here\\', u\\'there\\', u\\'when\\', \\nu\\'where\\', u\\'why\\', u\\'how\\', u\\'all\\', u\\'any\\', u\\'both\\', u\\'each\\', \\nu\\'few\\', u\\'more\\', u\\'most\\', u\\'other\\', u\\'some\\', u\\'such\\', u\\'no\\', \\nu\\'nor\\', u\\'not\\', u\\'only\\', u\\'own\\', u\\'same\\', u\\'so\\', u\\'than\\', \\nu\\'too\\', u\\'very\\', u\\'s\\', u\\'t\\', u\\'can\\', u\\'will\\', u\\'just\\', u\\'don\\', \\nu\"don\\'t\", u\\'should\\', u\"should\\'ve\", u\\'now\\', u\\'d\\', u\\'ll\\', \\nu\\'m\\', u\\'o\\', u\\'re\\', u\\'ve\\', u\\'y\\', u\\'ain\\', u\\'aren\\', u\"aren\\'t\", \\nu\\'couldn\\', u\"couldn\\'t\", u\\'didn\\', u\"didn\\'t\", u\\'doesn\\', \\nu\"doesn\\'t\", u\\'hadn\\', u\"hadn\\'t\", u\\'hasn\\', u\"hasn\\'t\", u\\'haven\\', \\nu\"haven\\'t\", u\\'isn\\', u\"isn\\'t\", u\\'ma\\', u\\'mightn\\', u\"mightn\\'t\", \\nu\\'mustn\\', u\"mustn\\'t\", u\\'needn\\', u\"needn\\'t\", u\\'shan\\', u\"shan\\'t\", \\nu\\'shouldn\\', u\"shouldn\\'t\", u\\'wasn\\', u\"wasn\\'t\", u\\'weren\\', \\nu\"weren\\'t\", u\\'won\\', u\"won\\'t\", u\\'wouldn\\', u\"wouldn\\'t\"]\\nAll of these words are lowercase by default. You should be aware that \\nstring objects must exactly match to return a true Boolean variable when \\ncomparing two individual strings. To put this more plainly, if we were to \\nexecute the code “you” == “YOU” ,  the Python interpreter returns false. \\nThe specific instance in which this affects our example can be observed \\nby executing the mistake()  and advised_preprocessing()  functions, \\nrespectively. Observe the following outputs:Chapter 3  Working With\\xa0raW text\\n\\n48[\\'I\\', \\'student\\', \\'University\\', \\'Alabama\\', \\'.\\', \\'I\\', \\'born\\', \\n\\'Ontario\\', \\',\\', \\'Canada\\', \\'I\\', \\'huge\\', \\'fan\\', \\'United\\', \\n\\'States\\', \\'.\\', \\'I\\', \\'going\\', \\'get\\', \\'degree\\', \\'Philosophy\\', \\n\\'improve\\', \\'chances\\', \\'becoming\\', \\'Philosophy\\', \\'professor\\', \\n\\'.\\', \\'I\\', \\'working\\', \\'towards\\', \\'goal\\', \\'4\\', \\'years\\', \\'.\\', \\n\\'I\\', \\'currently\\', \\'enrolled\\', \\'PhD\\', \\'program\\', \\'.\\', \\'It\\', \\n\\'difficult\\', \\',\\', \\'I\\', \\'confident\\', \\'good\\', \\'decision\\']\\n[\\'student\\', \\'University\\', \\'Alabama\\', \\'.\\', \\'born\\', \\'Ontario\\', \\n\\',\\', \\'Canada\\', \\'huge\\', \\'fan\\', \\'United\\', \\'States\\', \\'.\\', \\n\\'going\\', \\'get\\', \\'degree\\', \\'Philosophy\\', \\'improve\\', \\'chances\\', \\n\\'becoming\\', \\'Philosophy\\', \\'professor\\', \\'.\\', \\'working\\', \\n\\'towards\\', \\'goal\\', \\'4\\', \\'years\\', \\'.\\', \\'currently\\', \\'enrolled\\', \\n\\'PhD\\', \\'program\\', \\'.\\', \\'difficult\\', \\',\\', \\'confident\\', \\'good\\', \\n\\'decision\\']\\nAs you can see, the mistake()  function does not catch the uppercase \\n“I” characters, meaning that there are several stop words still in the text. \\nThis is solved by uppercasing all the stop words and then evaluating \\nwhether each uppercase word in the sample text was in the stop_words  \\nlist. This is exemplified with the following two lines of code:\\nstop_words = [word.upper() for word in stopwords.\\nwords(\\'english\\')]\\nword_tokens = [word for word in sample_word_tokens if word.\\nupper() not in stop_words]\\nAlthough embedded methods in feature extraction algorithms likely \\naccount for this case, you should be aware that strings must match exactly, \\nand you must account for this when preprocessing manually.\\nThat said, there is junk data that you should be aware of—specifically, \\nthe grammatical characters. You will be relieved to hear that the word_\\ntokenize()  function also categorizes colons and semicolons as individual Chapter 3  Working With\\xa0raW text\\n\\n49word tokens, but you still have to get rid of them. Thankfully, NLTK \\ncontains another tokenizer worth knowing about, which is defined and \\nutilized in the following code:\\nfrom nltk.tokenize import RegexpTokenizer\\ntokenizer = RegexpTokenizer(r\\'\\\\w+\\')\\nsample_word_tokens = tokenizer.tokenize(str(sample_word_\\ntokens))\\nsample_word_tokens = [word.lower() for word in sample_word_\\ntokens]\\nWhen we print the sample_word_tokens  variable, we get the following \\noutput:\\n[\\'student\\', \\'university\\', \\'alabama\\', \\'born\\', \\'ontario\\', \\n\\'canada\\', \\'huge\\', \\'fan\\', \\'united\\', \\'states\\', \\'going\\', \\'get\\', \\n\\'degree\\', \\'philosophy\\', \\'improve\\', \\'chances\\', \\'becoming\\', \\n\\'philosophy\\', \\'professor\\', \\'working\\', \\'towards\\', \\'goal\\', \\n\\'4\\', \\'years\\', \\'currently\\', \\'enrolled\\', \\'phd\\', \\'program\\', \\n\\'difficult\\', \\'confident\\', \\'good\\', \\'decision\\']\\nIn the course of this example, we have reached the final step! We have \\nremoved all the standard stop words, as well as all grammatical tokens. \\nThis is an example of a document that is ready for feature extraction, \\nwhereupon some additional preprocessing may occur.\\nNext, I’ll discuss some of the various feature extraction algorithms. \\nAnd let’s work on denser sample data alongside a preprocessed example \\nparagraph.Chapter 3  Working With\\xa0raW text\\n\\n50 The Bag-of-Words Model (BoW)\\nA BoW model is one of the more simplistic feature extraction algorithms \\nthat you will come across. The name “bag-of-words” comes from the \\nalgorithm simply seeking to know the number of times a given word is \\npresent within a body of text. The order or context of the words is not \\nanalyzed here. Similarly, if we have a bag filled with six pencils, eight \\npens, and four notebooks, the algorithm merely cares about recording the \\nnumber of each of these objects, not the order in which they are found, or \\ntheir orientation.\\nHere, I have defined a sample bag-of-words function.\\ndef bag_of_words(text):\\n     _bag_of_words = [collections.Counter(re.findall(r\\'\\\\w+\\', \\nword)) for word in text]\\n    bag_of_words = sum(_bag_of_words, collections.Counter())\\n    return bag_of_words\\nsample_word_tokens_bow = bag_of_words(text=sample_word_tokens)\\nprint(sample_word_tokens_bow)\\nWhen we execute the preceding code, we get the following output:\\nCounter({\\'philosophy\\': 2, \\'program\\': 1, \\'chances\\': 1, \\'years\\': 1,  \\n\\'states\\': 1, \\'born\\': 1, \\'towards\\': 1, \\'canada\\': 1, \\'huge\\': 1,  \\n\\'united\\': 1, \\'goal\\': 1, \\'working\\': 1, \\'decision\\': 1, \\n\\'currently\\': 1, \\'confident\\': 1, \\'going\\': 1, \\'4\\': 1, \\n\\'difficult\\': 1, \\'good\\': 1, \\'degree\\': 1, \\'get\\': 1, \\'becoming\\': 1,  \\n\\'phd\\': 1, \\'ontario\\': 1, \\'fan\\': 1, \\'student\\': 1, \\'improve\\': 1, \\n\\'professor\\': 1, \\'enrolled\\': 1, \\'alabama\\': 1, \\'university\\': 1})\\nThis is an example of a BoW model when presented as a dictionary. \\nObviously, this is not a suitable input format for a machine learning \\nalgorithm. This brings me to discuss the myriad of text preprocessing Chapter 3  Working With\\xa0raW text\\n\\n51functions available in the scikit-learn library, which is a Python library \\nthat all data scientists and machine learning engineers should be familiar \\nwith. For those who are new to it, this library provides implementations \\nof machine learning algorithms, as well as several data preprocessing \\nalgorithms. Although we won’t walk through much of this package, the text \\npreprocessing functions are extremely useful.\\n CountVectorizer\\nLet’s start by walking through the BoW equivalent—CountVectorizer, \\nan implementation of bag-of-words in which we code text data as a \\nrepresentation of features/words. The values of each of these features \\nrepresent the occurrence counts of words across all documents. If you \\nrecall, we defined a sample_sent_tokens  variable, which we will analyze. \\nWe define a bow_sklearn()  function beneath where we preprocess our \\ndata. The function is defined as follows:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ndef bow_sklearn(text=sample_sent_tokens):\\n     c = CountVectorizer(stop_words=\\'english\\',  \\ntoken_pattern=r\\'\\\\w+\\')\\n    converted_data = c.fit_transform(text).todense()\\n    print(converted_data.shape)\\n    return converted_data, c.get_feature_names()\\nTo provide context, in this example, we are assuming that each sentence \\nis an individual document, and we are creating a feature set in which each \\nfeature is an individual token. When we instantiate CountVectorizer() , we \\nset two parameters: stop_words , and token_pattern . These two arguments \\nare the embedded methods in the feature extraction that remove stop \\nwords and grammatical tokens. The fit_transform()  attribute expects \\nto receive a list, an array, or a similar object of iterable string objects. We \\nassign the bow_data  and feature_names  variables to the data that the  Chapter 3  Working With\\xa0raW text\\n\\n52bow_sklearn()  returns, respectively. Our converted data set is a 6 × 50 matrix, \\nwhich means that we have six sentences, all of which have 50 features. \\nObserve our data set and feature names, respectively, in the following \\noutputs:\\n[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\\n [0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0]\\n [0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0]\\n [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\\n [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\\n [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\\n[u\\'4\\', u\\'alabama\\', u\\'born\\', u\\'canada\\', u\\'chances\\', \\nu\\'confident\\', u\\'currently\\', u\\'decision\\', u\\'degree\\', \\nu\\'difficult\\', u\\'enrolled\\', u\\'fan\\', u\\'goal\\', u\\'going\\', u\\'good\\', \\nu\\'huge\\', u\\'improve\\', u\\'ontario\\', u\\'phd\\', u\\'philosophy\\', \\nu\\'professor\\', u\\'program\\', u\\'states\\', u\\'student\\', u\\'united\\', \\nu\\'university\\', u\\'working\\', u\\'years\\']\\nTo extrapolate this example to a larger number of documents, and \\nostensibly larger vocabulary sizes, our matrices for preprocessed text data \\ntends to have a large number of features, sometimes well over 1000. How \\nto evaluate these features effectively is the machine learning challenge \\nwe seek to solve. You typically want to use the bag-of-words feature \\nextraction technique for document classification. Why is this the case? \\nWe assume that documents of certain classifications contain certain \\nwords. For example, we expect a document referencing political science \\nto perhaps feature jargon such as dialectical materialism  or free market \\ncapitalism ; whereas a document that is referring to classical music will \\nhave terms such as crescendo , diminuendo , and so forth. In these instances \\nof document classification, the location of the word itself is not terribly \\nimportant. It’s important to know what portion of the vocabulary is present \\nin one class of document vs. another.Chapter 3  Working With\\xa0raW text\\n\\n53Next, let’s look at our first example problem in the code in the  \\ntext_classifiction_demo.py  file.\\n Example Problem 1: Spam Detection\\nSpam detection is a relatively common task in that most people have an \\ninbox (email, social media instant messenger account, or similar entity) \\ntargeted by advertisers or malicious actors. Being able to block unwanted \\nadvertisements or malicious files is an important task. Because of this, we \\nare interested in pursuing a machine learning approach to spam detection. \\nLet’s begin by describing the data set before digging into the problem.\\nThis data set was downloaded from the UCI Machine Learning \\nRepository, specifically the Text Data section. Our data set consists of 5574 \\nobservations—all SMS messages. We observe from our data set that most \\nof the messages are not terribly long. Figure\\xa0 3-1 is a histogram of our entire \\ndata set.\\nFigure 3-1.  SMS mes sage length histogramChapter 3  Working With\\xa0raW text\\n\\n54Something else we should be mindful of is the distribution between \\nthe class labels, which tends to be heavily skewed. In this data set, 4825 \\nobservations are marked as “ham” (being not spam), and 747 are marked \\nas “spam” . You must be vigilant in evaluating your machine learning \\nsolutions to ensure that they do not overfit the training data, and then fail \\nmiserably on test data.\\nLet’s briefly do some additional data set discovery before we move on \\nto tackling the problem directly. When we look at the header of our data \\nset, we observe the following:\\n0   ham  Go until jurong point, crazy.. Available only ...\\n1   ham                      Ok lar... Joking wif u oni...\\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\\n3   ham  U dun say so early hor... U c already then say...\\n4   ham  Nah I don\\'t think he goes to usf, he lives aro...\\nThe first column is our categorical label/response variable. The second \\ncolumn comprises text contained within each individual SMS.\\xa0We will use \\na bag-of-words representation via the CountVectorizer() . Our entire data \\nset has a vocabulary size of 8477 words. The load_spam_data()  function \\nshows that the preprocessing steps mimic the warmup example at the \\nbeginning of the chapter.\\nLet’s fit and train our model and evaluate the results. When beginning \\na classification task, I suggest that you evaluate the results of the logistic \\nregression. This determines if your data is linearly separable or not. If it \\nis, the logistic regression should work fine, which saves you from further \\nmodel selection and time-consuming hyper-parameter optimization. If it \\ndoes fail, then you can use those methods.\\nWe train a model using both L1 and L2 weight regularization in the \\ntext_classifiction_demo.py  file; however, we will walk through the L1 \\nnorm regularized example here because it yielded better test results:Chapter 3  Working With\\xa0raW text\\n\\n55#Fitting training algorithm\\nl = LogisticRegression(penalty=\\'l1\\')\\naccuracy_scores, auc_scores = [], []\\nThose of you that are not familiar with logistic regression you should \\nlearn about elsewhere; however, I will discuss the L1-regularized logistic \\nregression briefly. L1 norm regularization in linear models is standard \\nfor LASSO (least absolute shrinkage selection operator), where during \\nthe learning process, the L1 norm can theoretically force some regression \\ncoefficients to 0. In contrast, the L2 norm, often seen in ridge regression, \\ncan force some regression coefficients during the learning process to \\nnumbers close to 0. The difference between this is that coefficients that \\nare 0 effectively perform feature selection on our feature set by eliminating \\nthem. Mathematically, we represent this regularization via Equation 3.1.\\n minl og ;)\\niM\\niIpy x\\n=å-+\\n11(| qb q\\n (3.1)\\nWe w ill evaluate the distribution of test scores over several trials. scikit-  \\nlearn al gorithms’ fit()  method trains the algorithm of a given data set. As \\nsuch, all the iterations that optimize the parameters are performed. To see \\nlogging information in the training process, set the verbose  parameter to 1.\\nLet’s look at the code that will collect the distribution of both accuracy \\nand AUC scores.\\nfor i in range(trials):\\n   if i%10 == 0 and i > 0:\\n        print(\\'Trial \\' + str(i) + \\' out of 100 completed\\')\\n   l.fit(train_x, train_y)\\n   predicted_y_values = l.predict(train_x)\\n    accuracy_scores.append(accuracy_score(train_y, predicted_y_\\nvalues))Chapter 3  Working With\\xa0raW text\\n\\n56    fpr, tpr = roc_curve(train_y, predicted_y_values)[0],  \\nroc_curve(train_y, predicted_y_values)[1]\\n   auc_scores.append(auc(fpr, tpr))\\nscikit-learn performs cross-validation so long as you define a random \\nseed utilizing the np.random.seed()  function, which we do near the \\nbeginning of the file. During each trial, we are fitting the data set to the \\nalgorithm, predicting the accuracy and AUC score, and appending them \\nto a list that we defined. When we evaluate our results from training, we \\nobserve the following:\\nSummary Statistics (AUC):\\n        Min       Max      Mean      SDev    Range\\n0  0.965348   0.968378   0.967126   0.000882   0.00303\\nSummary Statistics (Accuracy Scores):\\n        Min      Max      Mean      SDev     Range\\n0  0.990356   0.99116  0.990828   0.000234   0.000804\\nTest Model Accuracy: 0.9744426318651441\\nTest True Positive Rate: 0.8412698412698413\\nTest False Positive Rate: 0.004410838059231254\\n[[1580    7]\\n [  40  212]]\\nFortunately, we see that logistic regression performs excellently on \\nthis problem. We have excellent accuracy and AUC scores, with very little \\nvariance from one trial to the next. Let’s evaluate the AUC score, as shown \\nin Figure\\xa0 3-2.Chapter 3  Working With\\xa0raW text\\n\\n57Our test AUC score is 0.92. This algorithm would be deployable in an \\napplication to test for spam results. In the course of solution discovery, \\nI suggest that you use this model rather than others. Although you are \\nencouraged to find other methods, I observed that the gradient-boosted \\nclassification tree and random forests performed considerably worse, \\nwith AUC scores of roughly 0.72. Let’s discuss a more sophisticated term \\nfrequency scheme.\\n Term Frequency Inverse Document Frequency\\nTerm frequency–inverse document frequency  (TFIDF) is based on BoW, but \\nprovides more detail than simply taking term frequency, as was done in \\nthe prior example. TFIDF yields a value that shows how important a given \\nword is by not only looking at term frequency, but also analyzing how \\nmany times the word appears across all documents. The first portion, term \\nfrequency, is relatively straightforward.\\nFigure 3-2.  Test set ROC curveChapter 3  Working With\\xa0raW text\\n\\n58Let’s look at an example to see how to calculate TFIDF .\\xa0We define a \\nnew body of text and use the sample text defined at the beginning of the \\nchapter, as follows:\\ntext = “‘I was a student at the University of \\nPennsylvania, but now work on\\nWall Street as a Lawyer. I have been living in \\nNew\\xa0York for roughly five years\\nnow, however I am looking forward to eventually \\nretiring to Texas once I have\\nsaved up enough money to do so. ”’\\ndocument_list = list([sample_text, text])\\nNow that we have a list of documents, let’s look at exactly what the \\nTFIDF algorithm does. The first portion, term frequency, has several \\nvariants, but we will focus on the standard raw count scheme. We simply \\nsum the terms across all documents. The term frequency is equivalent to \\nEquation 3.2.\\n f\\nftd\\ntdtd,\\n, ¢¢Îå (3.2)\\nft, d is equal to the frequency of the term across all documents. ftd¢, \\nis equal to the frequency of that same term but within each individual \\ndocument. In our code, we document these steps in the tf_idf_example()  \\nfunction, as follows:\\ndef tf_idf_example(textblobs=[text, text2]):\\ndef term_frequency(word, textblob): (1)Chapter 3  Working With\\xa0raW text\\n\\n59return textblob.words.count(word)/float(len(textblob.words))\\ndef document_counter(word, text):\\nreturn sum(1 for blob in text if word in blob)\\ndef idf(word, text): (2)\\nreturn np.log(len(text) /1 + float(document_counter(word, \\ntext)))\\ndef tf_idf(word, blob, text):\\nreturn term_frequency(word, blob) * idf(word, text)\\noutput = list()\\nfor i, blob in enumerate(textblobs):\\noutput.append({word: tf_idf(word, blob, textblobs) for word in \\nblob.words})\\nprint(output)\\nThanks to the TextBlob package, we are able to fairly quickly re-create \\nthe TFIDF toy implementation. I will address each of the functions within \\nthe tf_idf_example()  function. You are aware of term frequency, so I \\ncan discuss inverse document frequency. We define inverse document \\nfrequency as a measure of how frequently a word appears across the entire \\ncorpus. Mathematically, this relationship is expressed in Equation 3.3.\\n idf,tDN\\ndD td() =ÎÎ{}log:  (3.3)\\nThis e quation calculates the log of the total number of documents in \\nour corpus, divided by all the documents in which the term that we are \\nevaluating appears. In our code, we calculate this with the function (2). \\nNow, we are ready to proceed to the final step of the algorithm, which is \\nmultiplying the term frequency by the inverse document frequency, as \\nshown in the preceding code. We then yield the following output:Chapter 3  Working With\\xa0raW text\\n\\n60 [{\\'up\\': \\'0.027725887222397813\\', \\'money\\': \\n\\'0.021972245773362195\\', \\'am\\': \\'0.027725887222397813\\', \\'years\\': \\n\\'0.027725887222397813\\', \\'as\\': \\'0.027725887222397813\\', \\'at\\': \\n\\'0.027725887222397813\\', \\'have\\': \\'0.055451774444795626\\', \\n\\'in\\': \\'0.027725887222397813\\', \\'New\\': \\'0.021972245773362195\\', \\n\\'saved\\': \\'0.021972245773362195\\', \\'Texas\\': \\n\\'0.021972245773362195\\', \\'living\\': \\'0.021972245773362195\\', \\n\\'for\\': \\'0.027725887222397813\\', \\'to\\': \\'0.08317766166719343\\', \\n\\'retiring\\': \\'0.027725887222397813\\', \\'been\\': \\n\\'0.021972245773362195\\', \\'looking\\': \\'0.021972245773362195\\', \\n\\'Pennsylvania\\': \\'0.021972245773362195\\', \\'enough\\': \\n\\'0.021972245773362195\\', \\'York\\': \\'0.021972245773362195\\', \\n\\'forward\\': \\'0.027725887222397813\\', \\'was\\': \\n\\'0.027725887222397813\\', \\'eventually\\': \\'0.021972245773362195\\', \\n\\'do\\': \\'0.027725887222397813\\', \\'I\\': \\'0.11090354888959125\\', \\n\\'University\\': \\'0.027725887222397813\\', \\'however\\': \\n\\'0.027725887222397813\\', \\'but\\': \\'0.021972245773362195\\', \\'five\\': \\n\\'0.021972245773362195\\', \\'student\\': \\'0.021972245773362195\\', \\n\\'now\\': \\'0.04394449154672439\\', \\'a\\': \\'0.055451774444795626\\', \\n\\'on\\': \\'0.027725887222397813\\', \\'Wall\\': \\'0.021972245773362195\\', \\n\\'of\\': \\'0.027725887222397813\\', \\'work\\': \\'0.021972245773362195\\', \\n\\'roughly\\': \\'0.021972245773362195\\', \\'Street\\': \\n\\'0.021972245773362195\\', \\'so\\': \\'0.021972245773362195\\', \\'Lawyer\\': \\n\\'0.021972245773362195\\', \\'the\\': \\'0.027725887222397813\\', \\'once\\': \\n\\'0.021972245773362195\\'}, {\\'and\\': \\'0.0207285337484549\\', \\'is\\': \\n\\'0.0207285337484549\\', \\'each\\': \\'0.0207285337484549\\', \\'am\\': \\n\\'0.026156497379620575\\', \\'years\\': \\'0.026156497379620575\\', \\n\\'have\\': \\'0.05231299475924115\\', \\'in\\': \\'0.026156497379620575\\', \\n\\'children\\': \\'0.0414570674969098\\', \\'considering\\': \\n\\'0.0207285337484549\\', \\'retirement\\': \\'0.0207285337484549\\', \\n\\'doctor\\': \\'0.0207285337484549\\', \\'retiring\\': Chapter 3  Working With\\xa0raW text\\n\\n61\\'0.026156497379620575\\', \\'two\\': \\'0.0207285337484549\\', \\'long\\': \\n\\'0.0207285337484549\\', \\'next\\': \\'0.0207285337484549\\', \\'to\\': \\n\\'0.05231299475924115\\', \\'forward\\': \\'0.026156497379620575\\', \\n\\'was\\': \\'0.026156497379620575\\', \\'couple\\': \\'0.0207285337484549\\', \\n\\'more\\': \\'0.0207285337484549\\', \\'ago\\': \\'0.0207285337484549\\', \\n\\'them\\': \\'0.0207285337484549\\', \\'that\\': \\'0.0207285337484549\\', \\n\\'I\\': \\'0.1046259895184823\\', \\'University\\': \\n\\'0.026156497379620575\\', \\'who\\': \\'0.0414570674969098\\', \\'however\\': \\n\\'0.026156497379620575\\', \\'quite\\': \\'0.0207285337484549\\', \\n\\'me\\': \\'0.0207285337484549\\', \\'Yale\\': \\'0.0207285337484549\\', \\n\\'with\\': \\'0.0207285337484549\\', \\'the\\': \\'0.05231299475924115\\', \\n\\'a\\': \\'0.07846949213886173\\', \\'both\\': \\'0.0207285337484549\\', \\n\\'look\\': \\'0.026156497379620575\\', \\'of\\': \\'0.026156497379620575\\', \\n\\'grandfather\\': \\'0.0207285337484549\\', \\'spending\\': \\n\\'0.0207285337484549\\', \\'three\\': \\'0.0207285337484549\\', \\'time\\': \\n\\'0.0414570674969098\\', \\'making\\': \\'0.0207285337484549\\', \\'went\\': \\n\\'0.0207285337484549\\'}]\\nThis brings us to the end of our toy example using TFIDF .\\xa0Before we \\njump into the example, let’s review how we would utilize this example \\nin scikit-learn, such that we can input this data into a machine learning \\nalgorithm. Similar to CountVectorizer() , scikit-learn has provided a \\nTfidfVectorizer()  method that comes in handy. The following shows its \\nutilization. I will dive into a deeper use of its preprocessing methods later.\\ndef tf_idf_sklearn(document=document_list):\\n     t = TfidfVectorizer(stop_words=\\'english\\',  \\ntoken_pattern=r\\'\\\\w+\\')\\n    x = t.fit_transform(document_list).todense()\\n    print(x)Chapter 3  Working With\\xa0raW text\\n\\n62When we execute the function, it yields the following result:\\n[[0.         0.         0.         0.         0.         0.24235766\\n  0.17243947 0.          0.24235766 0.24235766 0.          0.\\n  0.24235766 0.          0.24235766 0.24235766 0.24235766 0.\\n  0.         0.17243947 0.24235766 0.24235766 0.          0.24235766\\n  0.24235766 0.24235766 0.          0.17243947 0.24235766 0.\\n  0.24235766 0.          0.17243947 0.24235766]\\n [0.20840129 0.41680258 0.20840129 0.20840129 0.20840129 0.\\n  0.14827924 0.20840129 0.          0.         0.20840129 0.20840129\\n  0.         0.20840129 0.          0.         0.         0.20840129\\n  0.20840129 0.14827924 0.          0.         0.20840129 0.\\n  0.         0.         0.41680258 0.14827924 0.          0.20840129\\n  0.         0.20840129 0.14827924 0.         ]]\\nThis function yields a 2 × 44 matrix, and it is ready for input into a \\nmachine learning algorithm for evaluation.\\nNow let’s work through another example problem using TFIDF as our \\nfeature extractor while utilizing another machine learning algorithm as we \\ndid for the BoW feature extraction.\\n Example Problem 2: Classifying Movie Reviews\\nWe obtained the following IMDB movie review data set from http://www.\\ncs.cornell.edu/people/pabo/movie-review-data/ .\\nWe are going to work with the raw text directly, rather than using \\npreprocessed text data sets often provided via various machine learning \\npackages.\\nLet’s take a snapshot of the data.\\ntristar / 1 : 30 / 1997 / r ( language , violence , dennis \\nrodman ) cast : jean-claude van damme ; mickey rourke ; dennis \\nrodman ; natacha lindinger ; paul freeman director : tsui hark Chapter 3  Working With\\xa0raW text\\n\\n63screenplay : dan jakoby ; paul mones ripe with explosions , \\nmass death and really weird hairdos , tsui hark\\'s \" double \\nteam \" must be the result of a tipsy hollywood power lunch \\nthat decided jean-claude van damme needs another notch on his \\nbad movie-bedpost and nba superstar dennis rodman should have \\nan acting career . actually , in \" double team , \" neither\\'s \\nperformance is all that bad . i\\'ve always been the one critic \\nto defend van damme -- he possesses a high charisma level that \\nsome genre stars ( namely steven seagal ) never aim for ; it\\'s \\njust that he\\'s never made a movie so exuberantly witty since \\n1994\\'s \" timecop . \" and rodman . . . well , he\\'s pretty much \\nrodman . he\\'s extremely colorful , and therefore he pretty much \\nfits his role to a t , even if the role is that of an ex-cia\\nAs you can see, this data is filled with lots of grammatical noise that we \\nwill need to remove, but is also rich in descriptive text. We will opt to use \\nthe TfidfVectorizer()  method on this data.\\nFirst, I would like to direct you to two functions at the beginning of  \\nthe file:\\ndef remove_non_ascii(text):\\n    return \".join([word for word in text if ord(word) < 128])\\nNotice that we are using the native Python function ord() . This \\nfunction expects a string, and it returns either the Unicode point for \\nUnicode objects or the value of the byte. If the ord()  function returns \\nan integer less than 128, this poses no problem for our preprocesser \\nand therefore we keep the string in question; otherwise, we remove \\nthe character. We end this step by joining all the remaining words back \\ntogether with the \".join()  function. The reasoning for preprocessing \\nduring data preparation is that our text preprocessor expects Unicode \\nobjects when being fed to it. When we are capturing raw text data, \\nparticularly if it is from an HTML page, many of the string objects Chapter 3  Working With\\xa0raW text\\n\\n64loaded before preprocessing and removal of stop words will not be \\nUnicode-compatible.\\nLet’s look at the function that loads our data.\\ndef load_data():\\n     negative_review_strings = os.listdir(\\'/Users/tawehbeysolow/\\nDownloads/review_data/tokens/neg\\')\\n     positive_review_strings =  os.listdir(\\'/Users/tawehbeysolow/\\nDownloads/review_data/tokens/pos\\')\\n    negative_reviews, positive_reviews = [], []\\nWe start by loading the file names of all the .txt  files to be processed. \\nTo do this, we use the os.listdir()  function. I suggest you use this \\nfunction when building similar applications that require preprocessing a \\nlarge number of files.\\nNext, we load our files with the open()  function, and then apply the \\nremove_non_ascii()  function, as follows:\\n    for positive_review in positive_review_strings:\\n         with open(\\'/Users/tawehbeysolow/Downloads/review_data/\\ntokens/pos/\\'+str(positive_review), \\'r\\') as positive_file:\\n             positive_reviews.append(remove_non_ascii(positive_\\nfile.read()))\\n    for negative_review in negative_review_strings:\\n         with open(\\'/Users/tawehbeysolow/Downloads/review_data/\\ntokens/neg/\\'+str(negative_review), \\'r\\') as negative_file:\\n             negative_reviews.append(remove_non_ascii(negative_\\nfile.read()))\\nWith our initial preprocessing done, we end by concatenating both \\nthe positive and negative reviews, in addition to the respective vectors \\nthat contain their labels. Now, we can get to the meat and potatoes of this \\nmachine learning problem, starting with the train_logistic_model()  Chapter 3  Working With\\xa0raW text\\n\\n65function. In a similar fashion, we use logistic regression as the baseline \\nfor the problem. Although most of the following functions are similar in \\nstructure to Example Problem 1, let’s look at the beginning of this function \\nto analyze what we have changed.\\n#Load and preprocess text data\\nx, y = load_data()\\nt = TfidfVectorizer(min_df=10, max_df=300, stop_\\nwords=\\'english\\', token_pattern=r\\'\\\\w+\\')\\nx = t.fit_transform(x).todense()\\nWe are utilizing two new arguments: min_df  corresponds to the \\nminimum document frequency to retain a word, and max_df  refers to \\nthe maximum amount of documents that a word can appear in before \\nit is omitted from the sparse matrix that we create. When increasing the \\nmaximum and minimum document frequencies, I noticed that the L1 \\npenalty model performed better than the L2 penalty model. I would posit \\nthat this is likely due to the fact that as we increase the min_df  parameter, \\nwe are creating a considerably sparser matrix than if we had a denser \\nmatrix. You should keep this in mind so as to not overselect features if they \\nperformed any feature selection on their matrices beforehand.\\nLet’s evaluate the results of the logistic regression, as shown in the \\nfollowing output (also see Figures\\xa0 3-3 and 3-4).\\nSummary Statistics from Training Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.723874   0.723874     0.0  0.723874    0.0\\nSummary Statistics from Training Set (Accuracy):\\n       Mean       Max  Range      Mean  SDev\\n0  0.726788   0.726788     0.0  0.726788    0.0\\nTraining Data Confusion Matrix:\\n[[272 186]\\n [ 70 409]]Chapter 3  Working With\\xa0raW text\\n\\n66Summary Statistics from Test Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.723874   0.723874     0.0  0.723874    0.0\\nSummary Statistics from Test Set (Accuracy):\\n        Mean       Max  Range      Mean  SDev\\n0  0.726788   0.726788     0.0  0.726788    0.0\\nTest Data Confusion Matrix:\\n[[272 186]\\n [ 70 409]]\\nSummary Statistics from Training Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981824   0.981824     0.0  0.981824    0.0\\nSummary Statistics from Training Set (Accuracy):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981857   0.981857     0.0  0.981857    0.0\\nTraining Data Confusion Matrix:\\n[[449   9]\\n [  8 471]]\\nSummary Statistics from Test Set (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.981824   0.981824     0.0  0.981824    0.0\\nSummary Statistics from Test Set (Accuracy):\\n        Mean       Max  Range      Mean  SDev\\n0  0.981857   0.981857     0.0  0.981857    0.0\\nTest Data Confusion Matrix:\\n[[449   9]\\n [  8 471]]Chapter 3  Working With\\xa0raW text\\n\\n67Both in training and performance, logistic regression performs \\nconsiderably better when utilizing the L2 weight regularization method, \\ngiven the parameters we used for the TfidfVectorizer()  feature \\nextraction algorithm.\\nFigure 3-3.  L1 logistic regression test set ROC curveChapter 3  Working With\\xa0raW text\\n\\n68I created multiple solutions to evaluate: a random forest classifier, a \\nnaïve Bayes classifier, and a multilayer perceptron. We begin with a general \\noverview of all of our methods and their respective orientations.\\nStarting with the multilayer perceptron in the mlp_movie_\\nclassification_model.py  file, notice that much of the neural network \\nis the same as the example in Chapter 2, with the exception of an extra \\nhidden layer. That said, I would like to direct your attention to lines 92 \\nthrough 94.\\nregularization = tf.contrib.layers.l2_regularizer(scale=0.0005, \\nscope=None)\\nregularization_penalty = tf.contrib.layers.apply_\\nregularization(regularization, weights.values())\\ncross_entropy = cross_entropy + regularization_penalty\\nFigure 3-4.  L2 logi stic regression test set ROC curveChapter 3  Working With\\xa0raW text\\n\\n69In these lines, we are performing weight regularization, as \\ndiscussed earlier in this chapter with the logistic regression L2 and L1 \\nloss parameters. Those of you who wish to apply this in TensorFlow \\ncan rest assured that these are the only modifications needed to add a \\nweight penalty to your neural network. While developing this solution, \\nI tried weight regularization utilizing L1 and L2 loss penalties, and I \\nexperimented with dropout. Weight regularization is the process of \\nlimiting the scope to which the weights can grow when utilizing different \\nvector norms. The two most referenced norms for weight regularization \\nare L1 and L2 norms. The following are their respective equations, which \\nare also illustrated in Figure\\xa0 3-5.\\n L1==\\n=å v1\\n11\\niN\\niv \\n L2==\\n=å v2\\n12\\niN\\niv \\nFigure 3-5.  L1 and L2 norm visualizationChapter 3  Working With\\xa0raW text\\n\\n70When initially utilizing both one and two hidden layer(s), I noticed  \\nthat both the test and training performance were considerably worse  \\nwith dropout, even using dropout percentages as low as 0.05. As such,  \\nI cannot suggest that you utilize dropout for this problem. As for weight \\nregularization, additional parameter selection is not advisable; however, \\nI found negligible differences with L1 vs. L2 regularization. The confusion \\nmatrix and the ROC curve are shown in Figure\\xa0 3-6.\\nTest Set Accuracy Score: 0.8285714285714286\\nTest Set Confusion Matrix:\\n[[122  26]\\n [ 22 110]]\\nFigure 3-6.  ROC curve for multilayer perceptronChapter 3  Working With\\xa0raW text\\n\\n71Let’s analyze the choice of parameters for the random forest and \\nnaïve Bayes classifiers. We kept our trees relatively short at a max_depth  \\nof ten splits. As for the naïve Bayes classifier, the only parameter we chose \\nis alpha, which we set to 0.005. Let’s evaluate Figures\\xa0 3-6 and 3-7 for the \\nresults of the model.\\nThe Figure\\xa0 3-8 shows the result of a naïve Bayes classifier.\\nFigure 3-7.  ROC curve for random forestChapter 3  Working With\\xa0raW text\\n\\n72Summary Statistics from Training Set Random Forest (AUC):\\n       Mean       Max  Range      Mean  SDev\\n0  0.987991   0.987991     0.0  0.987991    0.0\\nSummary Statistics from Training Set Random Forest (Accuracy):\\n      Mean      Max  Range     Mean  SDev\\n0  0.98826  0.98826    0.0  0.98826   0.0\\nTraining Data Confusion Matrix (Random Forest):\\n[[447  11]\\n [  0 479]]\\nSummary Statistics from Training Set Naive Bayes (AUC):\\n       Mean       Max  Range      Mean          SDev\\n0  0.965362   0.965362     0.0  0.965362   2.220446e-16\\nFigure 3-8.  ROC curve for naïve Bayes classifierChapter 3  Working With\\xa0raW text\\n\\n73Summary Statistics from Training Set Naive Bayes (Accuracy):\\n       Mean       Max  Range      Mean          SDev\\n0  0.964781   0.964781     0.0  0.964781   3.330669e-16\\nTraining Data Confusion Matrix (Naive Bayes):\\n[[454   4]\\n [ 29 450]]\\nTest Data Confusion Matrix:\\n[[189  27]\\n [ 49 197]]\\nTest Data Confusion Matrix (Random Forest):\\n[[162  54]\\n [ 19 227]]\\nWhen evaluating the results, the neural network has a tendency to \\noverfit to training data, but its test performance is very similar to logistic \\nregression, although slightly less accurate. When assessing the results \\nof the naïve Bayes classifier and the random forest classifier, we observe \\nroughly similar AUC scores, with only a difference in false positives and \\ntrue positives as the trade-off that we must accept. In this instance, it is \\nimportant to consider our objective.\\nIf we are using these algorithms to label the reviews that users input, \\nand then perform analytics on top of these reviews, we want to maximize \\nthe accuracy rate, or seek models with the highest true positive rate and \\ntrue negative rate. In the instance of spam detection, we likely want the \\nmodel that has the best ability to properly classify spam from normal mail.\\nI have introduced and applied the bag-of-words schemes in both \\nthe logistic model and the naïve Bayes classifier. This brings us to the \\nfinal part of this section, in which I discuss their relative advantages and \\ndisadvantages. You should be aware of this so as to not waste time altering \\nsubpar solutions. The major advantage of BoW is that it is a relatively \\nstraightforward algorithm that allows you to quickly turn text into a Chapter 3  Working With\\xa0raW text\\n\\n74format interpretable by a machine learning algorithm, and to attack NLP \\nproblems directly.\\nThe largest disadvantage of BoW is its relative simplicity. BoW does \\nnot account for the context of words, and as such, it does not make it the \\nideal feature extraction method for more complex NLP tasks. For example, \\n“4” and “four” are considered semantically indistinguishable, but in BoW, \\nthey are considered two different words altogether. When we expand this \\nto phrases, “I went to college for four years, ” and “For 4 years, I attended a \\nuniversity, ” are treated as orthogonal vectors. Another example of a BoW \\nshortcoming is that it cannot distinguish the ordering of words. As such,  \\n“I am stupid” and “ Am I stupid” appear as the same vector.\\nBecause of these shortcomings, it is appropriate for us to utilize more \\nadvanced models, such as word embeddings, for these difficult problems, \\nwhich are discussed in detail in the next chapter.\\n Summary\\nThis brings us to the end of Chapter 3! This chapter tackled working with \\ntext data in document classification problems. You also became familiar \\nwith two BoW feature extraction methods.\\nLet’s take a moment to go over some of the most important lessons \\nfrom this chapter. Just as with traditional machine learning, you must \\ndefine the type of problem and analyze data. Is this simply document \\nclassification? Are we trying to find synonyms? We have to answer these \\nquestions before tackling any other steps.\\nThe removal of stop words, grammatical tokens, and frequent words \\nimproves the accuracy of our algorithms. Not every word in a document is \\ninformative, so you should know how to remove the noise. That said, over-  \\nsele cting features can be detrimental to our model’s success, so you should \\nbe aware of this too!Chapter 3  Working With\\xa0raW text\\n\\n75Whenever you are working with a machine learning problem, within \\nor outside the NLP domain, you must establish a baseline solution and \\nthen improve if necessary!  I suggest that you always start a deep learning \\nproblem by seeing how the solution appears, such as with a logistic \\nregression. Although it is my goal to teach you how to apply deep learning \\nto NLP-based problems, there is no reason to use overly complex methods \\nwhere less complex methods will do better or equally as well (unless you \\nlike to practice your deep learning skills).\\nFinally, while preprocessing methods are useful, BoW-based models \\nare best utilized with document classification. For more advanced NLP \\nproblems, such as sentiment analysis, understanding semantics, and \\nsimilarly abstract problems, BoW likely will not yield the best results.Chapter 3  Working With\\xa0raW text\\n\\n77© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_4CHAPTER 4\\nTopic Modeling and\\xa0 \\nWord Embeddings\\nNow that you have had an introduction to working with text data, let’s \\ndive into one of the more advanced feature extraction algorithms. To \\naccomplish some of the more difficult problems, it is reasonable for me \\nto introduce you to other techniques to approach NLP problems. We will \\nmove through Word2Vec, Doc2Vec, and GloVe.\\n Topic Model and\\xa0Latent Dirichlet  \\nAllocation (LDA)\\nTopic models are a method of extracting information from bodies of text \\nto see what “topics” occur across all the documents. The intuition is that \\nwe expect certain topics to appear more in relevant documents and not as \\nmuch in irrelevant documents. This might be useful when using the topics \\nwe associate with a document as keywords for better and more intuitive \\nsearch, or when using it for shorthand summarization. Before we apply \\nthis model, let’s talk about how we actually extract topics.\\n\\n78Latent Dirichlet allocation (LDA) is a generative model developed in \\n2003 by David Blei, Andrew Ng, and Michael I.\\xa0Jordan. In their paper, they \\nhighlight the shortcomings of TFIDF . Most notably, TFIDF is unable to \\nunderstand the semantics of words, or the position of a word in text. This \\nled to the rise of LDA.\\xa0LDA is a generative model, meaning that it outputs \\nall the possible outcomes for a given phenomenon. Mathematically, we \\ncan describe the assumptions as follows:\\n 1. Choose N~Poisson (ξ) (a sequence of N words within \\na document have a Poisson distribution)\\n 2. Choose θ~Dir(α) (a parameter θ has a Dirichlet \\ndistribution)\\n 3. For each of the N words ( wn):\\n• Choose topic zn~Multinomial (θ) (Each topic zn has \\na multinomial distribution.)\\n• Choose wn from p (wn |\\xa0zn, β), a multinomial \\nprobability conditional on topic zn. (Each topic is \\nrepresented as a distribution over words, where a \\nprobability is generated from the probability of the \\nnth word, conditional upon the topic as well as β \\nwhere βij\\xa0=\\xa0p(wj\\xa0=\\xa01| zi\\xa0=\\xa01) with dimensions k\\xa0x\\xa0V . )\\n β = probability of a given word, V = number of \\nwords in the vocabulary, k = the dimensionality of \\nthe Dirichlet distribution, θ = the random variable \\nsampled from the probability simplex.\\nLet’s discuss some of the distributions utilized in these assumptions. \\nThe Poisson distribution represents events that occur in a fixed time or \\nspace and at a constant rate, independently of the time since the last event. \\nAn example of this distribution is a model of the number of people who \\ncall a pizzeria for delivery during a given period of time. The multinomial Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n79distribution is the k-outcome generalization of the binomial distribution; \\nin other words, the same concept as the binomial distribution but \\nexpanded to cases where there are more than two outcomes.\\nFinally, the Dirichlet distribution is a generalization of the beta \\ndistribution, but expanded to handle multivariate data. The beta \\ndistribution is a distribution of probabilities.\\nLDA assumes that (1) words are generated from topics, which have \\nfixed conditional distributions, and (2) that the topics within a document \\nare infinitely exchangeable, which means that the joint probability \\ndistribution of these topics is not affected  by the order in which they are \\nrepresented. Reviewing statements 1 and 2 allows us to state that words \\nwithin a topic are not  infinitely exchangeable.\\nLet’s discuss parameter θ (drawn from the Dirichlet distribution), \\nwhich the dimensionality of the distribution, k, is utilized. We assume that \\nk is known and fixed, and that k-dimensional Dirichlet random variable θ \\ncan take any values in the ( k\\xa0−\\xa01) probability simplex. Here, we define the \\nprobability simplex as the area of the distribution that we draw the random \\nvariable from, graphically represented as a multidimensional triangle with \\nk + 1 vertices. The probability distribution on the simplex itself can be \\nrepresented as follows:\\n pik\\ni\\nik\\nik qaa\\naqqaa|() =()\\n()æ\\nèç\\nççö\\nø÷\\n÷÷¼=\\n=-- å\\nÕG1\\n111111\\nG,,\\n (4.1)\\nα = k-vector of positive real valued numbers. Γ(x) = gamma function.\\nSubsequently, we define the joint distribution of a mixture of topics,  \\nas follows:\\n pp pz pw z\\nnN\\nnn n (, ,| (| || qa bq aq b zw ,) ), = () ()\\n=Õ\\n1  (4.2)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n80Therefore, a given sequence of words and topics must have the \\nfollowing form:\\n pp pz pw z\\nnN\\nnn n wz,( |( | () =ò()æ\\nèçö\\nø÷\\n=Õ qq\\n1)\\n (4.3)\\nIn the LDA paper, the authors provide a useful illustration for \\nEquations 4.1, 4.2, and 4.3, as shown in Figure\\xa0 4-1.\\nThe example given in the LDA paper describes Figure\\xa0 4-1 as an \\nillustration of a topic simplex inside a word simplex comprised of three \\nwords. Each of the simplex points represent a given word and topic, \\nrespectively.\\nFigure 4-1.  Topic and word simplexesChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n81Before we complete our discussion on the theory behind LDA, let’s \\nre-create all the work in Python. Thankfully, scikit-learn provides an \\nimplementation of LDA that we will utilize in the upcoming example.\\n Topic Modeling with\\xa0LDA on\\xa0Movie Review Data\\nNext, we look at the same movie review data that we used in our document \\nclassification example. The following is an example of some of the code \\nthat we will utilize to first create out topic model. We’ll start with an \\nimplementation in sklearn.\\ndef create_topic_model(model, n_topics=10, max_iter=5, min_\\ndf=10, max_df=300, stop_words=\\'english\\', token_pattern=r\\'\\\\w+\\'):\\n    print(model + \\' topic model: \\\\n\\')\\n    data = load_data()[0]\\n    if model == \\'tf\\':\\n         feature_extractor = CountVectorizer(min_df=min_df, max_\\ndf=max_df, stop_words=stop_words, token_pattern=r\\'\\\\w+\\')\\n    else:\\n         feature_extractor = TfidfVectorizer(min_df=min_df, max_\\ndf=max_df, stop_words=stop_words, token_pattern=r\\'\\\\w+\\')\\n    processed_data = feature_extractor.fit_transform(data)\\nWe load the movie reviews that we used in Chapter 3 for the \\nclassification problem. In this example, we will imagine that we want to \\nmake a topic model for a given number of movie reviews.\\nNote  We are importing the load_data()  function from a previous \\nfile. to execute the lda_demo.py file , use a relative import \\nfrom the code_applied_nlp_python  directory, and execute the \\nfollowing command: \\'python –m chapter4.topic_modeling\\'Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n82We load the data in with our function and then prepare it for input to \\nthe LDA fit_transform()  method. Like other NLP problems, we cannot \\nput raw text into any of our algorithms; we must always preprocess it in \\nsome form. However, for producing a topic model, we will utilize both the \\nterm frequency and the TFIDF algorithms, but mainly to compare results.\\nLet’s move through the rest of the function.\\n lda_model = LatentDirichletAllocation(n_topics=n_topics, \\nlearning_method=\\'online\\', learning_offset=50., max_iter=max_\\niter, verbose=1)\\nlda_model.fit(processed_data)\\ntf_features = feature_extractor.get_feature_names()\\n print_topics(model=lda_model, feature_names=tf_features, n_top_\\nwords=n_top_words)\\nWhen we execute the following function, we get this as our output:\\ntf topic model:\\nTopic #0: libby fugitive douglas sarah jones lee detective \\ndouble innocent talk\\nTopic #1: beatty joe hanks ryan crystal niro fox mail  \\nkathleen shop\\nTopic #2: wars phantom lucas effects menace neeson jedi anakin \\nspecial computer\\nTopic #3: willis mercury simon rising jackal bruce ray lynch \\nbaseball hughes\\nTopic #4: godzilla broderick redman bvoice kim michael \\nbloomington mission space york\\nTopic #5: planet apes joe sci fi space ape alien gorilla newman\\nTopic #6: d american fun guy family woman day ll james bit\\nTopic #7: bond brosnan bottle message blake theresa pierce \\ntomorrow dies crownChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n83Topic #8: van spielberg amistad gibson en home american kevin \\nending sense\\nTopic #9: scream 2 wild williamson horror smith kevin arquette \\nsidney finn\\nBeing that this is movie data, we can see that the topics refer to both the \\nmovie and the context surrounding it. For example, topic #4 lists “Godzilla” \\n(ostensibly a character) and “Broderick” (ostensibly an actor). We can also \\nproduce topic models utilizing other feature extraction methods.\\nNow let’s look at the results of the topic model when we use the TFIDF \\nfeature extractor.\\ntfidf topic model:\\nTopic #0: libby driver jacket attending terrorists tends finn \\ndoom tough parodies\\nTopic #1: godzilla beatty douglas arthur christ technology \\nburns jesus york cases\\nTopic #2: wars lucas episode menace jar niro jedi darth anakin \\nphantom\\nTopic #3: sub theron genre keaton cooper victor rita irene \\ndating rules\\nTopic #4: midnight kim stiller mulan spice newman disney junkie \\ntroopers strange\\nTopic #5: clooney palma kevin pacino snake home toy woody \\npfeiffer space\\nTopic #6: anna disney jude carpenter men wrong siege lee king \\nfamily\\nTopic #7: scream got mail bond hanks book performances summer \\ncute dewey\\nTopic #8: en van z n er met reese die fallen lou\\nTopic #9: family american effects home guy woman michael \\noriginal 10 jamesChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n84There are similar results, although we get slightly different results \\nfor some of the topics. In some ways, the TFIDF model can be less \\ninterpretable than the term-frequency model.\\nBefore we move forward, let’s discuss how to utilize the LDA model \\nwith a new package. Gensim is a machine learning library that is heavily \\nfocused on applying machine learning and deep learning to NLP tasks. \\nThe following is code that utilizes this package in the gensim_topic_\\nmodel()  function:\\ndef gensim_topic_model():\\n    def remove_stop_words(text): (1)\\n        word_tokens = word_tokenize(text.lower())\\n         word_tokens = [word for word in word_tokens if word not \\nin stop_words and re.match(\\'[a-zA-Z\\\\-][a-zA-Z\\\\-]{2,}\\', \\nword)]\\n        return word_tokens\\n    data = load_data()[0]\\n     cleaned_data = [remove_stop_words(data[i]) for i in \\nrange(0, len(data))]\\nWhen us ing this package, the Gensim LDA implementation expects a \\ndifferent input than the gensim implementation, although it still requires \\npreprocessing. When looking at function, we have to remove stop words \\nusing a proprietary function, as we did earlier in Chapter 3. In addition to \\nthis, we should be mindful to remove words that appear too frequently, \\nand not frequently enough. Thankfully, Gensim provides a method within \\nthe corpora.Dictionary()  function to do this, as shown here:\\ndictionary = gensim.corpora.Dictionary(cleaned_data)\\ndictionary.filter_extremes(no_below=100, no_above=300)\\ncorpus = [dictionary.doc2bow(text) for text in  cleaned_data]\\n lda_model = models.LdaModel(corpus=corpus, num_topics=n_topics, \\nid2word=dictionary, verbose=1)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n85Similar to the scikit-learn method, we can filter objects based on \\ndocument frequency. The preprocessing steps we are taking here are \\nslightly different than those present in the sklearn_topic_model()  \\nfunction, which will become central to our discussion at the end of this \\nsection. Similar to what you saw before, what seems like a minor change in \\npreprocessing steps can lead to a drastically different outcome.\\nWe execute the gensim_topic_model()  function and get the following \\nresult:\\nGensim LDA implemenation:\\nTopic #0: 0.116*\"movie\" + 0.057*\"people\" + 0.051*\"like\" + \\n0.049*\"good\" + 0.041*\"well\" + 0.038*\"film\" + 0.037*\"one\" + \\n0.037*\"story\" + 0.033*\"great\" + 0.028*\"new\"\\nTopic #1: 0.106*\"one\" + 0.063*\"movie\" + 0.044*\"like\" + \\n0.043*\"see\" + 0.041*\"much\" + 0.038*\"story\" + 0.033*\"little\" + \\n0.032*\"good\" + 0.032*\"way\" + 0.032*\"get\"\\nTopic #2: 0.154*\"film\" + 0.060*\"one\" + 0.047*\"like\" + \\n0.039*\"movie\" + 0.037*\"time\" + 0.032*\"characters\" + \\n0.031*\"scene\" + 0.028*\"good\" + 0.028*\"make\" + 0.027*\"little\"\\nTopic #3: 0.096*\"film\" + 0.076*\"one\" + 0.060*\"even\" + \\n0.053*\"like\" + 0.051*\"movie\" + 0.040*\"good\" + 0.036*\"time\" + \\n0.033*\"get\" + 0.030*\"would\" + 0.028*\"way\"\\nTopic #4: 0.079*\"film\" + 0.068*\"plot\" + 0.058*\"one\" + \\n0.057*\"would\" + 0.049*\"like\" + 0.039*\"two\" + 0.038*\"movie\" + \\n0.036*\"story\" + 0.035*\"scenes\" + 0.033*\"much\"\\nTopic #5: 0.136*\"film\" + 0.067*\"movie\" + 0.064*\"one\" + \\n0.039*\"first\" + 0.037*\"even\" + 0.037*\"would\" + 0.036*\"time\" + \\n0.035*\"also\" + 0.029*\"good\" + 0.027*\"like\"\\nTopic #6: 0.082*\"movie\" + 0.072*\"get\" + 0.068*\"film\" + \\n0.059*\"one\" + 0.046*\"like\" + 0.036*\"even\" + 0.035*\"know\" + \\n0.027*\"much\" + 0.027*\"way\" + 0.026*\"story\"Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n86Topic #7: 0.131*\"movie\" + 0.097*\"film\" + 0.061*\"like\" + \\n0.045*\"one\" + 0.032*\"good\" + 0.029*\"films\" + 0.027*\"see\" + \\n0.027*\"bad\" + 0.025*\"would\" + 0.025*\"even\"\\nTopic #8: 0.139*\"film\" + 0.060*\"movie\" + 0.052*\"like\" + \\n0.044*\"story\" + 0.043*\"life\" + 0.043*\"could\" + 0.041*\"much\" + \\n0.032*\"well\" + 0.031*\"also\" + 0.030*\"time\"\\nTopic #9: 0.116*\"film\" + 0.091*\"one\" + 0.059*\"movie\" + \\n0.035*\"two\" + 0.029*\"character\" + 0.029*\"great\" + 0.027*\"like\" \\n+ 0.026*\"also\" + 0.026*\"story\" + 0.026*\"life\"\\nSo far, the results from the scikit-learn implementation of LDA using \\nterm frequency as our feature extractor has given the most interpretable \\nresults. Most of the results are homogenous, which might not lead to much \\ndifferentiation, making the results from this less useful.\\nUsing this same data set, let’s utilize another topic extraction model.\\n Non-Negative Matrix Factorization (NMF)\\nNon-negative matrix factorization (NMF) is an algorithm that takes a \\nmatrix and returns two matrices that have no non-negative elements. NMF \\nis closely related to matrix factorization, except NMF only receives non-  \\nnegative values (0 and anything above 0).\\nWe want to utilize NMF rather than another type of matrix factorization \\nbecause we need positive coefficients, as is the case when using LDA.\\xa0We \\ncan describe the process with the following mathematical formula:\\n VWH= \\nThe matrix, V , is the original matrix that we input to the data. The two \\nmatrices that we output are W and H.\\xa0In this example, let’s assume matrix \\nV has 1000 rows and 200 columns. Each row represents a word and each \\ncolumn represents a document. Therefore, we have a 1000-word vocabulary \\nfeatured across 200 documents. As it relates to the preceding equation, V is an \\nm×n matrix, W is an m×p matrix, and H is a p ×n matrix. W is a features matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n87Let’s say that we would like to find five features such that we generate \\nmatrix W with 1000 rows and 5 columns. Matrix H subsequently has a \\nshape equivalent to 5 rows and 200 columns. When we perform matrix \\nmultiplication on W and H, we yield matrix V with 1000 rows and 200 \\ncolumns, equivalent to the dimensionality described earlier. We consider \\nthat each document is built from a number of hidden features, which NMF \\nwould therefore generate. The following is the scikit-learn implementation \\nof NMF that we will utilize for this example:\\ndef nmf_topic_model():\\n     def create_topic_model(model, n_topics=10, max_iter=5,  \\nmin_df=10,\\n                            max_df=300, stop_words=\\'english\\', \\ntoken_pattern=r\\'\\\\w+\\'):\\n        print(model + \\' NMF topic model: \\')\\n        data = load_data()[0]\\n        if model == \\'tf\\':\\n             feature_extractor = CountVectorizer(min_df=min_df, \\nmax_df=max_df,\\n                                  stop_words=stop_words,  \\ntoken_pattern=token_pattern)\\n        else:\\n             feature_extractor = TfidfVectorizer(min_df=min_df, \\nmax_df=max_df,\\n                                  stop_words=stop_words,  \\ntoken_pattern=token_pattern)\\n        processed_data = feature_extractor.fit_transform(data)\\n         nmf_model = NMF(n_components=n_components,  \\nmax_iter=max_iter)\\n        nmf_model.fit(processed_data)\\n        tf_features = feature_extractor.get_feature_names()Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n88         print_topics(model=nmf_model, feature_names=tf_\\nfeatures, n_top_words=n_topics)\\n    create_topic_model(model=\\'tf\\')\\nWe invoke the NMF topic extraction in virtually the same manner that \\nwe invoke the LDA topic extraction model. Let’s look at the output of both \\nthe term frequency preprocessed data and the TFIDF preprocessed data.\\ntf NMF topic model:\\nTopic #0: family guy original michael sex wife woman r men play\\nTopic #1: jackie tarantino brown ordell robert grier fiction \\npulp jackson michael\\nTopic #2: jackie hong drunken master fu kung chan arts martial ii\\nTopic #3: scream 2 williamson horror sequel mr killer sidney \\nkevin slasher\\nTopic #4: webb jack girl gives woman ll male killed sir talking\\nTopic #5: musical musicals jesus death parker singing woman \\nnation rise alan\\nTopic #6: bulworth beatty jack political stanton black warren \\nprimary instead american\\nTopic #7: godzilla effects special emmerich star york computer \\nmonster city nick\\nTopic #8: rock kiss city detroit got music tickets band \\nsoundtrack trying\\nTopic #9: frank chicken run shannon ca mun html sullivan \\nparticularly history\\nThe following is the TFIDF NMF topic model:\\nTopic #0: 10 woman sense james sex wife guy school day ending\\nTopic #1: scream horror williamson 2 sidney craven stab killer \\narquette 3\\nTopic #2: wars phantom jedi lucas menace anakin jar effects \\ndarth gonChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n89Topic #3: space deep alien ship armageddon harry effects \\ngodzilla impact aliens\\nTopic #4: disney mulan animated joe voice toy animation apes \\nmermaid gorilla\\nTopic #5: van amistad spielberg beatty cinque political slavery \\nen slave hopkins\\nTopic #6: carpenter ott ejohnsonott nuvo subscribe reviews \\njohnson net mail e\\nTopic #7: hanks joe ryan kathleen mail shop online fox tom meg\\nTopic #8: simon sandler mercury adam rising willis wedding \\nvincent kevin julian\\nTopic #9: murphy lawrence martin eddie ricky kit robbins miles \\nclaude police\\nBefore we evaluate the results and have a more thorough discussion \\non both methods, let’s focus on visualizing the results. In the preceding \\nexample, we’ve reasonably reduced the complexity so that users can \\nassess the different topics within the analyzed documents. However, this \\nisn’t as helpful when we want to look at larger amounts of data and make \\ninferences from this topic model relatively quickly.\\nWe’ll begin with what I believe is a useful plot, supplied by pyLDAvis. \\nThis software is extremely useful and works relatively easily when used \\nwith a Jupyter notebook, which are excellent for code visualization and \\nresults presentation. It is common to utilize a Jupyter notebook when using \\na virtual machine instance from either Amazon Web Services (AWS) or \\nGoogle Cloud.\\nNote  For those of you who have not worked with google Cloud   \\nor aWs, i recommend these tutorials:  google Compute engine:   \\nwww.youtube.com/watch?v=zzMCKv1g5z0  aWs: www.youtube.\\ncom/watch?v=q1vVedHbkAYChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n90Set up an instance and start a Jupyter notebook. We will make some \\nminor adjustments for running this on your local machine to running it \\nin the cloud. In this example, the scikit-learn implementations—given the \\npreprocessing algorithms provided—make gleaning interpretable topics \\nmuch easier than the Gensim model. Although it gives more flexibility and \\nhas a lot of features, Gensim requires you to fine-tune the preprocessing steps \\nfrom scratch. If you have the time to build results from scratch, this is not a \\nproblem; however, keep this in mind when building your own application, \\nand consider the difficulties of having to use this method in Gensim.\\nIn this demo, NMF and LDA typically give similar results; however, the \\nchoice of one model vs. the other is often relative to the way we conceive \\nof the data. LDA assumes that topics are infinitely exchangeable, but the \\nwords within a topic are not. As such, if we are not concerned about the \\ntopic probability per document remaining fixed (it assumedly would not \\nbe, as not all documents contain the same topics across large corpuses), \\nLDA is a better choice. NMF might be a better choice if we have a heavy \\ndegree of certainty with respect to fixed topic probability and the data \\nset is considerably smaller. Again, these statements should be taken in \\nconsideration when evaluating the results of the respective topic models, \\nas with all machine learning problems.\\nLet’s discuss a more advanced modeling technique that plays a role \\nin sentiment analysis (in addition to more advanced NLP tasks): word \\nembeddings. We begin by discussing a body of algorithms: Word2Vec.\\n Word2Vec\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean \\nare credited with creating Word2Vec in 2014 while working at Google. \\nWord2Vec represents a significant step forward in NLP-related tasks, as it \\nprovides a method for finding vector representations of words and phrases, \\nand it can be expanded as much as the documents.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n91First, let’s examine the Skip-Gram model, which is a shallow neural \\nnetwork whose objective is to predict a word in a sequence based on the \\nwords around it. Let’s take the following sequence of training words:\\n ww wwT =¼()12,, ,  \\nThe objective function is the average log probability, represented by \\nthe following:\\n 1\\n10Tpw w\\ntT\\ncj cjtj t\\n=- ££ ¹+ åå\\n,log)(|\\n \\n pw wp wwvv\\nvvtj tO IwT\\nwi\\nwW\\nwT\\nwiO(| (|+\\n===¢()\\n¢() å))exp\\nexp1  \\nc = size of the training context, T = the total number of training words,  \\nt = index position of the current word, j = the window that determines \\nwhich word in the sequence we are looking, wt = center word of the \\nsequence, and W= number of words in the vocabulary.\\nBefore we move on, it’s important that you understand the formula and \\nhow it explains what the model does. An n-gram is a continuous grouping \\nof n words. A Skip-Gram is a generalization of an n-gram, such that we \\nhave groupings of words, but they no longer need to be continuous; that \\nis, we can skip words to create Skip-Grams. Mathematically, we typically \\ndefine k-skip-n-grams as follows:\\n {, ,, | ww wi ikii i\\njn\\njjn 12\\n11 ¼- <\\n=-å }\\n \\nLet’s assume the following is an input to the k-skip-n-gram model:\\n“The cat down the street”\\nLet’s also assume that we are seeking to create a 2-skip-bi-gram model. \\nAs such, the training examples are as follows:\\n• “The, cat” , “The, down” , “The, the” , “The, street”\\n• “cat, the” , “cat, down” , “cat, the” , “cat, street”Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n92• “down, the” , “down, cat” , “down, the” , “down, street”\\n• “the, The” , “the, cat” , “the, down” , “the, street”\\n• “street, The” , “street, cat” , “street, down” , “street, the”\\nSo now you understand how the input data is represented as words.\\nLet’s discuss how we represent these words with respect to a neural \\nnetwork. The input layer for the Skip-Gram model is a one-hot encoded \\nvector with W components. In other words, every element of the vector is \\nrepresentative of a word in the vocabulary. The Skip-Gram architecture is \\ngraphically represented in Figure\\xa0 4-2.\\nFigure 4-2.  Skip-Gram model architectureChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n93The goal of the neural network is to predict the word that has the \\nhighest probability of coming next in the input sequence. This is precisely \\nwhy we want to use softmax, and ultimately how you intuitively understand \\nthe formula. We want to predict the word that is most probable given the \\ninput of the words, and we are calculating this probability based on the \\nentirety of the input and output sequences observed by the neural network.\\nBe that as it may, we have a minor problem. Softmax computation \\nscales proportionally to the input size, which bodes poorly for this \\nproblem because accurate results will likely require large vocabulary sizes \\nfor training data. Hence, it is often suggested that we use an alternative \\nmethod. One of the methods often referenced is negative sampling. \\nNegative sampling is defined in the following equation:\\n log~ log ss¢() + () -() éëùû\\n=å vv Pw vvwoT\\nwi\\nik\\nwi nw iT\\nwi\\n1\\uf045 \\nNegative sampling achieves a cheaper computation than the softmax \\nactivation function by approximating its output. More precisely, we are \\nonly going to change K number of weights in the word embedding rather \\nthan computing them all. The Word2Vec paper suggests to sample  \\nwith 5 to 20 words in smaller data sets, but 2 to 5 words in larger data sets \\ncan achieve positive results.\\nBeyond the training of the word embedding, what are we actually \\ngoing to use it for? Unlike many neural networks, the main objective is not \\nnecessarily to use it for the purpose of prediction, but rather to obtain the \\ntrained hidden layer weight matrix. The hidden layer weight matrix is our \\ntrained word embedding . Once this hidden layer is trained, certain words \\ncluster in areas of vector space, where they share similar contexts.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n94 Example Problem 4.2: Training a\\xa0Word \\nEmbedding (Skip-Gram)\\nLet’s display the power of Word2Vec by working through a demo example, \\nin Gensim and in TensorFlow. The following is some of the code that begins \\nour implementation of the TensorFlow Word2Vec Skip-Gram model:\\ndef remove_non_ascii(text):\\n    return \".join([word for word in text if ord(word) < 128])\\ndef load_data(max_pages=100):\\n    return_string = StringIO()\\n     device = TextConverter(PDFResourceManager(), return_string, \\ncodec=\\'utf-8\\', laparams=LAParams())\\n     interpreter = PDFPageInterpreter(PDFResourceManager(), \\ndevice=device)\\n     filepath = file(\\'/Users/tawehbeysolow/Desktop/applied_nlp_\\npython/datasets/economics_textbook.pdf\\', \\'rb\\')\\n     for page in PDFPage.get_pages(filepath, set(), \\nmaxpages=max_pages, caching=True, check_extractable=True):\\n        interpreter.process_page(page)\\n    text_data = return_string.getvalue()\\n    filepath.close(), device.close(), return_string.close()\\n    return remove_non_ascii(text_data)\\nFor our example problem, we will utilize the PDFMiner Python module. \\nFor those of you who often parse data in different forms, this package is \\nhighly recommended. PDF data is notorious in parsing, as it is often filled \\nwith images and metadata that makes preprocessing the data a hassle. \\nThankfully, PDFMiner takes care of most of the heavy lifting, making our \\nprimary concerns only cleaning out stop words, grammatical characters, \\nand other preprocessing steps, which are relatively straightforward. For this \\nproblem, we will read data from an economics textbook.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n95def gensim_preprocess_data():\\n    data = load_data()\\n    sentences = sent_tokenize(data)\\n     tokenized_sentences = list([word_tokenize(sentence) for \\nsentence in sentences])\\n    for i in range(0, len(tokenized_sentences)):\\n         tokenized_sentences[i] = [word for word in tokenized_\\nsentences[i] if word not in punctuation]\\n    return tokenized_sentences\\nWe now move to tokenizing the data based on sentences. Do not \\nremove punctuation before this step . The NLTK sentence tokenizer relies on \\npunctuation to determine where to split data based on sentences. If this is \\nremoved, it can cause you to debug something rather trivial. Regardless, \\nthe next format the data should take is that of a list, where every entry is a \\nsentence whose words are tokenized, such that the words appear as follows:\\n [[\\'This\\', \\'text\\', \\'adapted\\', \\'The\\', \\'Saylor\\', \\'Foundation\\', \\n\\'Creative\\', \\'Commons\\', \\'Attribution-NonCommercial-ShareAlike\\', \\n\\'3.0\\', \\'License\\', \\'without\\', \\'attribution\\', \\'requested\\', \\n\\'works\\', \\'original\\', \\'creator\\', \\'licensee\\'], [\\'Saylor\\', \\n\\'URL\\', \\'http\\', \\'//www.saylor.org/books\\', \\'Saylor.org\\', \\'1\\', \\n\\'Preface\\', \\'We\\', \\'written\\', \\'fundamentally\\', \\'different\\', \\n\\'text\\', \\'principles\\', \\'economics\\', \\'based\\', \\'two\\', \\'premises\\', \\n\\'1\\'], [\\'Students\\', \\'motivated\\', \\'study\\', \\'economics\\', \\'see\\', \\n\\'relates\\', \\'lives\\'], [\\'2\\'], [\\'Students\\', \\'learn\\', \\'best\\', \\n\\'inductive\\', \\'approach\\', \\'first\\', \\'confronted\\', \\'question\\', \\n\\'led\\', \\'process\\', \\'answer\\', \\'question\\'], [\\'The\\', \\'intended\\', \\n\\'audience\\', \\'textbook\\', \\'first-year\\', \\'undergraduates\\', \\n\\'taking\\', \\'courses\\', \\'principles\\', \\'macroeconomics\\', \\n\\'microeconomics\\'], [\\'Many\\', \\'may\\', \\'never\\', \\'take\\', \\n\\'another\\', \\'economics\\', \\'course\\'], [\\'We\\', \\'aim\\', \\'increase\\', \\n\\'economic\\', \\'literacy\\', \\'developing\\', \\'aptitude\\', \\'economic\\', Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n96\\'thinking\\', \\'presenting\\', \\'key\\', \\'insights\\', \\'economics\\', \\n\\'every\\', \\'educated\\', \\'individual\\', \\'know\\'], [\\'Applications\\', \\n\\'ahead\\', \\'Theory\\', \\'We\\', \\'present\\', \\'theory\\', \\'standard\\', \\n\\'books\\', \\'principles\\', \\'economics\\'], [\\'But\\', \\'beginning\\', \\n\\'applications\\', \\'also\\', \\'show\\', \\'students\\', \\'theory\\', \\n\\'needed\\'], [\\'We\\', \\'take\\', \\'kind\\', \\'material\\', \\'authors\\', \\'put\\', \\n\\'applications\\', \\'boxes\\', \\'place\\', \\'heart\\', \\'book\\'], [\\'Each\\', \\n\\'chapter\\', \\'built\\', \\'around\\', \\'particular\\', \\'business\\', \\n\\'policy\\', \\'application\\', \\'microeconomics\\', \\'minimum\\', \\'wages\\', \\n\\'stock\\', \\'exchanges\\', \\'auctions\\', \\'macroeconomics\\', \\'social\\', \\n\\'security\\', \\'globalization\\', \\'wealth\\', \\'poverty\\', \\'nations\\']\\nNow that we have finished the preprocessing of our data, we can work \\nwith the Gensim implementation of the Skip-Gram model.\\ndef gensim_skip_gram():\\n    sentences = gensim_preprocess_data()\\n     skip_gram = Word2Vec(sentences=sentences, window=1,  \\nmin_count=10, sg=1)\\n    word_embedding = skip_gram[skip_gram.wv.vocab] (1)\\nInvoking the Skip-Gram model is relatively straightforward, and the \\ntraining of the model is taken care of for us as well. The training process \\nof a Skip-Gram model mimics that of all neural networks, in that we pass \\nan input through all the layers and then backpropagate the error through \\neach of the respective weights in each layer, updating them until we have \\nreached a loss tolerance threshold or until the maximum number of \\nepochs has been reached. Once the word embedding has been trained, \\nwe obtain the weight matrix by indexing the model with the wv.vocab  \\nattribute of the model itself.\\nNow, let’s discuss visualizing the words as vectors.\\n    pca = PCA(n_components=2)\\n    word_embedding = pca.fit_transform(word_embedding)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n97    #Plotting results from trained word embedding\\n    plt.scatter(word_embedding[:, 0], word_embedding[:, 1])\\n    word_list = list(skip_gram.wv.vocab)\\n    for i, word in enumerate(word_list):\\n         plt.annotate(word, xy=(word_embedding[i, 0],  \\nword_embedding[i, 1]))\\nWord embeddings are output in dimensions that are difficult to \\nvisualize in their raw formats. As such, we need to find a way to reduce \\nthe dimensionality of this matrix, while also retaining all the variance and \\nattributes of the original data set. A preprocessing method that does this \\nis principal components analysis (PCA). Briefly, PCA transforms a matrix \\nso that it returns an eigen-decomposition called eigenvectors, in addition \\nto eigenvalues. For the sake of showing a two-dimensional plot, we want \\nto create a transformation that yields two  principal components. It is \\nimportant to remember that these principal components are not exactly \\nthe same as the original matrix, but an orthogonal transformation of the \\nword embedding that is related to it. Figure\\xa0 4-3 illustrates the matrix.\\nFigure 4-3.  Skip-Gram word embeddings generated via GensimChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n98In the vector space, words that are closer to one another appear in \\nsimilar contexts, and words that are further away from each other are more \\ndissimilar in respect to the contexts in which they appear. Cosine similarity \\nis a common method for measuring this. Mathematically, cosine distance \\nis described as follows:\\n cosq()=*AB\\nAB  \\nWe intuitively describe cosine similarity as the sum of the product of \\nall the respective elements of two given vectors, divided by the product of \\ntheir Euclidean norms. Two vectors that have a 0-degree difference yield \\na cosine similarity of 1; whereas two vectors with a 90-degree difference \\nyield a cosine similarity of 0. The following is an example of some of the \\ncosine distances between different word vectors:\\nCosine distance for people   and Saylor\\n -0.10727774727479297\\nCosine distance for URL   and people\\n -0.137377917173043\\nCosine distance for money   and URL\\n -0.03124461706797222\\nCosine distance for What   and money\\n -0.007384979727807199\\nCosine distance for one   and What\\n 0.022940581092187863\\nCosine distance for see   and one\\n 0.05983065381073224\\nCosine distance for economic   and see\\n -0.0530102968258333\\nGensim takes care of some of the uglier aspects of preprocessing the \\ndata. However, it is useful to know how to perform some of these things \\nfrom scratch, so let’s try implementing a word embedding utilizing the \\nsame data, except this time we will do it in TensorFlow.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n99Let’s walk through a toy implementation to ensure that you are aware \\nof what the model is doing, and then walk through an implementation that \\nis easier to deploy.\\ndef tf_preprocess_data(window_size=window_size):\\n    def one_hot_encoder(index, vocab_size):\\n        vector = np.zeros(vocab_size)\\n        vector[index] = 1\\n        return vector\\n    text_data = load_data()\\n    vocab_size = len(word_tokenize(text_data))\\n    word_dictionary = {}\\n    for index, word in enumerate(word_tokenize(text_data)):\\n        word_dictionary[word] = index\\n    sentences = sent_tokenize(text_data)\\n    tokenized_sentences = list([word_tokenize(sentence) for \\nsentence in sentences])\\n    n_gram_data = []\\nWe must prepare the data slightly differently for TensorFlow than we \\ndid for Gensim. The Gensim Word2Vec method takes care of most of the \\nback-end things for us, but it is worthwhile to implement a simple proof of \\nconcept from scratch and walk through the algorithm.\\nWe begin by making a dictionary that matches a word with an index \\nnumber. This index number forms the position in our one-hot encoded \\ninput and output vectors.\\nLet’s continue preprocessing the data.\\n#Creating word pairs for word2vec model\\n    for sentence in tokenized_sentences:\\n        for index, word in enumerate(sentence):\\n            if word not in punctuation:Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n100                for _word in sentence[max(index  - window_size, 0):\\n                                       min(index + window_size, \\nlen(sentence)) + 1]:\\n                    if _word != word:\\n                        n_gram_data.append([word, _word])\\nThe preceding section of code effectively creates our n-grams, and \\nultimately simulates how the Skip-Gram model convolves over a sentence \\nin such a way that it can predict the following word with the highest \\nprobability. We then create an m×n matrix, where m is the number of words \\nin our input sequences, and n is the number words in the vocabulary.\\n #One-hot encoding data and creating dataset intrepretable by \\nskip-gram model\\n x, y = np.zeros([len(n_gram_data), vocab_size]), \\nnp.zeros([len(n_gram_data), vocab_size])\\nfor i in range(0, len(n_gram_data)):\\n     x[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i]\\n[0]], vocab_size=vocab_size)\\n     y[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i]\\n[1]], vocab_size=vocab_size)\\nreturn x, y, vocab_size, word_dictionary\\nMoving forward to the function that we will use to construct our Skip-  \\nGram mo del, we begin by loading the data and the vocabulary size and \\nword dictionary. As with other neural network models, we instantiate the \\nplaceholders, variables, and weights. Per the Skip-Gram model diagram \\nshown in Figure\\xa0 4-2, we only need to contain a hidden and an output \\nweight matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n101def tensorflow_word_embedding(learning_rate=learning_rate, \\nembedding_dim=embedding_dim):\\n    x, y, vocab_size, word_dictionary = tf_preprocess_data()\\n    #Defining tensorflow variables and placeholder\\n    X = tf.placeholder(tf.float32, shape=(None, vocab_size))\\n    Y = tf.placeholder(tf.float32, shape=(None, vocab_size))\\n    weights = {\\'hidden\\': tf.Variable(tf.random_normal([vocab_\\nsize, embedding_dim])),\\n                \\'output\\': tf.Variable(tf.random_\\nnormal([embedding_dim, vocab_size]))}\\n     biases = {\\'hidden\\': tf.Variable(tf.random_\\nnormal([embedding_dim])),\\n               \\'output\\':  tf.Variable(tf.random_normal([vocab_\\nsize]))}\\n     input_layer = tf.add(tf.matmul(X, weights[\\'hidden\\']), \\nbiases[\\'hidden\\'])\\n     output_layer = tf.add(tf.matmul(input_layer, \\nweights[\\'output\\']), biases[\\'output\\'])\\nIn C hapter 5, we walk through implementing negative sampling. \\nHowever, because the number of examples that we are using here is relatively \\nminiscule, we can get away with utilizing the regular implementation of \\nsoftmax as provided by TensorFlow. Finally, we execute our graph, as with \\nother TensorFlow models, and observe the results shown in Figure\\xa0 4-4.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n102Cosine distance for dynamic   and limited\\n 0.4128825113896724\\nCosine distance for four   and dynamic\\n 0.2833843609582811\\nCosine distance for controversial   and four\\n 0.3266445485300576\\nCosine distance for hanging   and controversial\\n 0.37105348488163503\\nCosine distance for worked   and hanging\\n 0.44684699747383416\\nCosine distance for Foundation   and worked\\n 0.3751656692569623\\nFigure 4-4.  Word vectors from toy implementation of Skip-  GramChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n103Again, the implementations provided here are not final examples of \\nwhat well-trained word embeddings necessarily looks like. We will tackle \\nthat task more specifically in Chapter 5, as data collection is largely the \\nissue that we must discuss in greater detail. However, the Skip-Gram \\nmodel is only one of the word embeddings that we will likely encounter.\\nWe now will continue our discussion by tackling the continuous  \\nbag-  of- words model.\\n Continuous Bag-of-Words (CBoW)\\nSimilar to a Skip-Gram model, a continuous bag-of-words model (CBoW) \\nis training on the objective of predicting a word. Unlike the Skip-Gram \\nmodel, however, we are not trying to predict the next word in a given \\nsequence. Instead, we are trying to predict some center word based on \\nthe context around the target label. Let’s imagine the following input data \\nsentence:\\n“The boy walked to the red house”\\nIn the context of the CBoW model, we could imagine that we would \\nhave an input vector that appeared as follows:\\n“The, boy, to, the, red, house”\\nHere, “walked” is the target that we are trying to predict. Visually, the \\nCBoW model looks like Figure\\xa0 4-5.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n104Each word in the input is represented in a single one-hot encoded vector . \\nSimilar to the Skip-Gram model, the length of the input vector is equal to \\nthe number of words in the vocabulary. When evaluating our input data, a \\nvalue of “1” is for the words that are present and a “0” is for the words that \\nare not present. In Figure\\xa0 4-5, we are predicting a target word, w_t, based \\non the words w_t-2, w_ t-1, w_ t+1, and w_t+2.\\nWe then perform a weighted sum operation on this input vector with \\nweight and bias matrices that pass these values to the projection layer, \\nwhich is similar to the projection layer featured in the Skip-Gram model. \\nFinally, we predict the class label with another weighted sum operation \\nwith the output weight and bias matrices in addition to utilizing a softmax \\nclassifier. The training method is the same as the one used in the Skip-  \\nGram model.\\nNext, let’s work with a short example utilizing Gensim.\\nFigure 4-5.  CBoW model representationChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n105 Example Problem 4.2: Training a\\xa0Word \\nEmbedding (CBoW)\\nThe Gensim implementation of CBoW requires that only a single \\nparameter is changed, as shown here:\\ncbow = Word2Vec(sentences=sentences, window=skip_gram_window_\\nsize, min_count=10, sg=0)\\nWe invoke this method and observe the results in the same manner \\nthat we did for the Skip-Gram model. Figure\\xa0 4-6 shows the results.\\nFigure 4-6.  CBoW wor d embedding visualizationChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n106 Global Vectors for\\xa0Word  \\nRepresentation (GloVe)\\nGloVe is a contemporary and advanced method of vector representation \\nof words. In 2014, Jeffrey Pennington, Richard Socher, and Christopher \\nManning wrote a paper in which they describe GloVe. This type of \\nword embedding is an improvement over both matrix factorization–\\nbased representations of words, as well as the Skip-gram model. Matrix \\nfactorization–based methods of word representation are not particularly \\ngood at representing words with respect to their analogous nature. \\nHowever, Skip-Gram and CBoW train on isolated windows of text and do \\nnot utilize the same information that a matrix-based factorization method \\ndoes. Specifically, when we use LDA to create a topic model, we have \\nto preprocess the text in a way that encodes each word with statistical \\ninformation that represents the word in the context of the whole text. With \\nSkip-Gram and CBoW, the one-hot encoded vector doesn’t capture that \\nsame type of complexity.\\nGloVe specifically trains on “global word-to-word co-occurrence \\ncounts. ” Co-occurrence is the instance of two words appearing in a specific \\norder alongside one another. By global , I mean the co-occurrence counts \\nwith respect to all documents in the corpus that we are analyzing. In this \\nsense, GloVe is utilizing a bit of the intuition behind both models to try and \\novercome the respective shortcomings of the aforementioned alternatives.\\nLet’s begin by defining a co-occurrence matrix, X.\\xa0Each entry in the \\nmatrix represents the co-occurrence count of two specific words. More \\nspecifically, X i, j represents the number of times word j appears in the \\ncontext of word i. The following notation is also worth noting:\\n Xi=å\\nkikX, (4.4)\\n PP jiX\\nXijij\\ni,,=() = |\\n (4.5)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n107Equation 4.4 is defined as the number of times any word appears in the \\ncontext of the word I.\\xa0Equation 4.5 is the probability of a word j given word \\ni. We define this probability as the co-occurrence account of word j appears \\nin the context of the word “I” with the total co-occurrence counts of word i.\\nI suggest that the model should evaluate the ratios of co-occurrence \\nprobabilities, which we define as follows:\\n Fw wwP\\nPij kik\\njk,,\\uf025 () =\\n (4.6)\\nw\\xa0∈\\xa0ℝd= word vectors and \\uf025wkdÎ\\uf052 = context vectors, F = exp( x)∗\\nYou should observe that our definition of F has an asterisk above \\nit, particularly to indicate the fact that the value of F can be a multitude \\nof things; however, we often derive it to be the preceding definition. \\nThe purpose of F is to encode the value yielded from the co-occurrence \\nprobabilities into the word embedding.\\nThe following functions derive the target label and the error function \\nwe use to train the GloVe word embedding:\\n Fw wPiT\\nki k\\uf025() =, (4.7)\\n ww bb XiT\\nji ji j\\uf025\\uf025 ++ -log, (4.8)\\n Jf Xw wb bX\\nijV\\nij iT\\nji ji j = () ++ - ()\\n=å\\n,,, log\\n12\\uf025\\uf025\\n (4.9)\\nWhere f\\xa0(Xij) = weighting function\\nAs detailed in the GloVe paper, the weighting function should obey a \\nfew rules. Foremost, if f is a continuous function, it should vanish as x\\xa0→\\xa00, \\nf\\xa0(x) should be non-decreasing, and f(x) should be relatively small for large \\nvalues of x. These rules are to ensure that rare or frequent co-occurrence \\nvalues are not overweighted in the training of the word embedding. Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n108Although the weighting function can be altered, the GloVe paper suggests \\nthe following equation:\\n fxx\\nxifxx\\notherwis emm()=æ\\nèçö\\nø÷ <ì\\níï\\nîïa\\n1  \\nxm = maximum value of x, fixed to 100. The weighting function yields \\nthe values shown in Figure\\xa0 4-7 with respect to the x value.\\nNow that we have reviewed the model, it is useful for you to \\nunderstand how to use pretrained word embeddings, particularly since not \\neveryone will have the time or the ability to train these embeddings from \\nscratch due to the difficult nature of acquiring all of this data. Although \\nthere is not necessarily one predetermined place to get a word embedding \\nfrom, you should be aware of the following GitHub repository that contains \\nFigure 4-7.  Weighting function for GloVeChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n109the files for multitudes of word embeddings:  https://github.com/3Top/\\nword2vec-api#where-to-get-a-pretrained-models . You can feel free to \\nexperiment and deploy these word embeddings for different tasks where \\nthey see fit.\\nFor this example, we will use a GloVe word embedding that contains \\n6 billion words and 50 features. This word embedding was trained from \\ndata taken from Wikipedia and has a vocabulary containing 400,000 words. \\nNow, let’s begin with the code, shown here:\\ndef load_embedding(embedding_path=\\'/path/to/glove.6B.50D.txt\\'):\\n    vocabulary, embedding = [], []\\n    for line in open(embedding_path, \\'rb\\').readlines():\\n        row = line.strip().split(\\' \\')\\n        vocabulary.append(row[0]), embedding.append(row[1:])\\n     vocabulary_length, embedding_dim = len(vocabulary), \\nlen(embedding[0])\\n    return vocabulary, np.asmatrix(embedding), vocabulary_\\nlength, embedding_dim\\nWe begin this problem by loading the word embeddings using the \\nnative open()  function to read the file line by line. Each line in the file \\nstarts with a word in the vocabulary, and the subsequent entries in that \\nline represent the values within each of that word’s vector. We iterate \\nthrough all the lines in the file, appending the word and the word vector to \\ntheir respective arrays. As such, we are able to create a list of words within \\na vocabulary and reconstruct the word embeddings from a .txt  file. This \\ntrained embedding should look like Figure\\xa0 4-8.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n110Figure 4-8 shows the representation of the first 50 words in the \\nvocabulary when we look at the two principal components yielded from \\nthe transformation of our word embedding. Examples of words that seem \\nto appear in similar contexts are had  and has, and , and as, in addition to \\nhis and he. When comparing the cosine similarities of other words in the \\nvocabulary, we observe the following.\\nCosine Similarity Between so and u.s.: 0.5606769548631282\\nCosine Similarity Between them and so: 0.8815159254335486\\nCosine Similarity Between what and them: 0.8077565084355354\\nCosine Similarity Between him and what: 0.7972281857691554\\nCosine Similarity Between united and him: 0.5374600664967559\\nCosine Similarity Between during and united: 0.6205250403136882\\nCosine Similarity Between before and during: 0.8565694276984954\\nCosine Similarity Between may and before: 0.7855322363492923\\nCosine Similarity Between since and may: 0.7821437532357596\\nFigure 4-8.  GloV e pretrained embeddingChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n111 Example Problem 4.4: Using Trained Word \\nEmbeddings with\\xa0LSTMs\\nNow that we have visually inspected the word embedding, let’s focus \\non how to use trained embeddings with a deep learning algorithm. \\nLet’s imagine that we would like to include the following paragraph as \\nadditional training data for our word embedding.\\nsample_text = \"\\'Living in different places has been the\\ngreatest experience that I have had in my life. It has allowed\\nme to understand people from different walks of life, as well \\nas to question some of my own biases I have had with respect\\nto people who did not grow up as I did. If possible, everyone\\nshould take an opportunity to travel somewhere separate from \\nwhere they grew up.\"\\'.replace(\\'\\\\n\\', \")\\nWith our sample data assigned to a variable, let’s begin by performing \\nsome of the same preprocessing steps that we have familiarized ourselves \\nwith, exemplified by the following body of code:\\ndef sample_text_dictionary(data=_sample_text):\\n     count, dictionary = collections.Counter(data).most_\\ncommon(), {} #creates list of word/count pairs;\\n    for word, _ in count:\\n         dictionary[word] = len(dictionary) #len(dictionary) \\nincreases each iteration\\n         reverse_dictionary = dict(zip(dictionary.values(), \\ndictionary.keys()))\\n     dictionary_list = sorted(dictionary.items(),  \\nkey = lambda x : x[1])\\n    return dictionary, reverse_dictionary, dictionary_listChapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n112We start by using a remove_stop_words()  function, a redefinition of \\na sample preprocessing text algorithm defined in Chapter 3 that removes \\nstop words from relatively straightforward sample data. When you are \\nusing data that isn’t as clean as this sample data, I recommend that \\nyou preprocess the data in a manner similar to what you did using the \\neconomics textbook or War and Peace .\\nMoving to the sample_text_dictionary()  function, we create a \\nterm frequency dictionary, and then return these variables. This process \\nis important for you to understand, because this is an example of how \\nwe deal with words that are not in the vocabulary of a trained word \\nembedding:\\nfor i in range(len(dictionary)):\\n    word = dictionary_list[i][0]\\n    if word in vocabulary:\\n        _embedding_array.append(embedding_dictionary[word])\\n    else:\\n         _embedding_array.append(np.random.uniform(low=-0.2, \\nhigh=0.2, size=embedding_dim))\\nWe b egin by creating a variable title: _embedding_array . This variable \\nactually contains the word embedding representations of our sample \\ntext. To handle words that are not in the vocabulary, we will create a \\nrandomized distribution of numbers to simulate a word embedding, which \\nwe then feed as inputs to the neural network.\\nMoving forward, we make the final transformations to the embedding \\ndata before we create our computation graph.\\nembedding_array = np.asarray(_embedding_array)\\n decision_tree = spatial.KDTree(embedding_array, leafsize=100)Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n113We will use a k-nearest neighbors tree to find the embedding that is \\nclosest to the array that our neural network outputs. From this, we use \\nreverse_dictionary  to find the word that matches the predicted embedding.\\nLet’s build our computational graph, as follows:\\n#Initializing placeholders and other variables\\nX = tf.placeholder(tf.int32, shape=(None, None, n_input))\\nY = tf.placeholder(tf.float32, shape=(None, embedding_dim))\\n weights = {\\'output\\': tf.Variable(tf.random_normal([n_hidden, \\nembedding_dim]))}\\n biases = {\\'output\\': tf.Variable(tf.random_normal([embedding_\\ndim]))}\\n _weights = tf.Variable(tf.constant(0.0, shape=[vocabulary_\\nlength, embedding_dim]), trainable=True)\\n _embedding = tf.placeholder(tf.float32, [vocabulary_length, \\nembedding_dim])\\nembedding_initializer = _weights.assign(_embedding)\\nembedding_characters = tf.nn.embedding_lookup(_weights, X)\\n input_series = tf.reshape(embedding_characters, [-1, n_input])\\ninput_series = tf.split(input_series, n_input, 1)\\nYou will find most of this similar to the LSTM tutorial in Chapter 2, \\nbut direct your attention to the second grouping of code, specifically \\nwhere we create the _weights  and _embedding  variables. When we are \\nloading a trained word embedding, or have an embedding layer in our \\ncomputational graph, the data must pass through this layer before it can \\nget to the neural network. The dimension of the network is the number of \\nwords in the vocabulary by the number of features. Although the number \\nof features when training one’s own embedding can be altered, this is a \\npredetermined value when we load a word embedding.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n114We assign the weights variable to the _embedding  placeholder, which \\nis ultimately the weights our optimizer is tuning, whereupon we create an \\nembedding characters variable.  The tf.nn.embedding_lookup()  function \\nspecifically retrieves the index numbers of the _weights  variable. Finally, \\nwe transform the embedding_characters  variable into the input_series  \\nvariable, which is actually directly fed into the LSTM layer.\\nFrom this point forward, the passage of data from the LSTM layer \\nthrough the rest of the graph should be familiar from the tutorial. When \\nexecuting the code, you should see output such as the following:\\nInput Sequence: [\\'me\\', \\'to\\', \\'understand\\', \\'people\\']\\nActual Label: from\\nPredicted Label: an\\nEpoch: 210\\nError: 45.62042\\nInput Sequence: [\\'different\\', \\'walks\\', \\'of\\', \\'life,\\']\\nActual Label: as\\nPredicted Label: with\\nEpoch: 220\\nError: 64.55679\\nInput Sequence: [\\'well\\', \\'as\\', \\'to\\', \\'question\\']\\nActual Label: some\\nPredicted Label: has\\nEpoch: 230\\nError: 75.29771\\nAn immediate suggestion for improving the error rate is to load \\ndifferent sample texts, perhaps from an actual corpus of data to train on, \\nas the limited amount of data does not allow the accuracy to improve \\nsignificantly much.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n115Another suggestion is to use the load_data()  function that is \\ncommented out loading your own PDF file and experimenting from that \\npoint forward.\\nNow that we have reviewed the methods in which we can represent \\nwords as vectors, let’s discuss other textual representations. Thankfully, \\nsince most of these are Word2Vec abstractions , it will not require nearly as \\nmuch explanation this time around.\\n Paragraph2Vec: Distributed Memory \\nof\\xa0Paragraph Vectors (PV-DM)\\nParagraph2Vec is an algorithm that allows us to represent objects of \\nvarying length, from sentences to whole documents, for the same purposes \\nthat we represented words as vectors in the previous examples. This \\ntechnique was developed by Quoc Le and Tomas Mikolov, and largely is \\nbased off the Word2Vec algorithm.\\nIn Paragraph2Vec, we represent each paragraph as a unique vector \\nin a matrix, D. Every word is also mapped to a unique vector, represented \\nby a column in matrix W. We subsequently construct a matrix, h, which \\nis formed by concatenating matrices W and D. We think of this paragraph \\ntoken as an analog to the cell state from the LSTM, in that it is providing \\nmemory to the current context in the form of the topic of the paragraph. \\nIntuitively, that means that matrix W is the same across all paragraphs, \\nsuch that we observe the same representation of a given word. Training \\noccurs as it does in Word2Vec, and the negative sampling can occur in this \\ninstance by sampling from a fixed-length context in a random paragraph.\\nTo ensure that you understand how this works functionally, let’s look at \\none final example in this chapter.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n116 Example Problem 4.5: Paragraph2Vec Example \\nwith\\xa0Movie Review Data\\nOnce again, Gensim thankfully has a Doc2Vec method that makes \\nimplementation of this algorithm relatively straightforward. In this \\nexample, we will keep things relatively simple and represent sentences in \\na vector space, rather than create or approximate a paragraph tokenizer, \\nwhich we would likely want to be more precise than a heuristic that \\nwould be relatively quick to draw up (i.e., paragraphs comprised of four \\nsentences each). In the doc2vec_example.py  file, there are only slight \\ndifferences in the Doc2Vec model and the Word2Vec model, specifically \\nthe preprocessing.\\ndef gensim_preprocess_data(max_pages):\\n    sentences = namedtuple(\\'sentence\\', \\'words tags\\')\\n    _sentences = sent_tokenize(load_data(max_pages=max_pages))\\n    documents = []\\n    for i, text in enumerate(_sentences):\\n        words, tags = text.lower().split(), [i]\\n        documents.append(sentences(words, tags))\\n    return documents\\nThe Doc2Vec implementation expects what is known as a named \\ntuple object . This tuple contains a list of tokenized words contained \\nin the sentence, as well as an integer that indexes this document. In \\nonline documentation, some people utilize a class object entitled \\nLabledLineSentence() ; however, this performs the necessary \\npreprocessing the same way. When we run our script, we iterate through \\nall the sentences that we are analyzing, and view their associated cosine \\nsimilarities. The following is an example of some of them:\\nDocument sentence(words=[\\'this\\', \\'text\\', \\'adapted\\', \\'the\\', \\n\\'saylor\\', \\'foundation\\', \\'creative\\', \\'commons\\', \\'attribution-  Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n117noncommercial-  sharealike\\', \\'3.0\\', \\'license\\', \\'without\\', \\n\\'attribution\\', \\'requested\\', \\'works\\', \\'original\\', \\'creator\\', \\n\\'licensee\\', \\'.\\'], tags=[0])\\nDocument sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\n\\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\n\\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\n\\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], \\ntags=[1])\\nCosine Similarity Between Documents: -0.025641936104727547\\nDocument sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\n\\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\n\\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\n\\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], \\ntags=[1])\\nDocument sentence(words=[\\'students\\', \\'motivated\\', \\'study\\', \\n\\'economics\\', \\'see\\', \\'relates\\', \\'lives\\', \\'.\\'], tags=[2])\\nCosine Similarity Between Documents:\\n0.06511943195883922\\nBeyond this, Gensim also allows us to infer vectors without having \\nto retrain our models on these vectors. This is particularly important in \\nChapter 5, where we apply word embeddings in a practical setting. You \\ncan see this functionality when we execute the code with the training_\\nexample  parameter set to False. We have two sample documents, which we \\ndefine at the beginning of the file:\\nsample_text1 = \"\\'I love italian food. My favorite items are\\npizza and pasta, especially garlic bread. The best italian food\\nI have had has been in New  York. Little Italy was very fun\"\\'Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n118sample_text2 = \"\\'My favorite time of italian food is pasta with\\nalfredo sauce. It is very creamy but the cheese is the best\\npart. Whenevr I go to an italian restaurant, I am always \\ncertain to get a plate.\"\\'\\nThese two examples are fairly similar. When we train our model—more \\nthan 300 pages worth of data from an economics textbook, we get the \\nfollowing results:\\n Cosine Similarity Between Sample Texts:\\n0.9911814256706748\\nAgain, you should be aware that they will likely need significantly \\nlarger amounts of data to get reasonable results across unseen data. \\nThese examples show them how to train and infer vectors using various \\nframeworks. For those who are dedicated to training their own word \\nembeddings, the path forward should be fairly clear.\\n Summary\\nBefore we move on to work on natural language processing tasks, let’s \\nrecap some of the most important things learned in this chapter. As \\nyou saw in Chapter 3, preprocessing data correctly is the majority of the \\nwork that we need to perform when applying deep learning to natural \\nlanguage processing. Beyond cleaning out stop words, punctuation, and \\nstatistical noise, you should be prepared to wrangle data and organize \\nit in an interpretable format for the neural network. Well-trained word \\nembeddings often require the collection of billions of tokens.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n119Making sure that you aggregate the right data is extremely important, \\nas a couple billion tokens from radically different data sources can leave \\nyou with an embedding that doesn’t yield much of anything useful. \\nAlthough some of our examples yielded positive results, it does not \\nmean these applications would work in a production environment. You \\nmust (responsibly) collect large amounts of text data from sources while \\nmaintaining homogeneity in vocabulary and context.\\nIn the following chapter, we conclude the book by working on \\napplications of recurrent neural networks.Chapter 4  topiC Modeling and\\xa0Word eMbeddings\\n\\n121© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5_5CHAPTER 5\\nText Generation, \\nMachine Translation, \\nand\\xa0Other Recurrent \\nLanguage Modeling \\nTasks\\nIn Chapter 4, I introduced you to some of the more advanced deep \\nlearning and NLP techniques, and I discussed how to implement these \\nmodels in some basic problems, such as mapping word vectors. Before \\nwe conclude this book, I will discuss a handful of other NLP tasks that are \\nmore domain-specific, but nonetheless useful to go through.\\nBy this point, you should be relatively comfortable with preprocessing \\ntext data in various formats, and you should understand a few NLP tasks, \\nsuch as document classification, well enough to perform them. As such, \\nthis chapter focuses on combining many of the skills we have worked with \\nby tackling a couple of problems. All solutions provided in this chapter are \\nfeasible. You are more than welcome to present or complete new solutions \\nthat outperform them.\\n\\n122 Text Generation with\\xa0LSTMs\\nText generation is increasingly an important feature in AI-based tools. \\nParticularly when working with large amounts of data, it is useful \\nfor systems to be able to communicate with users to provide a more \\nimmersive and informative experience. For text generation, the main goal \\nis to create a generative model that provides some sort of insight with \\nrespect to the data. You should be aware that text generation should not \\nnecessarily create a summary of the document, but generate an output \\nthat is descriptive of the input text. Let’s start by inspecting the problem.\\nInitially, for such a task, we need a data source. From that, our data \\nsource changes the results. For this task, we start by working with Harry \\nPotter and the Sorcerer’s Stone . I chose this book since the context should \\nprovide some fairly notable results with respect to the topics that are \\ncontained within the generated text.\\nLet’s go through the steps that we’ve become accustomed to. We will \\nutilize the load_data()  preprocessing function that we used in word_\\nembeddings.py ; however, the only change that we will make is loading \\nharry_potter.pdf  instead of economics_textbook.pdf .\\nThat said, this function allows you to easily utilize the preprocessing \\nfunction for whatever purpose, so long as the directory and other \\narguments are changed. Being that this is a text generation example, we \\nshould not clean the data beyond removing non-ASCII characters.\\nThe following is an example of how the data appears:\\n\"Harry Potter Sorcerer\\'s Stone CHAPTER ONE THE BOY WHO LIVED \\nMr. Mrs. Dursley, number four, Privet Drive, proud say \\nperfectly normal, thank much. They last people \\'d expect \\ninvolved anything strange mysterious, n\\'t hold nonsense. Mr. \\nDursley director firm called Grunnings, made drills. He big, \\nbeefy man hardly neck, although large mustache. Mrs. Dursley \\nthin blonde nearly twice usual amount neck, came useful spent \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n123much time craning garden fences, spying neighbors. The Dursleys \\nsmall son called Dudley opinion finer boy anywhere. The \\nDursleys everything wanted, also secret, greatest fear somebody \\nwould discover. They think could bear anyone found Potters. \\nMrs. Potter Mrs. Dursley\\'s sister, n\\'t met several years; fact, \\nMrs. Dursley pretended n\\'t sister, sister good-for-nothing \\nhusband unDursleyish possible. The Dursleys shuddered think \\nneighbors would say Potters arrived street. The Dursleys knew \\nPotters small son,, never even seen. This boy another good \\nreason keeping Potters away; n\\'t want Dudley mixing child like. \\nWhen Mr. Mrs. Dursley woke dull, gray Tuesday story starts, \\nnothing cloudy sky outside suggest strange mysterious things \\nwould soon happening country. Mr. Dursley hummed picked boring \\ntie work, Mrs. Dursley gossiped away happily wrestled screaming \\nDudley high chair. None noticed large, tawny owl flutter past \\nwindow. At half past eight, Mr. Dursley picked briefcase, \\npecked Mrs. Dursley cheek, tried kiss Dudley good-bye missed, 1 \\nDudley tantrum throwing cereal walls. `` Little tyke, \"chortled \\nMr. Dursley left house. He got car backed number four\\'s drive. \\nIt corner street noticed first sign something peculiar -- cat \\nreading map. For second, Mr. Dursley n\\'t realize seen -- jerked \\nhead around look. There tabby cat standing corner Privet Drive, \\nn\\'t map sight. What could thinking ? It must trick light. Mr. \\nDursley blinked stared cat. It stared back. As Mr. Dursley \\ndrove around corner road, watched cat mirror. It reading sign \\nsaid Privet Drive --, looking sign; cats...\"\\nLet’s inspect our preprocessing function.\\ndef preprocess_data(sequence_length=sequence_length, max_\\npages=max_pages, pdf_file=pdf_file):\\n    text_data = load_data(max_pages=max_pages, pdf_file=pdf_file)\\n    characters = list(set(text_data.lower()))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n124     character_dict = dict((character, i) for i, character in \\nenumerate(characters))\\n     int_dictionary = dict((i, character) for i, character in \\nenumerate(characters))\\n    num_chars, vocab_size = len(text_data), len(characters)\\n    x, y = [], []\\n    for i in range(0, num_chars  - sequence_length, 1):\\n        input_sequence = text_data[i: i+sequence_length]\\n        output_sequence = text_data[i+sequence_length]\\n         x.append([character_dict[character.lower()] for \\ncharacter in input_sequence])\\n        y.append(character_dict[output_sequence.lower()])\\n    \\n    for k in range(0, len(x)): x[i] = [_x for _x in x[i]]\\n    x = np.reshape(x, (len(x), sequence_length, 1))\\n    x, y = x/float(vocab_size), np_utils.to_categorical(y)\\n     return x, y, num_chars, vocab_size, character_dict,  \\nint_dictionary\\nWhen inspecting the function, we use methods similar to the tf_\\npreprocess_data()  function in the toy example of a Skip-Gram model. \\nOur input and output sequences are fixed lengths, and we will transform \\nthe y variable to a one-hot encoded vector, where each entry in the vector \\nrepresents a possible character. We represent the sequence of characters \\nas a matrix, where each row represents the entire observation and each \\ncolumn represents a character.\\nLet’s look at the first example of Keras code used in the book.\\n    def create_rnn(num_units=num_units, activation=activation):\\n        model = Sequential()\\n         model.add(LSTM(num_units, activation=activation,  \\ninput_shape=(None, x.shape[1])))\\n        model.add(Dense(y.shape[1], activation=\\'softmax\\'))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n125         model.compile(loss=\\'categorical_crossentropy\\', \\noptimizer=\\'adam\\')\\n        model.summary()\\n        return model\\nKeras, unlike TensorFlow, is considerably less verbose. As such, this \\nmakes changing the architecture of a model relatively easy. We instantiate \\na model by assigning it to a variable, and then simply add layer types with \\nthe Sequential().add()  function.\\nAfter running the network with 200 epochs, we get the following result:\\n  driv, proud say perfecdly normal, thanp much. they last \\npeople \\'d expect involved anytsing strange mysterious, s\\'t \\nhold donsense. mr. dursley director firm called grunnings, \\nmade drills. he big, berfy man, ardly neck, althougl larte \\nmustache. mrs. dursley thic -londe. early twece uiual amount \\nnecd, came ueeful spent much time craning geddon fences, \\nspying neighbors. the dursleys small son called dudley \\nopinion finer boy anyw   rd. the dursleys everything wanted, \\nslso secret, greatest fear somebody would discover. they \\nthinn could bear antone found potters. mrs. potterimrs. \\ndursley\\'s sister, n\\'t met several years; fact, mrs. dursley \\npretended n\\'t sister, sister good-sur-notding husband \\nundursleyir   pousible. the dursleys suuddered think auigybors \\nwould say potters arrived strett. the dursleys knew potters \\nsmall. on,   ever even seen. thit boy another good reason \\nkeeping potters away; n\\'e want dudley mixing child like. \\nwten mr. mrs. dursley woke dull, gray tuesday story startss, \\nnothing cloudy skycoutside suggest strange mytter ous taings \\nwould soon darpening codntry. mr. dursley tummed picked \\nboring tie work, mrs. dursley gosudaed away happily wrestled \\nscreaming dudley aigh cuair. noneoloticed large, tawny owl \\nflutter past wincow. at ialf past, ight, mr. dursley picked \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n126briefcase, pecked mrs. dursley cheek, tried kiss dudley good-\\nbye missed, 1 dudley tantrum,hrowigg cereal walls. `` lwttle \\ntykp, \"chortled mr. dursley left house. he got car backel \\nnumber four\\'s drive. it corner street noticed fir t sign \\nsomathing pcculilr -- cat feading,ap. for sicond, mr. dursley \\nr\\'t realize scen -- jerked head around look. thereytab y \\ncat standing corneraprivet drive, n\\'tamap sight. what sould \\nthinking ? it muse trick light. mr. dursley blinked stared \\ncat. it stayed back. as mr. dursley drove around corner road, \\nwatched catcmirror. it reading sign saidsprivet druve --, \\nlookingtsign; cats could n\\'t read maps signs. mr. durs\\nNote  Some of the text is interpretable, but obviously not ever ything \\nis as good as it could be. In this instance, I suggest that you allow the \\nneural network to train longer and to add more data. Also consider \\nusing different models and model architectures. Beyond this example, \\nit would be useful to present a more advanced version of the LSTM \\nthat is also useful for speech modeling.\\n Bidirectional RNNs (BRNN)\\nBRNNs were created in 1997 by Mike Schuster and Kukdip Paliwal, who \\nintroduced the technique to a signal-processing academic journal. The \\npurpose of the model was to utilize information moving in both a “positive \\nand negative time direction. ” Specifically, they wanted to utilize both \\nthe information moving up to the prediction, as well as the same stream \\nof inputs moving in the opposite direction. Figure\\xa0 5-1 illustrates the \\narchitecture of a BRNN.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n127Let’s imagine that that we have a sequence of words, such as the \\nfollowing: The man walks down the boardwalk.\\nIn a regular RNN, assuming that we wanted to predict the word \\nboardwalk , the input data would be The , man , walks , down , the. If we \\ninput bigrams, it would be The , man , man , walks , and so forth. We keep \\nmoving through the input data, predicting the word that is most likely \\nto come next at each time step, culminating in our final target label, a \\nprobability that corresponds to the one-hot encoded vector that is most \\nlikely to be present given the input data. The only difference in a BRNN \\nis that while we are predicting the sequence left-to-right, we also are \\npredicting the sequence right-to-left.\\nBRNNs have been particularly useful for NLP tasks. The following is \\nthe code for building a BRNN:\\ndef create_lstm(input_shape=(1, x.shape[1])):\\n        model = Sequential()\\n        model.add(Bidirectional(LSTM(unites=n_units,\\n                                     activation=activation),\\n                                     input_shape=input_shape))\\n        model.add(Dense(train_y.shape[1]), activation=out_act)\\n         model.compile(loss=\\'categorical_crossentropy\\', \\nmetrics=[\\'accuracy\\'])\\n        return model...AA AA\\nX0 X1 X2 Xis 0\\nsiy0 y1 y2 yi\\ns0\\nFigure 5-1.  Bidirectional RNN\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n128The structure of the bidirectional RNN is nearly identical in that we are \\nonly adding a Bidirectional()  cast over our layer. This often increases \\nthe time it takes to train neural networks, but in general, it outperforms \\ntraditional RNN architectures in many tasks. With this in mind, let’s apply \\nour model.\\n Creating a\\xa0Name Entity Recognition Tagger\\nPeople who have worked with NLTK or similar packages have likely come \\nacross the name entity recognition  (NER) tagger. NER taggers typically \\noutput a label that identifies the entity within larger categories (person, \\norganization, location, etc.). Creating an NER tagger requires a large \\namount of annotated data.\\nFor this task, we will use a data set from Kaggle. When we unzip the \\ndata, we see that it comes in the following format:\\n played    on    Monday    (    home    team  in    CAPS )    :\\nVBD        IN    NNP       (    NN      NN    IN    NNP  )    :\\nO          O     O         O    O       O     O     O    0    O\\nAmerican    League\\nNNP        NNP\\nB-MISC     I-MISC\\nCleveland   2     DETROIT   1\\nNNP        CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nBALTIMORE   12    Oakland   11   (       10    innings         )\\nVB         CD    NNP       CD   (       CD    NN         )\\nB-ORG      O     B-ORG     O    O       O     O     O\\nTORONTO    5     Minnesota  3\\nTO         CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nMilwaukee   3     CHICAGO   2\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n129NNP        CD    NNP       CD\\nB-ORG      O     B-ORG     O\\nBoston     4     CALIFORNIA 1\\nThe data is tab-delimited but also in .txt  format. This requires some \\ndata wrangling before we get to training the BRNN.\\nLet’s start by turning the text data into an interpretable format, as \\nfollows:\\ndef load_data():\\n     text_data = open(\\'/Users/tawehbeysolow/Downloads/train.\\ntxt\\', \\'rb\\').readlines()\\n     text_data = [text_data[k].replace(\\'\\\\t\\', \\' \\').split() for k \\nin range(0, len(text_data))]\\n    index = range(0, len(text_data), 3)\\n    #Transforming data to matrix format for neural network\\n    input_data =   list()\\n    for i in range(1, len(index)-1):\\n        rows = text_data[index[i-1]:index[i]]\\n         sentence_no = np.array([i for i in np.repeat(i, \\nlen(rows[0]))], dtype=str)\\n        rows.append(np.array(sentence_no))\\n        rows = np.array(rows).T\\n        input_data.append(rows)\\nWe must first iterate through each line of the .txt  file. Notice that \\nthe data is organized in groups of three. A typical grouping looks like the \\nfollowing:\\ntext_data[0]\\n[\\'played\\', \\'on\\', \\'Monday\\', \\'(\\', \\'home\\', \\'team\\', \\'in\\', \\'CAPS\\', \\n\\')\\', \\':\\']\\n text_data[1]\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n130[\\'VBD\\', \\'IN\\', \\'NNP\\', \\'(\\', \\'NN\\', \\'NN\\', \\'IN\\', \\'NNP\\', \\')\\', \\':\\']\\ntext_data[2]\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']\\nThe first set of observations contains the text itself, the second set of \\nobservations contains the name entity tag, and the final set contains the \\nspecific tag. Back to the preprocessing function, we take the groupings of \\nsentences and append an array that contains a sentence number label, \\nwhich I will discuss the importance of shortly.\\nWhen looking at a snapshot of the input_data  variable, we see the \\nfollowing:\\ninput_data[0:1]\\n[array([[\\'played\\', \\'VBD\\', \\'O\\', \\'1\\'],\\n       [\\'on\\', \\'IN\\', \\'O\\', \\'1\\'],\\n       [\\'Monday\\', \\'NNP\\', \\'O\\', \\'1\\'],\\n       [\\'(\\', \\'(\\', \\'O\\', \\'1\\'],\\n       [\\'home\\', \\'NN\\', \\'O\\', \\'1\\'],\\n       [\\'team\\', \\'NN\\', \\'O\\', \\'1\\'],\\n       [\\'in\\', \\'IN\\', \\'O\\', \\'1\\'],\\n       [\\'CAPS\\', \\'NNP\\', \\'O\\', \\'1\\'],\\n       [\\')\\', \\')\\', \\'O\\', \\'1\\'],\\n       [\\':\\', \\':\\', \\'O\\', \\'1\\']], dtype=\\'|S6\\')]\\nWe need to remove the sentence label while observing the data in \\nsuch a fashion that the neural network implicitly understands how these \\nsentences are grouped. The reason we want to remove this label is that \\nneural networks read categorical labels (which the sentence number is \\nan analog for) in such a way that higher-numbered sentences explicitly \\nhave a greater importance than lower-numbered sentences. For this task, I \\nassume most you understand we do not want to bake this into the training \\nprocess. As such, we move to the following body of code:\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n131    input_data = pan.DataFrame(np.concatenate([input_data[j] for \\nj in range(0,len(input_data))]),\\n                        columns=[\\'word\\', \\'pos\\', \\'tag\\', \\'sent_no\\'])\\n    labels, vocabulary = list(set(input_data[\\'tag\\'].values)), \\nlist(set(input_data[\\'word\\'].values))\\n    vocabulary.append(\\'endpad\\'); vocab_size = len(vocabulary); \\nlabel_size = len(labels)\\n  aggregate_function = lambda input: [(word, pos, label) for \\nword, pos, label in zip(input[\\'word\\'].values.tolist(),\\n   input[\\'pos\\'].values.tolist(),\\n   input[\\'tag\\'].values.tolist())]\\nWe organize the input_data  into a data frame, and then create a \\ncouple of other variables that we will use in the function later, as well as \\nthe train_brnn_keras()  function. Some of these variables are familiar to \\nothers present in the scripts from the prior chapter ( vocab_size  represents \\nthe number of words in the vocabulary, for example). However, the \\nimportant parts are mainly the last two variables, which is what you should \\nfocus on to solve this problem.\\nThe lambda function, aggregate_function , takes a data frame as \\nan input, and then returns a three-tuple for each observation within a \\ngrouping. This is precisely how we will group all the observations within \\none sentence. A snapshot of our data after this transformation yields the \\nfollowing:\\n sentences[0]\\n[(\\'played\\', \\'VBD\\', \\'O\\'), (\\'on\\', \\'IN\\', \\'O\\'), (\\'Monday\\', \\'NNP\\', \\n\\'O\\'), (\\'(\\', \\'(\\', \\'O\\'), (\\'home\\', \\'NN\\', \\'O\\'), (\\'team\\', \\'NN\\', \\n\\'O\\'), (\\'in\\', \\'IN\\', \\'O\\'), (\\'CAPS\\', \\'NNP\\', \\'O\\'), (\\')\\', \\')\\', \\'O\\'), \\n(\\':\\', \\':\\', \\'O\\')]\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n132We have nearly finished all the necessary preprocessing; however, \\nthere is a key step that you should be aware of.\\n     x = [[word_dictionary[word[0]] for word in sent] for sent \\nin sentences]\\n     x = pad_sequences(maxlen=input_shape, sequences=x, \\npadding=\\'post\\', value=0)\\n     y = [[label_dictionary[word[2]] for word in sent] for sent \\nin sentences]\\n     y = pad_sequences(maxlen=input_shape, sequences=y, \\npadding=\\'post\\', value=0)\\n      = [np_utils.to_categorical(label, num_classes=label_size) \\nfor label in y]\\nIn the preceding lines of code, we are transforming our words to their \\ninteger labels as we did in many other examples, and creating a one-  \\nhot encoded matrix. This is similar to the previous chapter, however, we \\nshould specifically not use the pad_sequences()  function.\\nWhen working with sentence data, we do not always get sentences of \\nequal length; however, the input matrix for the neural network has to have \\nan equal number of features across all observations. Zero padding  is used \\nto add the extra features that normalize the size of all observations.\\nWith this step done, we are now ready to move to training our neural \\nnetwork. Our model is as follows:\\ndef create_brnn():\\n        model = Sequential()\\n         model.add(Embedding(input_dim=vocab_size+1,  \\noutput_dim=output_dim,\\n                             input_length=input_shape,  \\nmask_zero=True))\\n         model.add(Bidirectional(LSTM(units=n_units, \\nactivation=activation,\\n                                     return_sequences=True)))\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n133         model.add(TimeDistributed(Dense(label_size, \\nactivation=out_act)))\\n         model.compile(optimizer=\\'adam\\', loss=\\'categorical_\\ncrossentropy\\', metrics=[\\'accuracy\\'])\\n        model.summary()\\n        return model\\nMost of our model is similar to the prior Keras models built in this \\nchapter; however, we have an embedding layer (analogous to a word \\nembedding) that is stacked on top of the bidirectional LSTM, which is \\nsubsequently staked on top of a fully connected output layer.\\nWe train our network on roughly 90% of the data we have, and then \\nsubsequently evaluate the results. We find that our tagger on the training \\ndata yields an accuracy of 90% and higher, depending on the number of \\nepochs we train it for.\\nNow that we have dealt with this classification task and sufficiently \\nworked with BRNNs, let’s move on to another neural network model and \\ndiscuss how it can be effectively applied to another NLP task.\\n Sequence-to-Sequence Models (Seq2Seq)\\nSequence-to-sequence models (seq2seq) are notable because they take \\nin an input sequence and return an output sequence, both of variable \\nlength. This makes this model particularly powerful, and it is predisposed \\nto perform well on language modeling tasks. The particular model that \\nwe will utilize is best summarized in a paper by Sutskever et\\xa0al. Figure\\xa0 5-2 \\nillustrates the model.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n134The model is generally comprised of two parts: an encoder and a \\ndecoder. Both the encoder and the decoder are RNNs. The encoder reads \\nthe input sequence and outputs a fixed-length vector in addition to the \\nhidden and cell states from the LSTM unit. Subsequently, the decoder \\ntakes this fixed-length vector, in addition to the output hidden and cell \\nstates, and uses them as inputs to the first of its LSTM units. The decoder \\noutputs a fixed-length vector, which we will evaluate as a target label. We \\nwill perform prediction one character at a time, which easily allows us to \\nevaluate sequences of varying length from one observation to the next. \\nNext, you see this model in action.\\n Question and\\xa0Answer with\\xa0Neural Network \\nModels\\nOne popular application of deep learning to NLP is the chatbot. Many \\ncompanies use chatbots to handle generic customer service requests, \\nwhich require them to be flexible in translating questions into answers. \\nWhile the test case that we look at is a microcosm of questions and howa re you ?Ia m goodDECODER ENCODER\\n<GO>\\nEmbedding\\nFigure 5-2.  Encoder-decoder model\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n135answers, it is an example of how we can train a neural network to properly \\nanswer a question. We will use the Stanford Question Answering Dataset. \\nAlthough it is more representative of general knowledge, you would do \\nwell to recognize the way in which these problems are structured.\\nLet’s begin by examining how we will preprocess the data by utilizing \\nthe following function:\\n     dataset = json.load(open(\\'/Users/tawehbeysolow/Downloads/\\nqadataset.json\\', \\'rb\\'))[\\'data\\']\\n    questions, answers = [], []\\n    for j in range(0, len(dataset)):\\n        for k in range(0, len(dataset[j])):\\n             for i in range(0, len(dataset[j][\\'paragraphs\\'][k]\\n[\\'qas\\'])):\\n                 questions.append(remove_non_ascii(dataset[j]\\n[\\'paragraphs\\'][k][\\'qas\\'][i][\\'question\\']))\\n               answers.append(remove_non_ascii(dataset[j]\\n[\\'paragraphs\\'][k][\\'qas\\'][i][\\'answers\\'][0]\\n[\\'text\\']))\\nWhen we look at a snapshot of the data, we observe the following structure:\\n[{u\\'paragraphs\\': [{u\\'qas\\': [{u\\'question\\': u\\'To whom did the \\nVirgin Mary allegedly appear in 1858  in Lourdes France?\\', \\nu\\'id\\': u\\'5733be284776f41900661182\\', u\\'answers\\': [{u\\'text\\': \\nu\\'Saint Bernadette Soubirous\\', u\\'answer_start\\': 515}]}, \\n{u\\'question\\': u\\'What is in front of the Notre Dame Main \\nBuilding?\\', u\\'id\\': u\\'5733be284776f4190066117f\\', u\\'answers\\': \\n[{u\\'text\\': u\\'a copper statue of Christ\\', u\\'answer_start\\': \\n188}]}, {u\\'question\\': u\\'The Basilica of the Sacred heart \\nat Notre Dame is beside to which structure?\\', u\\'id\\': \\nu\\'5733be284776f41900661180\\', u\\'answers\\': [{u\\'text\\': u\\'the Main \\nBuilding\\', u\\'answer_start\\': 279}]}, {u\\'question\\': u\\'What is the \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n136Grotto at Notre Dame?\\', u\\'id\\': u\\'5733be284776f41900661181\\', \\nu\\'answers\\': [{u\\'text\\': u\\'a Marian place of prayer and \\nreflection\\', u\\'answer_start\\': 381}]}, {u\\'question\\': u\\'What \\nsits on top of the Main Building at Notre Dame?\\', u\\'id\\': \\nu\\'5733be284776f4190066117e\\', u\\'answers\\': [{u\\'text\\': u\\'a \\ngolden statue of the Virgin Mary\\', u\\'answer_start\\': 92}]}], \\nu\\'context\\': u\\'Architecturally, the school has a Catholic \\ncharacter. Atop the Main Building\\\\\\'s gold dome is a golden \\nstatue of the Virgin Mary. Immediately in front of the Main \\nBuilding and facing it, is a copper statue of Christ with arms \\nupraised with the legend \"Venite Ad Me Omnes\". Next to the \\nMain Building is the Basilica of the Sacred Heart. Immediately \\nbehind the basilica is the Grotto, a Marian place of prayer and \\nreflection. It is a replica of the grotto at Lourdes, France \\nwhere the Virgin Mary reputedly appeared to Saint Bernadette \\nSoubirous in 1858. At the end of the main drive (and in a \\ndirect line that connects through 3 statues and the Gold \\nDome), is a simple, modern stone statue of Mary.\\'}, {u\\'qas\\': \\n[{u\\'question\\': u\\'When did the Scholastic Magazine of Notre \\ndame begin publishing?\\', u\\'id\\': u\\'5733bf84d058e614000b61be\\', \\nu\\'answers\\'\\nWe have a JSON file with question and answers. Similar to the name \\nentity recognition task, we need to preprocess our data into a matrix \\nformat that we can input into a neural network. We must first collect the \\nquestions that correspond to the proper answers. Then we iterate through \\nthe JSON file, and append each of the questions and answers to the \\ncorresponding arrays.\\nNow let’s discuss how we are actually going to frame the problem for \\nthe neural network. Rather than have the neural network predict each \\nword, we are going to have the neural network predict each character given \\nan input sequence of characters. Since this is a multilabel classification \\nproblem, we will output a softmax probability for each element of the \\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n137output vector, and then choose the vector with the highest probability. \\nThis represents the character that is most likely to proceed given the prior \\ninput sequence.\\nAfter we have done this for the entire output sequence, we will \\nconcatenate this array of outputted characters so that we get a human-  \\nreadable message. As such, we move forward to the following part of the code:\\n    input_chars, output_chars = set(), set()\\n    for i in range(0, len(questions)):\\n        for char in questions[i]:\\n             if char not in input_chars: input_chars.add(char.\\nlower())\\n    for i in range(0, len(answers)):\\n        for char in answers[i]:\\n             if char not in output_chars:  output_chars.add(char.\\nlower())\\n     input_chars, output_chars = sorted(list(input_chars)), \\nsorted(list(output_chars))\\n     n_encoder_tokens, n_decoder_tokens = len(input_chars), \\nlen(output_chars)\\nWe iterated through each of the questions and answers, and collected \\nall the unique individual characters in both the output and input \\nsequences. This yields the following sets, which represent the input and \\noutput characters, respectively.\\ninput_chars; output_chars\\n[u\\' \\', u\\'\"\\', u\\'#\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\',\\', u\\'-\\', \\nu\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', u\\'6\\', u\\'7\\', \\nu\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'>\\', u\\'?\\', u\\'_\\', u\\'a\\', u\\'b\\', u\\'c\\', \\nu\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', u\\'l\\', u\\'m\\', \\nu\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', u\\'v\\', u\\'w\\', \\nu\\'x\\', u\\'y\\', u\\'z\\']\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n138[u\\' \\', u\\'!\\', u\\'\"\\', u\\'$\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\'+\\', \\nu\\',\\', u\\'-\\', u\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', \\nu\\'6\\', u\\'7\\', u\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'?\\', u\\'[\\', u\\']\\', u\\'a\\', \\nu\\'b\\', u\\'c\\', u\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', \\nu\\'l\\', u\\'m\\', u\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', \\nu\\'v\\', u\\'w\\', u\\'x\\', u\\'y\\', u\\'z\\']\\nThe two lists contain 53 and 55 characters, respectively; however, they \\nare virtually homogenous and contain all the letters of the alphabet, plus \\nsome grammatical and numerical characters.\\nWe move to the most important part of the preprocessing, in which \\nwe transform our input sequences to one-hot encoded vectors that are \\ninterpretable by the neural network.\\n(code redacted, please see github)\\n     x_encoder = np.zeros((len(questions), max_encoder_len,  \\nn_encoder_tokens))\\n     x_decoder = np.zeros((len(questions), max_decoder_len,  \\nn_decoder_tokens))\\n     y_decoder = np.zeros((len(questions), max_decoder_len,  \\nn_decoder_tokens))\\n     for i, (input, output) in enumerate(zip(questions, \\nanswers)):\\n        for _character, character in enumerate(input):\\n             x_encoder[i, _character, input_\\ndictionary[character.lower()]] = 1.\\n        for _character, character in enumerate(output):\\n             x_decoder[i, _character, output_\\ndictionary[character.lower()]] = 1.\\n             if i > 0: y_decoder[i, _character,  \\noutput_dictionary[character.lower()]] = 1.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n139We start by instantiating two input vectors and an output vector, \\ndenoted by x_encoder , x_decoder , and y_encoder . Sequentially, this \\nrepresents the order in which the data passes through the neural network \\nand validated against the target label. While the one-hot encoding that \\nwe chose to create here is similar, we make a minor change by creating a \\nthree-dimensional array to evaluate each question and answer. Each row \\nrepresents a question, each time step represents a character, and each \\ncolumn represents the type of character within our set of characters. We \\nrepeat this process for each question-and-answer pair until we have an \\narray with the entire data set, which yields 4980 observations of data.\\nThe last step defines the model, as given by the encoder_decoder()  \\nfunction.\\ndef encoder_decoder(n_encoder_tokens, n_decoder_tokens):\\n    encoder_input = Input(shape=(None, n_encoder_tokens))\\n    encoder = LSTM(n_units, return_state=True)\\n     encoder_output, hidden_state, cell_state = encoder(encoder_\\ninput)\\n    encoder_states = [hidden_state, cell_state]\\n    decoder_input = Input(shape=(None, n_decoder_tokens))\\n     decoder = LSTM(n_units, return_state=True,  \\nreturn_sequences=True)\\n     decoder_output, _, _ = decoder(decoder_input,  \\ninitial_state=encoder_states)\\n     decoder = Dense(n_decoder_tokens, activation=\\'softmax\\')\\n(decoder_output)\\n    model = Model([encoder_input, decoder_input], decoder)\\n     model.compile(optimizer=\\'adam\\', loss=\\'categorical_\\ncrossentropy\\',   metrics=[\\'accuracy\\'])\\n    model.summary()\\n    return model\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n140We instantiated our model slightly differently than other Keras models. \\nThis method of creating a model is done through using the Functional \\nAPI, rather than relying on the sequential model, as we have often done. \\nSpecifically, this method is useful when creating more complex models, \\nsuch as seq2seq models, and is relatively straightforward once you have \\nlearned how to use the sequential model. Rather than adding layers to \\nthe sequential model, we instantiate different layers as variables and \\nthen pass the data by calling the tensor we created. We see this when \\nobserving the encoder_output  variable when we instantiate it by calling \\nencoder(encoder_input). We keep doing this through the encoder-decoder \\nphase until we reach an output vector, which we define as a dense/fully \\nconnected layer  with a softmax activation function.\\nFinally, we move to training, and observe the following results:\\nModel Prediction: saint bernadette soubiroust\\nActual Output: saint bernadette soubirous\\nModel Prediction: a copper statue of christ\\nActual Output: a copper statue of christ\\nModel Prediction: the main building\\nActual Output: the main building\\nModel Prediction: a marian place of prayer and reflection\\nActual Output: a marian place of prayer and reflection\\nModel Prediction: a golden statue of the virgin mary\\nActual Output: a golden statue of the virgin mary\\nModel Prediction: september 18760\\nActual Output: september 1876\\nModel Prediction: twice\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n141Actual Output: twice\\nModel Prediction: the observer\\nActual Output: the observer\\nModel Prediction: three\\nActual Output: three\\nModel Prediction: 19877\\nActual Output: 1987\\nAs you can see, this model performs considerably well, with only three \\nepochs. Although there are some problems with the spelling from adding \\nextra characters, the messages themselves are correct in most instances. \\nFeel free to keep experimenting with this problem, particularly by altering \\nthe model architecture to see if there is one that yields better accuracy.\\n Summary\\nWith the chapter coming to a close, we should review the concepts that are \\nmost important in helping us successfully train our algorithms. Primarily, \\nyou should take note of the model types that are appropriate for different \\nproblems. The encoder-decoder model architecture introduces the “many-  \\nto- many” input-output scheme and shows where it is appropriate to apply it.\\nSecondarily, you should take note of where preprocessing techniques \\ncan be applied to seemingly different but related problems. The translation \\nof data from one language to another uses the same preprocessing steps \\nas creating a neural network that answered questions based on different \\nresponses. Paying attention to these modeling steps and how they relate \\nto the underlying structure of the data can save you time on seemingly \\ninnocuous tasks.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n142 Conclusion and\\xa0Final Statements\\nWe have reached the end of this book. We solved a wide array of NLP \\nproblems of varying complexities and domains. There are many \\nconcepts that are constant across all problem types, most specifically \\ndata preprocessing. The vast majority of what makes machine learning \\ndifficult is preprocessing data. You saw that similar problem types share \\npreprocessing steps, as we often reused parts of solutions as we moved to \\nmore advanced problems.\\nThere are some final principles that are worth remembering from \\nthis point forward. NLP with deep learning can require large amounts of \\ntext data. Collect it carefully and responsibly, and consider your options \\nwhen dealing with large data sets with respect to choice of language for \\noptimized run time (C/C++ vs. Python, etc.).\\nNeural networks, by and large, are fairly straightforward models to \\nwork with. The difficulty is finding good data that has predictive power, in \\naddition to structuring it in such a way that our neural network can find \\npatterns to exploit.\\nStudy carefully the preprocessing steps to take for document \\nclassification, creating a word embedding, or creating an NER tagger, for \\nexample. Each of these represents feature extraction schemes that can be \\napplied to different problems and illuminate a path forward during your \\nresearch.\\nAlthough intelligent preprocessing of data spoken about fairly often \\nin the machine learning community, it is particularly true of the NLP \\nparadigm of deep learning and data science. The models that we have \\ntrained give you a roadmap on how to work with similar data sets in \\nprofessional or academic environments. However, this does not mean that \\nthe models we have deployed could be used in production and work well.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n143There are a considerable number of variables that I did not discuss, \\nbeing that they are problems of maintaining production systems rather \\nthan the theory behind a model. Examples include unknown words \\nin vocabularies that appear over time, when to retrain models, how to \\nevaluate multiple models’ outputs simultaneously, and so forth.\\nIn my experience, finding out when to retrain models has best been \\nsolved by collecting large amounts of live performance data. See when \\nsignals deprecate, if they do at all, and track the effect of retraining, as well \\nas the persistence in retraining of models. Even if your model is accurate, it \\ndoes not mean that it will be easy to use in practice.\\nThink carefully about how to handle false classifications, particularly \\nif the penalty for misclassification could cause the loss of money and/or \\nother resources. Do not be afraid to utilize multiple models for multiple \\nproblem types. When experimenting, start simple and gradually add \\ncomplexity as needed. This is significantly easier than trying to design \\nsomething very complex in the beginning and then trying to debug a \\nsystem that you do not understand.\\nYou are encouraged to reread this book at your leisure, as well as for \\nreference, in addition to utilizing the code on my GitHub page to tackle the \\nproblems in their own unique fashion. While reading this book provides \\na start, the only way to become proficient in data science is to practice the \\nproblems on your own.\\nI hope you have enjoyed learning about natural language processing \\nand deep learning as much as I have enjoyed explaining it.\\nCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  \\nLANGUAGE MODELING TASKS\\n\\n145© Taweh Beysolow II 2018 \\nT . Beysolow II, Applied Natural Language Processing with Python ,  \\nhttps://doi.org/10.1007/978-1-4842-3733-5Index\\nA, B\\nBackpropagation through time \\n(BPTT), 36\\nBag-of-words (BoW) model\\nadvantages, 73\\nCountVectorizer, 51–52\\ndefinition, 50\\ndisadvantages, 74\\nfeature extraction algorithm, 50\\nmachine learning algorithm, 50\\nmovie reviews ( see IMDB  \\nmovie review data set)\\nscikit-learn library, 51\\nspam detection\\naccuracy and  \\nAUC scores, 55–56\\nCountVectorizer(), 54\\ndata set, 54\\nfit() method, 55\\ninbox, 53\\nload_spam_data(), 54\\nlogistic regression, 54–56\\nnp.random.seed()  \\nfunction, 56\\nridge regression, 55\\nROC curve, 57\\nSMS message length \\nhistogram, 53text_classifiction_demo.py \\nfile, 54\\nunwanted advertisements/\\nmalicious files, 53\\nTFIDF , 57\\nBidirectional RNNs  \\n(BRNNs), 126–128,  133\\nC\\nContinuous bag-of-words (CBoW) \\nmodel, 103–105\\nD, E, F\\nDeep learning\\napplications\\nlanguage modeling  \\ntasks, 11\\nNLP techniques and \\ndocument  \\nclassification, 10\\nRNNs, 11\\ntopic modeling, 10\\nword embeddings, 10–11\\nKeras, 7–8\\nmodels, 4\\nTensorFlow, 4–7\\nTheano, 8–9\\n\\n146G, H\\nGlobal Vectors for Word \\nRepresentation (GloVe)\\nco-occurrence, 106–107\\ncosine similarities, 110\\ndescription, 106\\nerror function, 107\\nGitHub repository, 108\\nmatrix-based factorization \\nmethod, 106\\nopen() function, 109\\npretrained embeddings, 108–110\\nweighting function, 107–108\\nWikipedia, 109\\nI\\nIMDB movie review data set\\n” .join() function, 63\\nL1 and L2 norm visualization, 69\\nload data, 64\\nlogistic regression, 65–66\\nmachine learning packages, 62\\nmin_df and max_df, 65\\nmlp_movie_classification_\\nmodel.py file, 68\\nopen() function, 64\\nord() function, 63\\nos.listdir() function, 64\\npositive and negative rate, 73\\nremove_non_ascii() function, 64\\nROC curve\\nL1 and L2 logistic regression \\ntest set, 67–68multilayer perceptron, 70\\nnaïve Bayes classifier, 71–73\\nrandom forest, 71\\nTfidfVectorizer() method, 63\\ntrain_logistic_model()  \\nfunction, 65\\nJ\\nJupyter notebook, 89\\nK\\nKeras, 7–8\\nL\\nLatent Dirichlet allocation (LDA)\\nassumptions, 78\\nbeta distribution, 79\\njoint probability  \\ndistribution, 79\\nmovie review data\\ndocument classification, 81\\nfit_transform() method, 82\\nGensim, 84–85\\nscikit-learn  \\nimplementation, 86\\nsklearn_topic_model() \\nfunction, 85\\nTFIDF model, 82–84\\ntopics, 82–83\\nmultinomial distribution, 78\\nPoisson distribution, 78\\nprobability distribution, 79Index\\n\\n147TFIDF , 78\\ntopics and words simplexes, 80\\nLong short-term memory (LSTM)\\nBasicLSTMCell() function, 41\\nformulae, 38\\ngates, 39\\nplaceholder variables, 40–41\\nsigmoid activation  \\nfunction, 38–39\\ntanh activation function, 38\\nunits/blocks, 37–38\\nword embeddings\\ncomputation graph, 112\\n_embedding_array, 112\\nerror rate, 114\\nexecuting code, 114\\nload_data() function, 115\\npreprocessing steps, 111\\nremove_stop_words() \\nfunction, 112\\nreverse_dictionary, 113\\nsample data, 111\\nsample_text_dictionary() \\nfunction, 112\\ntf.nn.embedding_lookup() \\nfunction, 114\\ntraining data, 111\\n_weights and _embedding \\nvariables, 113\\nM\\nMean squared error (MSE), 29–30\\nModeling stock returnsLSTM, 40\\nMLPs, 15\\nRNNs, 32\\nMultilayer perceptron  \\nmodels (MLPs)\\ncross entropy, 30\\nerror function, 18–19\\nFord Motor Company (F), 15\\nlearning rate\\nactivation function, 16, 24–25\\nAdam optimization \\nalgorithm, 20–21\\nepochs, 22\\nfloating-point value, 20\\noptimal solution, 20\\nparameter, 20, 22\\nplaceholder variables, 23\\nReLU activation function, 26\\nsigmoid activation  \\nfunction, 24–25\\nTensorFlow, 22–23\\ntraining and test sets, 22\\nvanishing gradient, 26\\nweights, neural network, 24\\nMSE and RMSE loss  \\nfunction, 29–30\\nneural networks, 17\\nnormal distribution, 17\\nsentiment analysis, 30\\nSLPs, 13\\nstandard normal distribution, 14\\nTensorFlow, 15\\ntf.random_normal(), 17\\ntrain_data, 15Index\\n\\n148vanishing gradients and  \\nReLU, 27–28\\nvisualization, 14\\nweight and bias units, 17–18\\nN, O\\nName entity recognition (NER) \\ntagger\\naggregate_function, 131\\ncategories, 128\\ndata set, Kaggle, 128\\nembedding layer, 133\\nfeature extraction, 142\\ninput_data variable, 130–131\\ninteger labels, 132\\nneural network, 130,  132\\ntext data, 129\\ntrain_brnn_keras() function, 131\\nzero padding, 132\\nNatural language processing (NLP)\\nBayesian statistics, 3\\nbifurcation, 3\\ncomplexities and domains, 142\\ncomputational linguistics, 2\\ncomputing power, 3\\ndeep learning, Python ( see  \\nDeep learning)\\ndefinition, 1\\nformal language theory, 2\\nmachine learning concepts, 4\\nprinciples, 142\\nSLP , 2–3spell-check, sentences, 31\\nNatural Language Toolkit (NLTK) \\nmodule, 45–46\\nNatural language understanding \\n(NLU), 3\\nNeural networks\\ncharacters, 136–138\\nchatbots, 134\\ndense/fully connected layer, 140\\nencoder_decoder() function, \\n139–140\\nJSON file, 136\\nKeras models, 140\\none-hot encoded  \\nvectors, 138–139\\nseq2seq models, 140\\nStanford Question Answering \\nDataset, 135–136\\nNon-negative matrix factorization \\n(NMF)\\nfeatures, 87\\nGensim model, 90\\nJupyter notebook, 89–90\\nand LDA, 90\\nmathematical formula, 86\\nscikit-learn implementation, \\n87–88,  90\\ntopic extraction, 88\\nP,  Q\\nParagraph2Vec algorithm, 115\\nmovie review data, 116–118\\nPrincipal components analysis \\n(PCA), 97Multilayer perceptron models \\n(MLPs) ( cont. )Index\\n\\n149R\\nRecurrent neural networks (RNNs)\\nactivation function, 35\\nBPTT , 36\\nbuild_rnn() function, 32\\nchain rule, 36\\ndata set, 33, 35\\nfloating-point decimal, 35\\ngradient descent algorithm, 35\\nhidden state, 32, 33\\nLSTM ( see Long short-term \\nmemory (LSTM))\\nplaceholder variables, 34\\nsigmoid activation function, 37\\nstate_size, 32–33\\nstructure, 31–32\\ntanh activation and derivative \\nfunction, 36–37\\nTensorFlow, 32\\nvanishing gradient, 36–37\\nRoot mean squared error  \\n(RMSE), 29–30\\nS\\nSequence-to-sequence models \\n(seq2seq), 133–134\\nSigmoid activation  \\nfunction, 24–25\\nSingle-layer perceptron (SLP),  \\n2–3,  13\\nSkip-Gram model\\narchitecture, 92\\nk-skip-n-grams, 91negative sampling, 93\\nneural network, 93\\nn-gram, 91\\nobjective function, 91\\none-hot encoded vector, 92\\n2-skip-bi-gram model, 91\\ntraining words, 91\\nword embedding\\ncosine similarity, 98\\nGensim, 96–99\\nhidden layer weight  \\nmatrix, 93\\nindex number, 99\\nnegative sampling, 101\\nneural networks, 96\\none-hot encoding data, 100\\nPCA, 97\\nPDFMiner, 94\\nTensorFlow, 94, 101–102\\ntokenizing data, 95\\nvisualizing, 96–97\\nvocabulary size and word \\ndictionary, 100\\nT , U, V\\nTensorFlow, 4–7\\nTerm frequency–inverse document \\nfrequency (TFIDF), 57\\nText generation, LSTMs\\nAI-based tools, 122\\nBRNNs, 126–128\\ndata, 122\\nepochs, 125–126Index\\n\\n150Harry Potter and the Sorcerer’s \\nStone, 122\\nKeras code, 124\\nload_data(), 122\\npreprocessing function, 123–124\\nSequential().add()  \\nfunction, 125\\nSkip-Gram model, 124\\ntf_preprocess_data(), 124\\nTheano, 8–9\\nTokenization and stop words\\nBoolean variable, 47\\ndata set, 44\\nfeature extraction  \\nalgorithms, 48–49\\nfunction words, 46\\ngrammatical characters, 48\\nlowercase, 47\\nmistake() and advised_\\npreprocessing()  \\nfunctions, 47–48\\nNLTK module, 45–46\\nsample_sent_tokens, 46\\nsample text, 44sample_word_tokens, 45, 49\\nsingle string objects, 44\\nuppercase, 48\\nTopic models, 10\\ndescription, 77\\nLDA ( see Latent Dirichlet \\nallocation (LDA))\\nNMF ( see Non-negative matrix \\nfactorization (NMF))\\nWord2Vec, 90–93\\nW, X, Y\\nWord embeddings, 10–11\\nCBoW, 103–105\\nGloVe, 106–110\\nLSTM ( see Long short-term \\nmemory (LSTM))\\nParagraph2Vec, 115–118\\nSkip-Gram model ( see Skip-  \\nGram mo del)\\nZ\\nZero padding, 132Text generation, LSTMs ( cont. )Index'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applied Natural Language Processing with PythonImplementing Machine Learning  and Deep Learning Algorithms for  Natural Language Processing—Taweh Beysolow IIApplied Natural Language Processing with PythonImplementing Machine Learning and Deep Learning Algorithms for Natural Language ProcessingTaweh  Beysolow  IIApplied Natural Language Processing with PythonISBN-13 (pbk): 978-1-4842-3732-8     ISBN-13 (electronic): 978-1-4842-3733-5https://doi.org/10.1007/978-1-4842-3733-5Library of Congress Control Number: 2018956300Copyright © 2018 by Taweh Beysolow II This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.Trademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark. The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.Managing Director, Apress Media LLC: Welmoed SpahrAcquisitions Editor: Celestin Suresh JohnDevelopment Editor: Siddhi ChavanCoordinating Editor: Divya ModiCover designed by eStudioCalamarCover image designed by Freepik (www.freepik.com)Distributed to the book trade worldwide by Springer Science+Business Media New\\xa0York,  233 Spring Street, 6th Floor, New\\xa0York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505,  e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.For information on translations, please e-mail rights@apress.com, or visit http://www.apress.com/rights-permissions.Apress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at http://www.apress.com/bulk-sales.Any source code or other supplementary material referenced by the author in this book is available to readers on GitHub via the book’s product page, located at www.apress.com/ 978-1-4842-3732-8. For more detailed information, please visit http://www.apress.com/source-code.Printed on acid-free paperTaweh\\xa0Beysolow\\xa0IISan Francisco, California, USATo my family, friends, and colleagues for their continued support and encouragement to do more with myself than  I often can conceive of doingvTable of ContentsChapter 1 :  What Is Natural Language Pr ocessing?  ������������������������������ 1The History of  Natural Language Processing  �������������������������������������������������������� 2A Review of  Machine Learning and  Deep Lear ning ���������������������������������������������� 4NLP, Machine Learning, and  Deep Lear ning Packages with  Python  ���������������� 4Applications of  Deep Lear ning to  NLP ����������������������������������������������������������� 10Summar y������������������������������������������������������������������������������������������������������������� 12Chapter 2 :  Review of\\xa0Deep Learning ��������������������������������������������������� 13Multilayer Perceptrons and  Recurrent Neural Networks  ������������������������������������ 13Toy Example 1: Modeling Stock Returns with  the MLP Model  ����������������������� 15Vanishing Gradients and  Why ReLU Helps to  Prevent Them  �������������������������� 27Loss Functions and  Backpropagation  ������������������������������������������������������������ 29Recurr ent Neural Networks and  Long Short-T erm Memory  �������������������������� 30Toy Example 2: Modeling Stock Returns with  the RNN Model  ����������������������� 32Toy Example 3: Modeling Stock Returns with  the LSTM Model  ��������������������� 40Summary ������������������������������������������������������������������������������������������������������������� 41About the Author  ��������������������������������������������������������������������������������� ixAbout the Technical Review er ������������������������������������������������������������� xiAcknowledgments  ����������������������������������������������������������������������������� xiiiIntroduction  ���������������������������������������������������������������������������������������� xvviChapter 3 :  Working with\\xa0 Ra w Text  ���������������������������������������������������� 43Tokenization and  Stop Words  ������������������������������������������������������������������������������ 44The Bag-of-Wor ds Model (BoW)  �������������������������������������������������������������������������� 50CountV ectorizer  ��������������������������������������������������������������������������������������������� 51Example Problem 1:  Spam Detection  ������������������������������������������������������������ 53Term Frequency Inverse Document Frequency  ��������������������������������������������� 57Example Problem 2:  Classifying Movie Reviews  ������������������������������������������� 62Summar y������������������������������������������������������������������������������������������������������������� 74Chapter 4 :  Topic Modeling and\\xa0 Word Embeddings  ���������������������������� 77Topic Model and  Latent Dirichlet Allocation (LDA)  ���������������������������������������������� 77Topic Modeling with  LDA on  Movie Review Data  ������������������������������������������� 81Non-Negative Matrix Factorization (NMF)  ����������������������������������������������������������� 86Word2Vec  ������������������������������������������������������������������������������������������������������������ 90Example Problem 4 �2: Training a  Word Embedding (Skip-Gram)  ������������������� 94Continuous Bag-of-Wor ds (CBoW)  �������������������������������������������������������������������� 103Example Problem 4 �2: Training a  Word Embedding (CBoW)  ������������������������� 105Global Vectors for  Word Representation (GloVe)  ����������������������������������������������� 106Example Problem 4 �4: Using Trained Word Embeddings with  LSTMs  ���������� 111Paragraph2Vec: Distributed Memory of  Paragraph Vectors (PV-DM)  ���������������� 115Example Problem 4 �5: Paragraph2Vec Example with  Movie  Review Data  ������������������������������������������������������������������������������������������������ 116Summar y����������������������������������������������������������������������������������������������������������� 118Chapter 5 :  Text Generation, Machine Translation, and\\xa0Other  Recurr ent Language Modeling Tasks  ������������������������������ 121Text Generation with  LSTMs  ����������������������������������������������������������������������������� 122Bidirectional RNNs (BRNN)  �������������������������������������������������������������������������� 126Table of Con TenTs Table of Con TenTsviiCreating a  Name Entity Recognition Tagger  ������������������������������������������������������ 128Sequence-to-Sequence Models (Seq2Seq)  ������������������������������������������������������ 133Question and  Answer with  Neur al Network Models  ������������������������������������������ 134Summar y����������������������������������������������������������������������������������������������������������� 141Conclusion and  Final Statements  ��������������������������������������������������������������������� 142 Index  ������������������������������������������������������������������������������������������������� 145Table of Con TenTs Table of Con TenTsixAbout the AuthorTaweh\\xa0Beysolow  II\\xa0is a data scientist and author currently based in San Francisco, California. He has a bachelor’s degree in economics from St. Johns University and a master’s degree in applied statistics from Fordham University. His professional experience has included working at Booz Allen Hamilton, as a consultant and in various startups as a data scientist, specifically focusing on machine learning. He has applied machine learning to federal consulting, financial services, and agricultural sectors.xiAbout the Technical ReviewerSantanu\\xa0Pattanayak \\xa0currently works at GE Digital as a staff data scientist and is the author of the deep learning book  Pro Deep Learning with TensorFlow: A Mathematical Approach to Advanced Artificial Intelligence in Python  (Apress, 2017). He has more than eight years of experience in the data analytics/data science field and a background in development and database technologies. Prior to joining GE, Santanu worked at companies such as RBS, Capgemini, and IBM.\\xa0He graduated with a degree in electrical engineering from Jadavpur University, Kolkata, and is an avid math enthusiast. Santanu is currently pursuing a master’s degree in data science from the Indian Institute of Technology (IIT), Hyderabad. He also devotes his time to data science hackathons and Kaggle competitions, where he ranks within the top 500 across the globe. Santanu was born and brought up in West Bengal, India, and currently resides in Bangalore, India, with his wife.xiiiAcknowledgmentsA special thanks to Santanu Pattanayak, Divya Modi, Celestin Suresh John, and everyone at Apress for the wonderful experience. It has been a pleasure to work with you all on this text. I couldn’t have asked for a better team.xvIntroductionThank you for choosing Applied Natural Language Processing with Python  for your journey into natural language processing (NLP). Readers should be aware that this text should not be considered a comprehensive study of machine learning, deep learning, or computer programming. As such, it is assumed that you are familiar with these techniques to some degree. Regardless, a brief review of the concepts necessary to understand the tasks that you will perform in the book is provided.After the brief review, we begin by examining how to work with raw text data, slowly working our way through how to present data to machine learning and deep learning algorithms. After you are familiar with some basic preprocessing algorithms, we will make our way into some of the more advanced NLP tasks, such as training and working with trained word embeddings, spell-check, text generation, and question-and-answer generation.All of the examples utilize the Python programming language and popular deep learning and machine learning frameworks, such as scikit-  learn, Keras, and TensorFlow. Readers can feel free to access the source code utilized in this book on the corresponding GitHub page and/or try their own methods for solving the various problems tackled in this book with the datasets provided.1© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5_1CHAPTER 1What Is Natural Language Processing?Deep learning and machine learning continues to proliferate throughout various industries, and has revolutionized the topic that I wish to discuss in this book: natural language processing (NLP). NLP is a subfield of computer science that is focused on allowing computers to understand language in a “natural” way, as humans do. Typically, this would refer to tasks such as understanding the sentiment of text, speech recognition, and generating responses to questions.NLP has become a rapidly evolving field, and one whose applications have represented a large portion of artificial intelligence (AI) breakthroughs. Some examples of implementations using deep learning are chatbots that handle customer service requests, auto-spellcheck on cell phones, and AI assistants, such as Cortana and Siri, on smartphones. For those who have experience in machine learning and deep learning, natural language processing is one of the most exciting areas for individuals to apply their skills. To provide context for broader discussions, however, let’s discuss the development of natural language processing as a field.2 The History of\\xa0Natural Language ProcessingNatural language processing can be classified as a subset of the broader field of speech and language processing. Because of this, NLP shares similarities with parallel disciplines such as computational linguistics, which is concerned with modeling language using rule-based models. NLP’s inception can be traced back to the development of computer science in the 1940s, moving forward along with advances in linguistics that led to the construction of formal language theory. Briefly, formal language theory models language on increasingly complex structures and rules to these structures. For example, the alphabet is the simplest structure, in that it is a collection of letters that can form strings called words . A formal language is one that has a regular, context-free, and formal grammar. In addition to the development of computer sciences as a whole, artificial intelligence’s advancements also played a role in our continuing understanding of NLP .In some sense, the single-layer perceptron (SLP) is considered to be the inception of machine learning/AI.\\xa0Figure\\xa0 1-1 shows a photo of this model.The SLP was designed by neurophysiologist Warren McCulloch and logician Walter Pitt. It is the foundation of more advanced neural network models that are heavily utilized today, such as multilayer perceptrons.  Figure 1-1.  Single-layer perceptronChapter 1  What Is Natural laNguage proCessINg?3The SLP model is seen to be in part due to Alan Turing’s research in the late 1930s on computation, which inspired other scientists and researchers to develop different concepts, such as formal language theory.Moving forward to the second half of the twentieth century, NLP starts to bifurcate into two distinct groups of thought: (1) those who support a symbolic approach to language modelling, and (2) those who support a stochastic approach. The former group was populated largely by linguists who used simple algorithms to solve NLP problems, often utilizing pattern recognition. The latter group was primarily composed of statisticians and electrical engineers. Among the many approaches that were popular with the second group was Bayesian statistics. As the twentieth century progressed, NLP broadened as a field, including natural language understanding (NLU) to the problem space (allowing computers to react accurately to commands). For example, if someone spoke to a chatbot and asked it to “find food near me, ” the chatbot would use NLU to translate this sentence into tangible actions to yield a desirable outcome.Skip closer to the present day, and we find that NLP has experienced a surge of interest alongside machine learning’s explosion in usage over the past 20 years. Part of this is due to the fact that large repositories of labeled data sets have become more available, in addition to an increase in computing power. This increase in computing power is largely attributed to the development of GPUs; nonetheless, it has proven vital to AI’s development as a field. Accordingly, demand for materials to instruct data scientists and engineers on how to utilize various AI algorithms has increased, in part the reason for this book.Now that you are aware of the history of NLP as it relates to the present day, I will give a brief overview of what you should expect to learn. The focus, however, is primarily to discuss how deep learning has impacted NLP , and how to utilize deep learning and machine learning techniques to solve NLP problems.Chapter 1  What Is Natural laNguage proCessINg?4 A Review of\\xa0Machine Learning and\\xa0Deep LearningYou will be refreshed on important machine learning concepts, particularly deep learning models such as multilayer perceptrons  (MLPs), recurrent neural networks  (RNNs), and long short-term memory  (LSTM) networks. You will be shown in-depth models utilizing toy examples before you tackle any specific NLP problems. NLP , Machine Learning, and\\xa0Deep Learning Packages with\\xa0PythonEqually important as understanding NLP theory is the ability to apply it in a practical context. This book utilizes the Python programming language, as well as packages written in Python. Python has become the lingua franca for data scientists, and support of NLP , machine learning, and deep learning libraries is plentiful. I refer to many of these packages when solving the example problems and discussing general concepts.It is assumed that all readers of this book have a general understanding of Python, such that you have the ability to write software in this language. If you are not familiar with this language, but you are familiar with others, the concepts in this book will be portable with respect to the methodology used to solve problems, given the same data sets. Be that as it may, this book is not intended to instruct users on Python. Now, let’s discuss some of the technologies that are most important to understanding deep learning. TensorFlowOne of the groundbreaking releases in open source software, in addition to machine learning at large, has undoubtedly been Google’s TensorFlow. It is an open source library for deep learning that is a successor to Theano, a similar machine learning library. Both utilize data flow graphs for Chapter 1  What Is Natural laNguage proCessINg?5computational processes. Specifically, we can think of computations as dependent on specific individual operations. TensorFlow functionally operates by the user first defining a graph/model, which is then operated by a TensorFlow session that the user also creates.The reasoning behind using a data flow graph rather than another computational format computation is multifaceted, however one of the more simple benefits is the ability to port models from one language to another. Figure\\xa0 1-2 illustrates a data flow graph.For example, you may be working on a project where Java is the language that is most optimal for production software due to latency reasons (high-frequency trading, for example); however, you would like to utilize a neural network to make predictions in your production system. Rather than dealing with the time-consuming task of setting up a training framework in Java for TensorFlow graphs, something could be written in Python relatively quickly, and then the graph/model could be restored by loading the weights in the production system by utilizing Java. TensorFlow code is similar to Theano code, as follows.    #Creating weights and biases dictionaries     weights = {\\'input\\': tf.Variable(tf.random_normal([state_size+1, state_size])),biasesweightsinputstargetsMatMulAdd SoftmaxXentGraph of Nodes , also called operations (ops)Figure 1-2.  Data flow graph diagramChapter 1  What Is Natural laNguage proCessINg?6         \\'output\\': tf.Variable(tf.random_normal([state_size, n_classes]))}     biases = {\\'input\\': tf.Variable(tf.random_normal([1, state_size])),         \\'output\\': tf.Variable(tf.random_normal([1, n_classes]))}    #Defining placeholders and variables    X = tf.placeholder(tf.float32, [batch_size, bprop_len])    Y = tf.placeholder(tf.int32, [batch_size, bprop_len])     init_state = tf.placeholder(tf.float32, [batch_size, state_size])    input_series = tf.unstack(X, axis=1)    labels = tf.unstack(Y, axis=1)    current_state = init_state    hidden_states = []    #Passing values from one hidden state to the next     for input in input_series: #Evaluating each input within the series of inputs         input = tf.reshape(input, [batch_size, 1]) #Reshaping input into MxN tensor         input_state = tf.concat([input, current_state], axis=1) #Concatenating input and current state tensors         _hidden_state = tf.tanh(tf.add(tf.matmul(input_state, weights[\\'input\\']), biases[\\'input\\'])) #Tanh transformation         hidden_states.append(_hidden_state) #Appending the next state        current_state = _hidden_state #Updating the current stateTensorFlow is not always the easiest library to use, however, as there often serious gaps between documentation for toy examples vs.  real-  world examples that reasonably walk the reader through the complexity of implementing a deep learning model.Chapter 1  What Is Natural laNguage proCessINg?7In some ways, TensorFlow can be thought of as a language inside of Python, in that there are syntactical nuances that readers must become aware of before they can write applications seamlessly (if ever). These concerns, in some sense, were answered by Keras. KerasDue to the slow development process of applications in TensorFlow, Theano, and similar deep learning frameworks, Keras was developed for prototyping applications, but it is also utilized in production engineering for various problems. It is a wrapper for TensorFlow, Theano, MXNet, and DeepLearning4j. Unlike these frameworks, defining a computational graph is relatively easy, as shown in the following Keras demo code.def create_model():    model = Sequential()    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),                       input_shape=(None, 40, 40, 1),                       padding=\\'same\\', return_sequences=True))    model.add(BatchNormalization())    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),                       padding=\\'same\\', return_sequences=True))    model.add(BatchNormalization())        model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),                       padding=\\'same\\', return_sequences=True))    model.add(BatchNormalization())    model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),                       padding=\\'same\\', return_sequences=True))    model.add(BatchNormalization())    model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),Chapter 1  What Is Natural laNguage proCessINg?8                   activation=\\'sigmoid\\',                   padding=\\'same\\', data_format=\\'channels_last\\'))    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adadelta\\')    return modelAlthough having the added benefit of ease of use and speed with respect to implementing solutions, Keras has relative drawbacks when compared to TensorFlow. The broadest explanation is that Keras users have considerably less control over their computational graph than TensorFlow users. You work within the confines of a sandbox when using Keras. TensorFlow is better at natively supporting more complex operations, and providing access to the most cutting-edge implementations of various algorithms. TheanoAlthough it is not covered in this book, it is important in the progression of deep learning to discuss Theano. The library is similar to TensorFlow in that it provides developers with various computational functions (add, matrix multiplication, subtract, etc.) that are embedded in tensors when building deep learning and machine learning models. For example, the following is a sample Theano code.(code redacted please see github)X, Y = T.fmatrix(), T.vector(dtype=theano.config.floatX)    weights = init_weights(weight_shape)    biases = init_biases(bias_shape)    predicted_y = T.argmax(model(X, weights, biases), axis=1)    cost = T.mean(T.nnet.categorical_crossentropy(predicted_y, Y))    gradient = T.grad(cost=cost, wrt=weights)    update = [[weights, weights  - gradient * 0.05]]Chapter 1  What Is Natural laNguage proCessINg?9    train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)    predict = theano.function(inputs=[X], outputs=predicted_y, allow_input_downcast=True)    for i in range(0, 10):        print(predict(test_x_data[i:i+1]))if __name__ == \\'__main__\\':    model_predict()When looking at the functions defined in this sample, notice that T is the variable defined for a tensor, an important concept that you should be familiar with. Tensors can be thought of as objects that are similar to vectors; however, they are distinct in that they are often represented by arrays of numbers, or functions, which are governed by specific transformation rules unique unto themselves. Tensors can specifically be a single point or a collection of points in space-time (any function/model that combines x, y, and z axes plus a dimension of time), or they may be a defined over a continuum, which is a tensor field . Theano and TensorFlow use tensors to perform most of the mathematical operations as data is passed through a computational graph, colloquially known as a model .It is generally suggested that if you do not know Theano, you should focus on mastering TensorFlow and Keras. Those that are familiar with the Theano framework, however, may feel free to rewrite the existing TensorFlow code in Theano.Chapter 1  What Is Natural laNguage proCessINg?10 Applications of\\xa0Deep Learning to\\xa0NLPThis section discusses the applications of deep learning to NLP . Introduction to\\xa0NLP Techniques and\\xa0Document ClassificationIn Chapter 3, we walk through some introductory techniques, such as word tokenization, cleaning text data, term frequency, inverse document frequency, and more. We will apply these techniques during the course of our data preprocessing as we prepare our data sets for some of the algorithms reviewed in Chapter 2. Specifically, we focus on classification tasks and review the relative benefits of different feature extraction techniques when applied to document classification tasks. Topic ModelingIn Chapter 4, we discuss more advanced uses of deep learning, machine learning, and NLP .\\xa0We start with topic modeling and how to perform it via latent Dirichlet allocation, as well as non-negative matrix factorization. Topic modeling is simply the process of extracting topics from documents. You can use these topics for exploratory purposes via data visualization or as a preprocessing step when labeling data. Word EmbeddingsWord embeddings are a collection of models/techniques for mapping words (or phrases) to vector space, such that they appear in a high-  dimension al field. From this, you can determine the degree of similarity, or dissimilarity, between one word (or phrase, or document) and another. When we project the word vectors into a high-dimensional space, we can envision that it appears as something like what’s shown in Figure\\xa0 1-3.Chapter 1  What Is Natural laNguage proCessINg?11Ultimately, how you utilize word embeddings is up to your own interpretation. They can be modified for applications such as spell check, but can also be used for sentiment analysis, specifically when assessing larger entities, such as sentences or documents in respect to one another. We focus simply on how to train the algorithms and how to prepare data to train the embeddings themselves. Language Modeling Tasks Involving RNNsIn Chapter 5, we end the book by tackling some of the more advanced NLP applications, which is after you have been familiarized with preprocessing text data from various format and training different algorithms. Specifically, we focus on training RNNs to perform tasks such as name entity recognition, answering questions, language generation, and translating phrases from one language to another.walkedwalkingswamswimmingVerb tenseFigure 1-3.  Visualization of word embeddingsChapter 1  What Is Natural laNguage proCessINg?12 SummaryThe purpose of this book is to familiarize you with the field of natural language processing and then progress to examples in which you can apply this knowledge. This book covers machine learning where necessary, although it is assumed that you have already used machine learning models in a practical setting prior.While this book is not intended to be exhaustive nor overly academic, it is my intention to sufficiently cover the material such that readers are able to process more advanced texts more easily than prior to reading it. For those who are more interested in the tangible applications of NLP as the field stands today, it is the vast majority of what is discussed and shown in examples. Without further ado, let’s begin our review of machine learning, specifically as it relates to the models used in this book.Chapter 1  What Is Natural laNguage proCessINg?13© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5_2CHAPTER 2Review of\\xa0Deep LearningYou should be aware that we use deep learning and machine learning methods throughout this chapter. Although the chapter does not provide a comprehensive review of ML/DL, it is critical to discuss a few neural network models because we will be applying them later. This chapter also briefly familiarizes you with TensorFlow, which is one of the frameworks utilized during the course of the book. All examples in this chapter use toy numerical data sets, as it would be difficult to both review neural networks and learn to work with text data at the same time.Again, the purpose of these toy problems is to focus on learning how to create a TensorFlow model, not to create a deployable solution. Moving forward from this chapter, all examples focus on these models with text data. Multilayer Perceptrons and\\xa0Recurrent Neural NetworksTraditional neural network models, often referred to as multilayer perceptron models  (MLPs), succeed single-layer perceptron models  (SLPs). MLPs were created to overcome the largest shortcoming of the SLP model, which was the inability to effectively handle data that is not linearly separable. In practical cases, we often observe that multivariate data is14non-linear, rendering the SLP null and void. MLPs are able to overcome this shortcoming—specifically because MLPs have multiple layers. We’ll go over this detail and more in depth while walking through some code to make the example more intuitive. However, let’s begin by looking at the MLP visualization shown in Figure\\xa0 2-1.Each layer of an MLP model is connected by weights, all of which are initialized randomly from a standard normal distribution. The input layer has a set number of nodes, each representative of a feature within a neural network. The number of hidden layers can vary, but each of them typically has the same number of nodes, which the user specifies. In regression, the output layer has one node. In classification, it has K nodes, where K is the number of classes.Next, let’s have an in-depth discussion on how an MLP works and complete an example in TensorFlow.Figure 2-1.  Multilayer perceptronChapter 2  review of\\xa0Deep Learning15 Toy Example 1: Modeling Stock Returns with\\xa0the\\xa0MLP ModelLet’s imagine that we are trying to predict Ford Motor Company (F) stock returns given the returns of other stocks on the same day. This is a regression problem, given that we are trying to predict a continuous value. Let’s begin by defining an mlp_model  function with arguments that will be used later, as follows:def mlp_model(train_data=train_data, learning_rate=0.01, iters=100, num_hidden1=256):This Python function contains all the TensorFlow code that forms the body of the neural network. In addition to defining the graph, this function invokes the TensorFlow session that trains the network and makes predictions. We’ll begin by walking through the function, line by line, while tying the code back to the theory behind the model.First, let’s address the arguments in our function: train_data  is the variable that contains our training data; in this example; it is the returns of specific stocks over a given period of time. The following is the header of our data set:0  0.002647 -0.001609   0.012800   0.000323   0.016132 -0.004664 -0.0185981  0.000704   0.000664   0.023697 -0.006137 -0.004840   0.003555 -0.0066642  0.004221   0.003600   0.002469 -0.010400 -0.008755 -0.002737    0.0253673  0.003328   0.001605   0.050493   0.006897   0.010206   0.002260 -0.0071564  0.001397   0.004052 -0.009965 -0.012720 -0.019235 -0.002255    0.017916Chapter 2  review of\\xa0Deep Learning165 -0.009326 -0.003754 -0.014506 -0.006607 -0.034865   0.011463    0.0038446  0.008446   0.005747   0.022830   0.009312   0.021757 -0.000319    0.0239827  0.002705   0.002623   0.007636   0.020099 -0.007433 -0.008303  -0.0043308 -0.011224 -0.009530 -0.008161 -0.003230 -0.015381 -0.003381  -0.0106749  0.012496   0.010942   0.016750   0.007777   0.001233   0.008724    0.033367Each of the columns r epresent the percentage return of a stock on a given day, with our training set containing 1180 observations and our test set containing 582 observations.Moving forward, we come to the learning rate and activation function. In machine learning literature, the learning rate is often represented by the symbol η (eta). The learning rate is a scalar value that controls the degree to which the gradient is updated to the parameter that we wish to change. We can exemplify this technique when referring to the gradient descent update method. Let’s first look at the equation, and then we can break it down iteratively. qq hq tt iN i iNhx y+==- ()- () 112 1S (2.1) qq hqq q tt ii i iNhx yh x+==- ()- () Ñ()1112S In Equation 2.1, we are updating some parameter, θ, at a given time step, t. hθ(x)i is equal to the hypothesized label/value, y being the actual label/value, in addition to N being equal to the total number of observations in the data set we are training on.∇θhθ(x)i is the gradient of the output with respect to the parameters of the model.Chapter 2  review of\\xa0Deep Learning17Each unit in a neural network (with the exception of the input layer) receives the weighted sum of inputs multiplied by weights, all of which are summed with a bias. Mathematically, this can be described in Equation 2.2. yf xw bT=() + , (2.2)In neural networks, the parameters are the weights and biases. When referring to Figure\\xa0 2-1, the weights are the lines that connect the units in a layer to one another and are typically initialized by randomly sampling from a normal distribution. The following is the code where this occurs:     weights = {\\'input\\': tf.Variable(tf.random_normal([train_x.shape[1], num_hidden])),             \\'hidden1\\': tf.Variable(tf.random_normal([num_hidden, num_hidden])),             \\'output\\': tf.Variable(tf.random_normal([num_hidden, 1]))}     biases = {\\'input\\': tf.Variable(tf.random_normal([num_hidden])),             \\'hidden1\\': tf.Variable(tf.random_normal([num_hidden])),            \\'output\\': tf.Variable(tf.random_normal([1]))}Because they are part of the computational graph, weights and biases in TensorFlow must be initialized as TensorFlow variables with the tf.Variable() . TensorFlow thankfully has a function that generates numbers randomly from a normal distribution called tf.random_normal() , which takes an array as an argument that specifies the shape of the matrix that you are creating. For people who are new to creating neural networks, choosing the proper dimensions for the weight and bias units is a typical source of frustration. The following are some quick pointers to keep in mind :• When r eferring to the weights, the columns of a given layer need to match the rows of the next layer.Chapter 2  review of\\xa0Deep Learning18• The columns of the wei ghts for every layer must match the number of units for each layer in the biases.• The o utput layer columns for the weights dictionary (and array shape for the bias dictionary) should be representative of the problem that you are modeling.  (If regression, 1; if classification, N, where N = the number of classes).You might be curious as to why we initialize the weights and biases randomly. This leads us to one of the key components of neural networks’ success. The easiest explanation is to imagine the following two scenarios:• All weig hts are initialized to 1 . If all the weights are initialized to 1, every neuron is passed the same value, equal to the weighted sum, plus the bias, and then put into some activation function, whatever this value may be.• All weig hts are initialized to 0 . Similar to the prior scenario, all the neurons are passed the same value, except that this time, the value is definitely zero.The more general problem associated with weights that are initialized at the same location is that it makes the network susceptible to getting stuck in\\xa0local minima. Let’s imagine an error function, such as the one shown in Figure\\xa0 2-2.Chapter 2  review of\\xa0Deep Learning19Imagine that when we initialize the neural network weights at 0, and subsequently that when it calculates the error, it yields the value at the Y variable in Figure\\xa0 2-2. The gradient descent algorithm always gives the same update for the weights from the first iteration of the algorithm, and it likely gives the same value moving forward. Because of that, we are not taking advantage of the ability of neural networks to start from any point in the solution space. This effectively removes the stochastic nature of neural networks, and considerably reduces the probability of reaching the best possible solution for the weight optimization problem. Let’s discuss the learning rate.Figure 2-2.  Error plotChapter 2  review of\\xa0Deep Learning20 Learning RateThe learning rate is typically a static floating-point value that determines the degree to which the gradient, or error, affects the update to the parameter that you seek to optimize. In example problems, it is common to see learning rates initialized anywhere from 0.01 to 1e–4. The initialization of the learning rate parameter is another point worth mentioning, as it can affect the speed at which the algorithm converges upon a solution. Briefly, the following are two scenarios that are problematic:• The learning rate is too large.  When the learning rate is too large, the error rate moves around in an erratic fashion. Typically, we observe that the algorithm on one iteration seems to find a better solution than the prior one, only to get worse upon the next, and oscillating between these two bounds. In a worst-case scenario, we eventually start to receive NaN values for error rates, and all solutions effectively become defunct. This is the exploding gradient problem, which I discuss later.• The learning rate is too small.  Although, over time, this does not yield an incorrect, ultimately, we spend an inordinate amount of time waiting for the solution to converge upon an optimal solution.The optimal solution is to pick a learning rate that is large enough to minimize the number of iterations needed to converge upon an optimal solution, while not being so large that it passes this solution in its optimization path. Some solutions, such as adaptive learning rate algorithms, solve the problem of having to grid search or iteratively use different learning rates. The mlp_model()  function uses the Adam (ada ptive moment estimation) optimization algorithm, which updates the learning rate aw we learn. I briefly discuss how this algorithm works, and why you should use it to expedite learning rate optimization.Chapter 2  review of\\xa0Deep Learning21Adam was first described in a paper that written by Diederik Kingma and Jimmy Lei Ba. Adam specifically seeks to optimize learning rates by estimating the first and second moments of the gradients. For those who are unfamiliar, moments  are defined as specific measures of the shape of a set of points. As it relates to statistics, these points are typically the values within a probability distribution. We can define the zeroth moment as the total probability; the first moment as the mean; and second moment as the variance. In this paper, they describe the optimal parameters for Adam, in addition to some initial assumptions, as follows:• α\\xa0=\\xa0Stepsize; α\\xa0≔\\xa00.001,  \\xa0ϵ\\xa0=\\xa010−8• β1, β2\\xa0=\\xa0Exponential decay rates for\\xa01st and 2nd\\xa0moment estimateions\\xa0 β1\\xa0=\\xa00.9,  β2\\xa0=\\xa00.999; β1, β2\\xa0∈\\xa0[0, 1)• f(θ)\\xa0=\\xa0Stochastic objective function that we\\xa0are\\xa0optimizing with parameters\\xa0 θ• m\\xa0=\\xa01st\\xa0moment vector,  v\\xa0=\\xa02nd\\xa0moment vector\\xa0(Both initialized\\xa0as\\xa00s)With this in mind, although we have not converged upon an optimal solution, the following is the algorithm that we use:• gt\\xa0=\\xa0∇θft(θt\\xa0−\\xa01)• ˆˆ,mv=-Bias corrected first and second moment estimates  resppect ively;• mm gv vgtt tt tt ::==bb bb11 12 12211 *+ -() ** +-() *--• ˆ:, : mmvvttt ttt==1112--bb\\uf0b5• qq attttmv:= -*-1ˆ() +\\uf0b7\\uf0f2While the preceding formulae describe Adam when optimizing one parameter, we can extrapolate the formulae to adjust for multiple parameters (as is the case with multivariate problems). In the paper, Adam Chapter 2  review of\\xa0Deep Learning22outperformed other standard optimization techniques and was seen as the default learning rate optimization algorithm.As for the final parameters, num_hidden  refers to the number of units in the hidden layer(s). A commonly referenced rule of thumb is to make this number equal to the number of inputs plus the number of outputs, and then multiplied by 2/3.Epochs  refers to the number of times the algorithm should iterate through the entirety of the training set. Given that this is situation dependent, there is no general suggestible number of epochs that a neural network should be trained. However, a suggestible method is to pick an arbitrarily large number (1500, for example), plot the training error, and then observe which number of epochs is sufficient. If needed, you can enlarge the upper limit to allow the model to optimize its solution further.Now that I have finished discussing the parameters, let’s walk through the architecture, code, and mathematics of the multilayer perceptron, as follows:#Creating training and test setstrain_x, train_y = train_data[0:int(len(train_data)*.67), 1:train_data.shape[1]], train_data[0:int(len(train_data)*.67), 0]test_x, test_y = train_data[int(len(train_data)*.67):, 1:train_data.shape[1]], train_data[int(len(train_data)*.67):, 0]Observe that we are creating both a training set and a test set. The training and test sets contain 67% and 33%, respectively, of the original data set labeled train_data . It is suggested that machine learning problems have these two data sets, at a minimum. It is optional to create a validation set as well, but this step is omitted for the sake of brevity in this example.Next, let’s discuss the following important aspect of working with TensorFlow:#Creating placeholder values and instantiating weights and biases as dictionariesChapter 2  review of\\xa0Deep Learning23X = tf.placeholder(\\'float\\', shape = (None, 7))Y = tf.placeholder(\\'float\\', shape = (None, 1))When working in TensorFlow, it is important to refer to machine learning models as graphs , since we are creating computational graphs with different tensor objects. Any typical deep learning or machine learning model expects an explanatory and response variable; however, we need to specify what these are. Since they are not a part of the graph, but are representational objects that we are passing data through, they are defined as placeholder variables , which we can access from TensorFlow (imported as tf) by using tf.placeholder() . The three arguments for this function are dtype (data type), shape, and name. dtype and shape are the only required arguments. The following are quick rules of thumb:• Generally, the shape of the X and Y variables should be initialized as a tuple. When working with a two-  dimensional data set, the shape of the X variable should be (none, number of features), and the shape of the Y variable should be (none, [1 if regression, N if classification]), where N is the number of classes.• The data type specified for these placeholders should reflect the values that you are passing through them. In this instance, we are passing through a matrix of floating-point values and predicting a floating-point value, so both placeholders for the response and explanatory variables have the float data type. In the instance that this was a classification problem, assuming the same data passed through the explanatory variable, the response variable has the int data type since the labels for the classes are integers.Chapter 2  review of\\xa0Deep Learning24Since I discussed the weights in the neural network already, let’s get to the heart of the neural network structure: the input through the output layers, as shown in the following code (inside mlp_model()  function):#Passing data through input, hidden, and output layersinput_layer = tf.add(tf.matmul(X, weights[\\'input\\']), biases[\\'input\\']) (1)input_layer = tf.nn.sigmoid(input_layer) (2)input_layer = tf.nn.dropout(input_layer, 0.20) (3)hidden_layer = tf.add(tf.multiply(input_layer, weights[\\'hidden1\\']), biases[\\'hidden1\\'])hidden_layer = tf.nn.relu(hidden_layer)hidden_layer = tf.nn.dropout(hidden_layer, 0.20)output_layer = tf.add(tf.multiply(hidden_layer, weights [\\'output\\']),biases[\\'output\\']) (4)When looking at the first line of highlighted code (1), we see the input layer operation. Mathematically, operations from one neural network layer to the next can be represented by the following equation: layerf Xw biaskk kTk =*() + ()  (2.2.1)f(x) is equal to some activation function. The output from this operation is passed to the next layer, where the same operation is run, including any operations placed between layers. In TensorFlow, there are built-in mathematical operations to represent the preceding equation:  tf.add()  and tf.matmul() .After we create the output, which in this instance is a matrix of shape (1, 256), we pass it to an activation function. In the second line of highlighted code (2), we first pass the weighted sum of the inputs and bias to a sigmoid activation function, given in Equation 2.3. s=+æèçöø÷ -11ex (2.3)Chapter 2  review of\\xa0Deep Learning25e is the exponential function. Activation functions serve as a way to scale the outputs from Equation 2.2, and are sometimes directly related to how we classify outputs. More importantly, this is the core component of the neural network that introduces non-linearity to the learning process. Simply stated, if we use a linear activation function, where f(x) = x, we are simply repetitively passing the outputs of a linear function from the input layer to the output layer. Figure\\xa0 2-3 illustrates this activation function.Although the range here is from –6 to 6, the function essentially looks like −∞  to ∞, in that there are asymptotes at 0 and 1 as X grows infinitely larger or infinitely smaller, respectively. This function is one of the more common activation functions utilized in neural networks, which we use in the first layer.-6 -5 -4 -3 -2 -1 01 23 45 61.00.80.60.40.20.0Activation Function OuptutX ValueSigmoid Activation FunctionFigure 2-3.  Sigmoid activation functionChapter 2  review of\\xa0Deep Learning26Also, we defined the derivative of this function, which is important in mathematically explaining the vanishing gradient problem (discussed later in the chapter). Although going through all the activation functions in neural networks would be exhaustive, it is worth discussing the other activation function that this neural network utilizes. The hidden layer uses a ReLU activation function, which is mathematically defined in Equation 2.4. ReLU xx() = () max0,  (2.4)The function is illustrated in Figure\\xa0 2-4.Figure 2-4.  ReLU activation functionChapter 2  review of\\xa0Deep Learning27Both mathematically and visually, the ReLU activation function is simple. The output of a ReLU is a matrix of 0s, with some positive values. One of the major benefits of the ReLU activation function lies in the fact that it produces a sparse matrix as its output. This attribute is ultimately why I have decided to include it as the activation function in the hidden layer, particularly as it relates to the vanishing gradient problem. Vanishing Gradients and\\xa0Why ReLU Helps to\\xa0Prevent ThemThe vanishing gradient problem is specific to the training of neural networks, and it is part of the improvements that researchers sought to make with LSTM over RNN (both are discussed later in this chapter). The vanishing gradient problem is a phenomenon observed when the gradient gets so small that the updates to weights from one iteration to the next either stops completely or is considerably negligible.Logically, what proceeds is a situation in which the neural network effectively stops training. In most cases, this results in poor weight optimization, and ultimately, bad training and test set performance. Why this happens can be explained precisely by how the updates for each of the weights are calculated:When we look at Figure\\xa0 2-3, we see the derivative of the sigmoid function. The majority of the function’s derivate falls in a narrow range, with most of the values being close to 0. When considering how to calculate the gradient of differing hidden layers, this is precisely what causes a problem as our network gets deeper. Mathematically, this is represented by the following equation: ¶¶=¶¶¶¶¶¶¶¶=åEWEyyssssWk kk 30233333\\uf0b5 Chapter 2  review of\\xa0Deep Learning28As you can see, when we backpropagate the error to layer k, which in this example is 0 (the input layer), we are multiplying several derivatives of the activation function’s output several times. This is a brief explanation of the chain rule and underlies most of a neural networks’ backpropagation training algorithm. The chain rule is a formula that specifies how to calculate a derivative that is composed of two or more functions. Assume that we have a two-layer neural network. Let’s also assume that our respective gradients are 0.001 and 0.002. This yields 2 e–6 as a respective gradient of the output layer. Our update to the next gradient would be described as negligible.You should know that any activation function that yields non-  sparse outputs, particularly when used for multiple layers in succession, typically causes vanishing gradients. We are able to substantially mitigate this problem by using a combination of sparse and non-sparse output activation functions, or exclusively utilize non-spare activation functions. We illustrate an example of such a neural network in the mlp_model()  function. For now, however, let’s take a look at one last activation layer before we finish analyzing this MLP .Observe that after every activation layer, we use the dropout layer , invoked by tf.nn.dropout() . Dropout layers have the same dimensions as the layer preceding them; however, they arbitrarily set a random selection of weights’ values to 0, effectively “shutting off” the neurons that are connected to them. In every iteration, there are a different set of random neurons that shut off. The benefit of using dropout is to prevent overfitting, which is the instance in which a model performs well in training data but poorly in test data.There are a multitude of factors that can cause overfitting, including (but not limited to) not having enough training data or not cross-  validating data (which induces a model to memorize idiosyncrasies of a given orientation of a data set rather than generalizing to the distribution underlying the data). Although you should solve issues like these first, adding dropout is not a bad idea. When you execute functions without dropout, you notice overfitting relative to the models that do contain dropout.Let’s discuss some final MLP topics—specifically, the key components to what causes the model to learn.Chapter 2  review of\\xa0Deep Learning29 Loss Functions and\\xa0BackpropagationLoss functions are specifically how we define the degree to which our model was incorrect. In regression, the most typical choices are mean squared error  (MSE) or root mean squared error  (RMSE). Mathematically, they are defined as follows: MSENhx yiNii= ()- ()=å112q (2.5) RMSENhx yiNii= ()- ()=å112q (2.6)error = tf.reduce_mean(tf.pow(output_layer\\xa0– Y,2)) (mean squared error in code)Intuitively, MSE (see Equation 2.5) provides a method for assessing what was the average error over all predictions in a given epoch. RMSE (see Equation 2.6) provides the same statistic, but takes the square root of the MSE value. The benefit of RMSE is that it provides a statistic in the same unit as the predicted value, allowing the user to assess the performance of the model more precisely. MSE does not have this benefit, and as such, it becomes less interpretable—except in the sense that a lower MSE from one epoch to the next is desirable.As an example, if we are predicting money, what does it mean that our prediction is $0.30 squared inaccurate? While we can tell that we have a better solution if the next epoch yields an MSE of $0.10, it is much harder to tell precisely what an MSE of $0.10 translates to in a given prediction. We compare the results of using RMSE vs. MSE in the final toy example in the chapter. In natural language processing, however, we more often deal with error functions reserved for classification tasks. With that in mind, you should be accustomed to the following formulas.Chapter 2  review of\\xa0Deep Learning30The binary cross entropy is \\uf04cyh yp ypxi,q() () =- ()+-() -() logl og ) 11 (2.7)The multiclass cross entropy is \\uf04cyh ssxijyi,,q() () =- + () max0 D (2.8)Cross entropy  is the number of bits needed to identify an event drawn from a set. The same principles (with respect to training using an MSE or RMSE loss function) are carried when using a cross-entropy-based loss function. Our objective is to optimize the weights in a direction that minimizes the error as much as possible.At this point, we have walked through the MLP from the initialization of the parameters, what they mean, how the layer moves from each layer, what the activation functions do to it, and how the error is calculated. Next, let’s dig into recurrent neural networks, long short-term memory, and their relative importance in the field of natural language processing. Recurrent Neural Networks and\\xa0Long Short-Term MemoryDespite the relative robustness of MLPs, they have their limitations. The model assumes independence between inputs and outputs, making it a suboptimal choice for problems in which the output of function is statistically dependent on the preceding inputs. As this relates to natural language processing (NLP), there are tasks that MLPs might be particularly useful for, such as sentiment analysis. In these problems, one body of text being classified as negative is not dependent on assessing the sentiment of a separate body of text.As an example, I wouldn’t need to read multiple restaurant reviews to determine whether an individual review is positive or negative. It can be determined by the attributes of a given observation. However, this is not Chapter 2  review of\\xa0Deep Learning31always the type of NLP problem we encounter. For example, let’s assume that we are trying to spell-check on the following sentences:“I am happy that we are going too the mall!”“I am happy to. That class was excellent. ”Both sentences are incorrect in their usage of the words too and to, respectively, because of the context in which they appear. We must use the sequence of words prior, and perhaps even the words after, to determine what is incorrect. Another similar problem would be predicting words in a sentence; for example, let’s look at the following sentence.“I was born in Germany. I speak _______. ”Although there isn’t necessarily one answer to complete this sentence, as being born in Germany does not predetermine someone to speaking only German, there is a high probability that the missing word is German . However, we can only say that because of the context that surrounds the words, and assuming that the neural network was trained on sentences (or phrases) and has a similar structure. Regardless, these types of problems call for a model that can accommodate some sort of memory related to the prior inputs, which brings us to recurrent neural network. Figure\\xa0 2-5 shows the structure of an RNN.oVWWWUVUxsUnfoldot-1xt-1st-1WWUVot +1xt +1st +1UVotxtstFigure 2-5.  Recurrent neural networkChapter 2  review of\\xa0Deep Learning32It is important to examine the structure of the RNN as it relates to resolving the statistical dependency problem. Similar to the prior example, let’s walk through some example code in TensorFlow to illustrate the model structure using a toy problem. Similar to the MLP , we will work with a toy problem to create a function that loads and preprocesses our data for the neural network, and then make a function to build our neural network. The following is the beginning of the function:def build_rnn(learning_rate=0.02, epochs=100, state_size=4):The first two arguments should be familiar. They represent the same concepts as in the MLP example. However, we have a new argument called state_size . In a vanilla RNN, the model we are building here, we pass what is called the hidden state  from a given time step forward. The hidden state is similar to the hidden layer of an MLP in that it is a function of the hidden states at previous time steps. The following defines the hidden state and output as hf Wx Wh btx ht hh th =+ + ()-1  (2.9) yW hbth ot o =+  (2.10)ht is the hidden state, W is the weight matrix, b is the bias array, y is the output of the function, and f(x) is the activation function of our choosing. Toy Example 2: Modeling Stock Returns with\\xa0the\\xa0RNN ModelUsing the code in the build_rnn()  function, observe the following.#Loading data     x, y = load_data(); scaler = MinMaxScaler(feature_range=(0, 1))    x, y = scaler.fit_transform(x), scaler.fit_transform(y)Chapter 2  review of\\xa0Deep Learning33     train_x, train_y = x[0:int(math.floor(len(x)*.67)),   :], y[0:int(math.floor(len(y)*.67))]    #Creating weights and biases dictionaries     weights = {\\'input\\': tf.Variable(tf.random_normal([state_size+1, state_size])),         \\'output\\': tf.Variable(tf.random_normal([state_size, train_y.shape[1]]))}     biases = {\\'input\\': tf.Variable(tf.random_normal([1, state_size])),         \\'output\\': tf.Variable(tf.random_normal([1, train_y.shape[1]]))}We b egin by loading the training and test data, performing a similar split in the test set such that the first 67% of the complete data set becomes the training set and the remaining 33% becomes the test set. In this instance, we distinguish between two classes, 0 or 1, indicating whether the price went up or down. Moving forward, however, we must refer back to the state size parameter to understand the shape of the matrices we produce, again as TensorFlow variables, for the weight and bias matrices.To crystallize your understanding of the state size parameter, refer to Figure\\xa0 2-5, in which the center of the neural network represents a state. We multiply the given input, as well as the previous state, by a weight matrix, and sum all of this with the bias. Similar to the MLP , the weighted sum value forms the input for the activation function.The output of the activation function forms the hidden state at time step t, whose value becomes part of the weighted sum in Equation 2.10 . The value of this matrix application ultimately forms the output for the RNN.\\xa0We repeat these operations for as many states that we have, which is equal to the number of inputs that we pass through the neural network. When referring back to the image, this is what is meant by the RNN being “unfolded. ” The state_size  in our example is set to 4, meaning that we are inputting four input sequences before we make a prediction.Chapter 2  review of\\xa0Deep Learning34Let’s now walk through the TensorFlow code associated with these operations.#Defining placeholders and variables    X = tf.placeholder(tf.float32, [batch_size, train_x.shape[1]])    Y = tf.placeholder(tf.int32, [batch_size, train_y.shape[1]])     init_state = tf.placeholder(tf.float32, [batch_size, state_size])    input_series = tf.unstack(X, axis=1)    labels = tf.unstack(Y, axis=1)    current_state = init_state    hidden_states = []    #Passing values from one hidden state to the next     for input in input_series: #Evaluating each input within the series of inputs         input = tf.reshape(input, [batch_size, 1]) #Reshaping input into MxN tensor         input_state = tf.concat([input, current_state], axis=1) #Concatenating input and current state tensors         _hidden_state = tf.tanh(tf.add(tf.matmul(input_state, weights[\\'input\\']), biases[\\'input\\'])) #Tanh transformation         hidden_states.append(_hidden_state) #Appending the next state        current_state = _hidden_state #Updating the current stateSimilar to the MLP model, we need to define place holder variables for both the x and y tensors that our data will pass through. However, a new placeholder will be here, which is the init_state , representing the initial state matrix. Notice that the current state is the init_state  placeholder for the first iteration through the next. It also holds the same dimensions and expects the same data type.Chapter 2  review of\\xa0Deep Learning35Moving forward, we iterate through every input_sequence  in the data set, where _hidden_state  is the Python definition of formula (see Equation 2.9). Finally, we must come to the output state, given by the following:logits = [tf.add(tf.matmul(state, weights[\\'output\\']), biases[\\'output\\']) for state in hidden_states]The code here is representative of Equation 2.10 . However, this will only give us a floating-point decimal, which we need to convert into a label somehow. This brings us to an activation function which will be important to remember for multiclass classification, and therefore for the remainder of this text, the softmax activation function. Subsequently, we define this activation function as the following: SyeeiyiNyii () =æèççöø÷÷=å1  (2.11)When you look at the formula, we are summing some value over all the possible values. As such, we define this as a probability score. When relating this back to classification, particularly with the RNN, we are outputting the relative probability of an observation being of one class vs another (or others). The label we choose in this instance is the one with the highest relative score, meaning that we choose a given label k because it has the highest probability of being true based on the model’s prediction. Equation 2.11  is subsequently represented in the code by the following line:predicted_labels = [tf.nn.softmax(logit) for logit in logits] #predictions for each logit within the seriesBeing that this is a classification problem, we use a cross entropy–based loss function and for this toy example we will use the gradient descent algorithm, both of which were elaborated upon in the prior section MLPs. Invoking the TensorFlow session also is performed in the same fashion as it would be for the MLP graph (and furthermore for all TensorFlow computational graphs). In a slight derivation from the MLP , Chapter 2  review of\\xa0Deep Learning36we calculate errors at each time step of an unrolled network and sum these errors. This is known as backpropagation through time  (BPTT), which is utilized specifically because the same weight matrix is used for every time step. As such, the only changing variable besides the input is the hidden state matrix. As such, we can calculate each time step’s contribution to the error. We then sum these time step errors to get the error. Mathematically, this is represented by the following equation: ¶¶=¶¶¶¶¶¶¶¶=åEWEyyssssWk kk 30333333\\uf0b5 This is an application of the chain rule, as described briefly in the section on how we backpropagate the error from the output layer back to the input layer to update the weights with respect to their contribution to the total error. BPPT applies the same logic; instead, we treat the time steps as the layers. However, although RNNs solved many problems of MLPs, they had relative limitations, which you should be aware of.One of the largest drawbacks of RNNs is that the vanishing gradient problem reappears. However, instead of it being due to having very deep neural network layers, it is caused by trying to evaluate arbitrarily long sequences. The activation function used in RNNs is often the tanh activation function. Mathematically, we define this as follows: tanh x()=-+--eeeexxxx  Figure 2-6 illustrates the activation function.Chapter 2  review of\\xa0Deep Learning37Similar to the problem with the sigmoid activation function, the derivative of the tanh function can 0, such that when backpropagated over large sequences results in a gradient that is equal to 0. Similar to the MLP , this can cause problems with learning. Depending on the choice of activation function, we also might experience the opposite of the vanishing gradient problem—the exploding gradient. Simply stated, this is the result of the gradients appearing as NaN values. There are couple of solutions for the vanishing gradient function in RNNs. Among them are to try weight regularization via an L1 or L2 norm, or to try different activation functions as we did in the MLP , utilizing functions such as ReLU.\\xa0However, one of the more straightforward solutions is to use a model devised in the 1990s by Sepp Hochreiter and Jürgen Schmidhuber: the long short-term memory unit, or LSTM. Let’s start with what this model looks like, as shown in Figure\\xa0 2-7.Figure 2-6.  Tanh activation and derivative functionChapter 2  review of\\xa0Deep Learning38LSTMs are distinguished structurally by the fact that we observe them as blocks, or units, rather than the traditional structure a neural network often appears as. That said, the same principles are generally applied here. However, we have an improvement over the hidden state from the vanilla RNN.\\xa0I will walk through the formulae associated with the LSTM. iW xW hW cbtx it hith ct i =+ ++ ()-- s11  (2.12) fW xW hW cbtx ft hf th ft f =+ ++ () -- s11  (2.13) cf ci Wx Wh btt tt xc th ct c =+ ++ ()--\\uf06f\\uf06f11 tanh  (2.14) oW xW hW cbtx ot ho tc ot o =+ ++ ()- s1  (2.15) ho ctt t = ()\\uf06ftanh  (2.16)it is the input gate, ft is the forget gate, ct is the cell state, ot is the output gate, htis the output vector, σ is the sigmoid activation function, and tanh is the tanh activation function. Both the hidden and cell states are initialized at 0 upon initialization of the algorithm.The formulae from the LSTM is similar to that of the vanilla RNN, however there is some slight complexity added. Initially, let’s draw our attention to the diagram, specifically the LSTM unit in the center, and understand the directional flow as they relate to the formulae. xxx+tanhtanhxx+tanhxx+tanhxx+tanhtanhxx+tanhtanhAAXt-1ht-1 ht ht+1Xt+1 XtFigure 2-7.  LSTM units/blocksChapter 2  review of\\xa0Deep Learning39Preliminarily, let’s discuss the notation. Each block, denoted by  , represents a neural network layer, through which we pass through values. The horizontal lines with arrows represent the vectors and direction in which the data moves. After it moves through a neural network layer, the data often is passed to a pointwise operation object, represented by  .Now that I have discussed how to read the diagram, let’s dive in deeper.LSTMs are distinguished by having gates that regulate the information that passes through individual units, as well as what information passes to the next unit. Individually, these are the input gate, the output gate, and the forget gate. In addition to these three gates, an LSTM also contains a cell, which is an important aspect of the unit.On the diagram, the cell is represented by the horizontal line, and it is mathematically represented in Equation 2.14 . The cell state is similar to the hidden state, featured here as well as in the RNN, except there is discretion as to how much information we pass from one unit to the next. When looking at the diagram, an input, xt, is passed through the input gate. Here the neural network is put through a neural network layer, with a sigmoid activation function that passes the output to a pointwise multiplication operator. This operation is combined with the forget gate, ft, which is the entirety of Equation 2.14 .Above all, what you should take away from this operation is that its output is a number between and including 0 and 1. The closer the number is to 1, information is increasingly passed to the subsequent unit. In contrast, the closer the number is to 0, information is decreasingly passed to the subsequent unit.In Equation 2.13 , the forget gate, is what regulates this acceptance of information, which is represented by ct\\xa0−\\xa01.Moving to Equation 2.15  and relating it to the diagram, this is the neural network layer furthest to the right that is passed through another sigmoid layer, in similar fashion in to the input layer. The output of this sigmoid activated neural network layer is then multiplied with the tanh activated cell state vector, in Equation 2.16  Finally, we pass both the Chapter 2  review of\\xa0Deep Learning40cell state vector and the output vector to the next LSTM unit. While I do not draw out the LSTM in the same fashion as the RNN, I utilize the TensorFlow API’s implementation of the LSTM. Toy Example 3: Modeling Stock Returns with\\xa0the\\xa0LSTM ModelAs was the case in our prior neural network examples, we must still create TensorFlow placeholders and variables. For this example, the LSTM expects sequences of data, which we facilitate by first creating a three-  dimensional X placeholder variables. To avoid debugging issues when deploying this API with different data sets, you should be careful to read the following instructions carefully .    X = tf.placeholder(tf.float32, (None, None, train_x.shape[1]))    Y = tf.placeholder(tf.float32, (None, train_y.shape[1]))     weights = {\\'output\\': tf.Variable(tf.random_normal([n_hidden, train_y.shape[1]]))}     biases = {\\'output\\':  tf.Variable(tf.random_normal([train_y.shape[1]]))}    input_series = tf.reshape(X, [-1, train_x.shape[1]])    input_series = tf.split(input_series, train_x.shape[1], 1)     lstm = rnn.core_rnn_cell.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0, reuse=None, state_is_tuple=True)     _outputs, states = rnn.static_rnn(lstm, input_series, dtype=tf.float32)     predictions = tf.add(tf.matmul(_outputs[-1], weights[\\'output\\']), biases[\\'output\\'])     accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax (tf.nn.softmax(predictions), 1)tf.argmax(Y, 1)), dtype=tf.     float32)),Chapter 2  review of\\xa0Deep Learning41     error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=predictions))     adam_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(error)When creating a sequence of variables, we start by creating a three-  dimensional placeholder named X, which is what we feed our data into. We transform this variable by creating a two-dimensional vector of the observations with the tf.reshape() .Next, we create a tensor object for each of these observations with the tf.split()  function, which are then stored as a list underneath  input_series .Then, we can create an LSTM cell using the BasicLSTMCell()  function. The static_rnn()  function accepts any type of RNN cell, so you can utilize other types of RNNs, such as GRUs or vanilla RNNs, and the inputs. Everything else follows the same pattern as the prior examples, in that we create TensorFlow variables to calculate accuracy, the error rate, and the Adam optimizer. SummaryWe have reached the end of our brief, but necessary review of machine learning before we dive deeply into tackling problems using these models on text data. However, it is important for us to review some key concepts:• Model choice matters!  Understand the data that you are analyzing. Is the label you are predicting dependent on other prior observed labels, or are these inputs and outputs statistically independent of one another? Failing to inspect these key properties of your data beforehand will waste time and provide you with suboptimal results. Do not skip these steps.Chapter 2  review of\\xa0Deep Learning42• Parameter choice matters!  Picking the right model for a problem is the first step, but you have to tune this model properly to get optimal results. Inspect model performance when you alter the number of hidden units and epochs. I suggest utilizing algorithms such as Adam to tune the learning rate while the network is training. Where possible, grid search or use similar reactive search methods to find better parameters.• Activation functions matter!  Be mindful of how your neural network behaves with respect to the vanishing gradient problem, particularly if you are working with long sequences or have very deep neural networks.With these concepts in mind, there is one that we did not cover in this chapter: data preprocessing. It is more appropriate to discuss with the problems we are facing.Let’s move from this chapter and get into the weeds of natural language processing with a couple of example problems. In the next chapter, we walk through a couple of methods for preprocessing text, discuss their relative advantages and disadvantages, and compare model performance when using them.Chapter 2  review of\\xa0Deep Learning43© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5_3CHAPTER 3Working with\\xa0 Raw TextThose who approach NLP with the intention of applying deep learning are most likely immediately confronted with a simple question: How does a machine learning algorithm learn to interpret text data? Similar to the situations in which a feature set may have a categorical feature, we must perform some preprocessing. While the preprocessing we perform in NLP often is more involved than simply converting a categorical feature using label encoding, the principle is the same. We need to find a way to represent individual observations of texts as a row, and encode a static number of features, represented as columns, across all of these observations. As such, feature extraction becomes the most important aspect of text preprocessing.Thankfully, there has been a considerable amount of work, including ongoing work, to develop preprocessing algorithms of various complexities. This chapter introduces these preprocessing methods, walks through which situations they each work well with, and applies them to example NLP problems that focus on document classification. Let’s start by discussing what you should be aware of prior to performing feature extraction from text.44 Tokenization and\\xa0Stop WordsWhen you are working with raw text data, particularly if it uses a web crawler to pull information from a website, for example, you must assume that not all of the text will be useful to extract features from. In fact, it is likely that more noise will be introduced to the data set and make the training of a given machine learning model less effective. As such, I suggest that you perform preliminary steps. Let’s walk through these steps using the following sample text.sample_text = \"\\'I am a student from the University of Alabama. Iwas born in Ontario, Canada and I am a huge fan of the United States. I am going to get a degree in Philosophy to improvemy chances of becoming a Philosophy professor. I have beenworking towards this goal for 4 years. I am currently enrolledin a PhD program. It is very difficult, but I am confident thatit will be a good decision\"\\'When the sample_text  variable prints, there is the following output:\\'I am a student from the University of Alabama. Iwas born in Ontario, Canada and I am a huge fan of the UnitedStates. I am going to get a degree in Philosophy to improve mychances of becoming a Philosophy professor. I have been workingtowards this goal for 4 years. I am currently enrolled in a PhD program. It is very difficult, but I am confident that it willbe a good decision\\'You should observe that the computer reads bodies of text, even if punctuated, as single string objects. Because of this, we need to find a way to separate this single body of text so that the computer evaluates each word as an individual string object. This brings us to the concept of word tokenization , which is simply the process of separating a single string Chapter 3  Working With\\xa0raW text45object, usually a body of text of varying length, into individual tokens that represent words or characters that we would like to evaluate further. Although you can find ways to implement this from scratch, for brevity’s sake, I suggest that you utilize the Natural Language Toolkit (NLTK) module.NLTK allows you to use some of the more basic NLP functionalities, as well as pretrained models for different tasks. It is my goal to allow you to train your own models, so we will not be working with any of the pretrained models in NLTK.\\xa0However, you should read through the NLTK module documentation to become familiar with certain functions and algorithms that expedite text preprocessing. Relating back to our example, let’s tokenize the sample data via the following code:from nltk.tokenize import word_tokenize, sent_tokenizesample_word_tokens = word_tokenize(sample_text)sample_sent_tokens = sent_tokenize(sample_text)When you print the sample_word_tokens  variable, you should observe the following:[\\'I\\', \\'am\\', \\'a\\', \\'student\\', \\'from\\', \\'the\\', \\'University\\', \\'of\\', \\'Alabama\\', \\'.\\', \\'I\\', \\'was\\', \\'born\\', \\'in\\', \\'Ontario\\', \\',\\', \\'Canada\\', \\'and\\', \\'I\\', \\'am\\', \\'a\\', \\'huge\\', \\'fan\\', \\'of\\', \\'the\\', \\'United\\', \\'States\\', \\'.\\', \\'I\\', \\'am\\', \\'going\\', \\'to\\', \\'get\\', \\'a\\', \\'degree\\', \\'in\\', \\'Philosophy\\', \\'to\\', \\'improve\\', \\'my\\', \\'chances\\', \\'of\\', \\'becoming\\', \\'a\\', \\'Philosophy\\', \\'professor\\', \\'.\\', \\'I\\', \\'have\\', \\'been\\', \\'working\\', \\'towards\\', \\'this\\', \\'goal\\', \\'for\\', \\'4\\', \\'years\\', \\'.\\', \\'I\\', \\'am\\', \\'currently\\', \\'enrolled\\', \\'in\\', \\'a\\', \\'PhD\\', \\'program\\', \\'.\\', \\'It\\', \\'is\\', \\'very\\', \\'difficult\\', \\',\\', \\'but\\', \\'I\\', \\'am\\', \\'confident\\', \\'that\\', \\'it\\', \\'will\\', \\'be\\', \\'a\\', \\'good\\', \\'decision\\']Chapter 3  Working With\\xa0raW text46You will also observe that we have defined another tokenized object, sample_sent_tokens . The difference between word_tokenize()  and  sent_tokenize()  is simply that the latter tokenizes text by sentence delimiters. This is observed in the following output:  [\\'I am a student from the University of Alabama.\\', \\'I \\\\nwas born in Ontario, Canada and I am a huge fan of the United States.\\', \\'I am going to get a degree in Philosophy to improve my chances of \\\\nbecoming a Philosophy professor.\\', \\'I have been working towards this goal\\\\nfor 4 years.\\', \\'I am currently enrolled in a PhD program.\\', \\'It is very difficult, \\\\nbut I am confident that it will be a good decision\\']Now we have individual tokens that we can preprocess! From this step forward, we can clean out some of the junk text that we would not want to extract features from. Typically, the first thing we want to get rid of are stop words , which are usually defined as very common words in a given language. Most often, lists of stop words that we build or utilize in software packages include function words , which are words that express a grammatical relationship (rather than having an intrinsic meaning). Examples of function words include the, and , for, and of.In this example, we use the list of stop words from the NLTK package.[u\\'i\\', u\\'me\\', u\\'my\\', u\\'myself\\', u\\'we\\', u\\'our\\', u\\'ours\\', u\\'ourselves\\', u\\'you\\', u\"you\\'re\", u\"you\\'ve\", u\"you\\'ll\", u\"you\\'d\", u\\'your\\', u\\'yours\\', u\\'yourself\\', u\\'yourselves\\', u\\'he\\', u\\'him\\', u\\'his\\', u\\'himself\\', u\\'she\\', u\"she\\'s\", u\\'her\\', u\\'hers\\', u\\'herself\\', u\\'it\\', u\"it\\'s\", u\\'its\\', u\\'itself\\', u\\'they\\', u\\'them\\', u\\'their\\', u\\'theirs\\', u\\'themselves\\', u\\'what\\', u\\'which\\', u\\'who\\', u\\'whom\\', u\\'this\\', u\\'that\\', u\"that\\'ll\", u\\'these\\', u\\'those\\', u\\'am\\', u\\'is\\', u\\'are\\', u\\'was\\', u\\'were\\', Chapter 3  Working With\\xa0raW text47u\\'be\\', u\\'been\\', u\\'being\\', u\\'have\\', u\\'has\\', u\\'had\\', u\\'having\\', u\\'do\\', u\\'does\\', u\\'did\\', u\\'doing\\', u\\'a\\', u\\'an\\', u\\'the\\', u\\'and\\', u\\'but\\', u\\'if\\', u\\'or\\', u\\'because\\', u\\'as\\', u\\'until\\', u\\'while\\', u\\'of\\', u\\'at\\', u\\'by\\', u\\'for\\', u\\'with\\', u\\'about\\', u\\'against\\', u\\'between\\', u\\'into\\', u\\'through\\', u\\'during\\', u\\'before\\', u\\'after\\', u\\'above\\', u\\'below\\', u\\'to\\', u\\'from\\', u\\'up\\', u\\'down\\', u\\'in\\', u\\'out\\', u\\'on\\', u\\'off\\', u\\'over\\', u\\'under\\', u\\'again\\', u\\'further\\', u\\'then\\', u\\'once\\', u\\'here\\', u\\'there\\', u\\'when\\', u\\'where\\', u\\'why\\', u\\'how\\', u\\'all\\', u\\'any\\', u\\'both\\', u\\'each\\', u\\'few\\', u\\'more\\', u\\'most\\', u\\'other\\', u\\'some\\', u\\'such\\', u\\'no\\', u\\'nor\\', u\\'not\\', u\\'only\\', u\\'own\\', u\\'same\\', u\\'so\\', u\\'than\\', u\\'too\\', u\\'very\\', u\\'s\\', u\\'t\\', u\\'can\\', u\\'will\\', u\\'just\\', u\\'don\\', u\"don\\'t\", u\\'should\\', u\"should\\'ve\", u\\'now\\', u\\'d\\', u\\'ll\\', u\\'m\\', u\\'o\\', u\\'re\\', u\\'ve\\', u\\'y\\', u\\'ain\\', u\\'aren\\', u\"aren\\'t\", u\\'couldn\\', u\"couldn\\'t\", u\\'didn\\', u\"didn\\'t\", u\\'doesn\\', u\"doesn\\'t\", u\\'hadn\\', u\"hadn\\'t\", u\\'hasn\\', u\"hasn\\'t\", u\\'haven\\', u\"haven\\'t\", u\\'isn\\', u\"isn\\'t\", u\\'ma\\', u\\'mightn\\', u\"mightn\\'t\", u\\'mustn\\', u\"mustn\\'t\", u\\'needn\\', u\"needn\\'t\", u\\'shan\\', u\"shan\\'t\", u\\'shouldn\\', u\"shouldn\\'t\", u\\'wasn\\', u\"wasn\\'t\", u\\'weren\\', u\"weren\\'t\", u\\'won\\', u\"won\\'t\", u\\'wouldn\\', u\"wouldn\\'t\"]All of these words are lowercase by default. You should be aware that string objects must exactly match to return a true Boolean variable when comparing two individual strings. To put this more plainly, if we were to execute the code “you” == “YOU” ,  the Python interpreter returns false. The specific instance in which this affects our example can be observed by executing the mistake()  and advised_preprocessing()  functions, respectively. Observe the following outputs:Chapter 3  Working With\\xa0raW text48[\\'I\\', \\'student\\', \\'University\\', \\'Alabama\\', \\'.\\', \\'I\\', \\'born\\', \\'Ontario\\', \\',\\', \\'Canada\\', \\'I\\', \\'huge\\', \\'fan\\', \\'United\\', \\'States\\', \\'.\\', \\'I\\', \\'going\\', \\'get\\', \\'degree\\', \\'Philosophy\\', \\'improve\\', \\'chances\\', \\'becoming\\', \\'Philosophy\\', \\'professor\\', \\'.\\', \\'I\\', \\'working\\', \\'towards\\', \\'goal\\', \\'4\\', \\'years\\', \\'.\\', \\'I\\', \\'currently\\', \\'enrolled\\', \\'PhD\\', \\'program\\', \\'.\\', \\'It\\', \\'difficult\\', \\',\\', \\'I\\', \\'confident\\', \\'good\\', \\'decision\\'][\\'student\\', \\'University\\', \\'Alabama\\', \\'.\\', \\'born\\', \\'Ontario\\', \\',\\', \\'Canada\\', \\'huge\\', \\'fan\\', \\'United\\', \\'States\\', \\'.\\', \\'going\\', \\'get\\', \\'degree\\', \\'Philosophy\\', \\'improve\\', \\'chances\\', \\'becoming\\', \\'Philosophy\\', \\'professor\\', \\'.\\', \\'working\\', \\'towards\\', \\'goal\\', \\'4\\', \\'years\\', \\'.\\', \\'currently\\', \\'enrolled\\', \\'PhD\\', \\'program\\', \\'.\\', \\'difficult\\', \\',\\', \\'confident\\', \\'good\\', \\'decision\\']As you can see, the mistake()  function does not catch the uppercase “I” characters, meaning that there are several stop words still in the text. This is solved by uppercasing all the stop words and then evaluating whether each uppercase word in the sample text was in the stop_words  list. This is exemplified with the following two lines of code:stop_words = [word.upper() for word in stopwords.words(\\'english\\')]word_tokens = [word for word in sample_word_tokens if word.upper() not in stop_words]Although embedded methods in feature extraction algorithms likely account for this case, you should be aware that strings must match exactly, and you must account for this when preprocessing manually.That said, there is junk data that you should be aware of—specifically, the grammatical characters. You will be relieved to hear that the word_tokenize()  function also categorizes colons and semicolons as individual Chapter 3  Working With\\xa0raW text49word tokens, but you still have to get rid of them. Thankfully, NLTK contains another tokenizer worth knowing about, which is defined and utilized in the following code:from nltk.tokenize import RegexpTokenizertokenizer = RegexpTokenizer(r\\'\\\\w+\\')sample_word_tokens = tokenizer.tokenize(str(sample_word_tokens))sample_word_tokens = [word.lower() for word in sample_word_tokens]When we print the sample_word_tokens  variable, we get the following output:[\\'student\\', \\'university\\', \\'alabama\\', \\'born\\', \\'ontario\\', \\'canada\\', \\'huge\\', \\'fan\\', \\'united\\', \\'states\\', \\'going\\', \\'get\\', \\'degree\\', \\'philosophy\\', \\'improve\\', \\'chances\\', \\'becoming\\', \\'philosophy\\', \\'professor\\', \\'working\\', \\'towards\\', \\'goal\\', \\'4\\', \\'years\\', \\'currently\\', \\'enrolled\\', \\'phd\\', \\'program\\', \\'difficult\\', \\'confident\\', \\'good\\', \\'decision\\']In the course of this example, we have reached the final step! We have removed all the standard stop words, as well as all grammatical tokens. This is an example of a document that is ready for feature extraction, whereupon some additional preprocessing may occur.Next, I’ll discuss some of the various feature extraction algorithms. And let’s work on denser sample data alongside a preprocessed example paragraph.Chapter 3  Working With\\xa0raW text50 The Bag-of-Words Model (BoW)A BoW model is one of the more simplistic feature extraction algorithms that you will come across. The name “bag-of-words” comes from the algorithm simply seeking to know the number of times a given word is present within a body of text. The order or context of the words is not analyzed here. Similarly, if we have a bag filled with six pencils, eight pens, and four notebooks, the algorithm merely cares about recording the number of each of these objects, not the order in which they are found, or their orientation.Here, I have defined a sample bag-of-words function.def bag_of_words(text):     _bag_of_words = [collections.Counter(re.findall(r\\'\\\\w+\\', word)) for word in text]    bag_of_words = sum(_bag_of_words, collections.Counter())    return bag_of_wordssample_word_tokens_bow = bag_of_words(text=sample_word_tokens)print(sample_word_tokens_bow)When we execute the preceding code, we get the following output:Counter({\\'philosophy\\': 2, \\'program\\': 1, \\'chances\\': 1, \\'years\\': 1,  \\'states\\': 1, \\'born\\': 1, \\'towards\\': 1, \\'canada\\': 1, \\'huge\\': 1,  \\'united\\': 1, \\'goal\\': 1, \\'working\\': 1, \\'decision\\': 1, \\'currently\\': 1, \\'confident\\': 1, \\'going\\': 1, \\'4\\': 1, \\'difficult\\': 1, \\'good\\': 1, \\'degree\\': 1, \\'get\\': 1, \\'becoming\\': 1,  \\'phd\\': 1, \\'ontario\\': 1, \\'fan\\': 1, \\'student\\': 1, \\'improve\\': 1, \\'professor\\': 1, \\'enrolled\\': 1, \\'alabama\\': 1, \\'university\\': 1})This is an example of a BoW model when presented as a dictionary. Obviously, this is not a suitable input format for a machine learning algorithm. This brings me to discuss the myriad of text preprocessing Chapter 3  Working With\\xa0raW text51functions available in the scikit-learn library, which is a Python library that all data scientists and machine learning engineers should be familiar with. For those who are new to it, this library provides implementations of machine learning algorithms, as well as several data preprocessing algorithms. Although we won’t walk through much of this package, the text preprocessing functions are extremely useful. CountVectorizerLet’s start by walking through the BoW equivalent—CountVectorizer, an implementation of bag-of-words in which we code text data as a representation of features/words. The values of each of these features represent the occurrence counts of words across all documents. If you recall, we defined a sample_sent_tokens  variable, which we will analyze. We define a bow_sklearn()  function beneath where we preprocess our data. The function is defined as follows:from sklearn.feature_extraction.text import CountVectorizerdef bow_sklearn(text=sample_sent_tokens):     c = CountVectorizer(stop_words=\\'english\\',  token_pattern=r\\'\\\\w+\\')    converted_data = c.fit_transform(text).todense()    print(converted_data.shape)    return converted_data, c.get_feature_names()To provide context, in this example, we are assuming that each sentence is an individual document, and we are creating a feature set in which each feature is an individual token. When we instantiate CountVectorizer() , we set two parameters: stop_words , and token_pattern . These two arguments are the embedded methods in the feature extraction that remove stop words and grammatical tokens. The fit_transform()  attribute expects to receive a list, an array, or a similar object of iterable string objects. We assign the bow_data  and feature_names  variables to the data that the  Chapter 3  Working With\\xa0raW text52bow_sklearn()  returns, respectively. Our converted data set is a 6 × 50 matrix, which means that we have six sentences, all of which have 50 features. Observe our data set and feature names, respectively, in the following outputs:[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0] [0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0] [0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1] [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]][u\\'4\\', u\\'alabama\\', u\\'born\\', u\\'canada\\', u\\'chances\\', u\\'confident\\', u\\'currently\\', u\\'decision\\', u\\'degree\\', u\\'difficult\\', u\\'enrolled\\', u\\'fan\\', u\\'goal\\', u\\'going\\', u\\'good\\', u\\'huge\\', u\\'improve\\', u\\'ontario\\', u\\'phd\\', u\\'philosophy\\', u\\'professor\\', u\\'program\\', u\\'states\\', u\\'student\\', u\\'united\\', u\\'university\\', u\\'working\\', u\\'years\\']To extrapolate this example to a larger number of documents, and ostensibly larger vocabulary sizes, our matrices for preprocessed text data tends to have a large number of features, sometimes well over 1000. How to evaluate these features effectively is the machine learning challenge we seek to solve. You typically want to use the bag-of-words feature extraction technique for document classification. Why is this the case? We assume that documents of certain classifications contain certain words. For example, we expect a document referencing political science to perhaps feature jargon such as dialectical materialism  or free market capitalism ; whereas a document that is referring to classical music will have terms such as crescendo , diminuendo , and so forth. In these instances of document classification, the location of the word itself is not terribly important. It’s important to know what portion of the vocabulary is present in one class of document vs. another.Chapter 3  Working With\\xa0raW text53Next, let’s look at our first example problem in the code in the  text_classifiction_demo.py  file. Example Problem 1: Spam DetectionSpam detection is a relatively common task in that most people have an inbox (email, social media instant messenger account, or similar entity) targeted by advertisers or malicious actors. Being able to block unwanted advertisements or malicious files is an important task. Because of this, we are interested in pursuing a machine learning approach to spam detection. Let’s begin by describing the data set before digging into the problem.This data set was downloaded from the UCI Machine Learning Repository, specifically the Text Data section. Our data set consists of 5574 observations—all SMS messages. We observe from our data set that most of the messages are not terribly long. Figure\\xa0 3-1 is a histogram of our entire data set.Figure 3-1.  SMS mes sage length histogramChapter 3  Working With\\xa0raW text54Something else we should be mindful of is the distribution between the class labels, which tends to be heavily skewed. In this data set, 4825 observations are marked as “ham” (being not spam), and 747 are marked as “spam” . You must be vigilant in evaluating your machine learning solutions to ensure that they do not overfit the training data, and then fail miserably on test data.Let’s briefly do some additional data set discovery before we move on to tackling the problem directly. When we look at the header of our data set, we observe the following:0   ham  Go until jurong point, crazy.. Available only ...1   ham                      Ok lar... Joking wif u oni...2  spam  Free entry in 2 a wkly comp to win FA Cup fina...3   ham  U dun say so early hor... U c already then say...4   ham  Nah I don\\'t think he goes to usf, he lives aro...The first column is our categorical label/response variable. The second column comprises text contained within each individual SMS.\\xa0We will use a bag-of-words representation via the CountVectorizer() . Our entire data set has a vocabulary size of 8477 words. The load_spam_data()  function shows that the preprocessing steps mimic the warmup example at the beginning of the chapter.Let’s fit and train our model and evaluate the results. When beginning a classification task, I suggest that you evaluate the results of the logistic regression. This determines if your data is linearly separable or not. If it is, the logistic regression should work fine, which saves you from further model selection and time-consuming hyper-parameter optimization. If it does fail, then you can use those methods.We train a model using both L1 and L2 weight regularization in the text_classifiction_demo.py  file; however, we will walk through the L1 norm regularized example here because it yielded better test results:Chapter 3  Working With\\xa0raW text55#Fitting training algorithml = LogisticRegression(penalty=\\'l1\\')accuracy_scores, auc_scores = [], []Those of you that are not familiar with logistic regression you should learn about elsewhere; however, I will discuss the L1-regularized logistic regression briefly. L1 norm regularization in linear models is standard for LASSO (least absolute shrinkage selection operator), where during the learning process, the L1 norm can theoretically force some regression coefficients to 0. In contrast, the L2 norm, often seen in ridge regression, can force some regression coefficients during the learning process to numbers close to 0. The difference between this is that coefficients that are 0 effectively perform feature selection on our feature set by eliminating them. Mathematically, we represent this regularization via Equation 3.1. minl og ;)iMiIpy x=å-+11(| qb q (3.1)We w ill evaluate the distribution of test scores over several trials. scikit-  learn al gorithms’ fit()  method trains the algorithm of a given data set. As such, all the iterations that optimize the parameters are performed. To see logging information in the training process, set the verbose  parameter to 1.Let’s look at the code that will collect the distribution of both accuracy and AUC scores.for i in range(trials):   if i%10 == 0 and i > 0:        print(\\'Trial \\' + str(i) + \\' out of 100 completed\\')   l.fit(train_x, train_y)   predicted_y_values = l.predict(train_x)    accuracy_scores.append(accuracy_score(train_y, predicted_y_values))Chapter 3  Working With\\xa0raW text56    fpr, tpr = roc_curve(train_y, predicted_y_values)[0],  roc_curve(train_y, predicted_y_values)[1]   auc_scores.append(auc(fpr, tpr))scikit-learn performs cross-validation so long as you define a random seed utilizing the np.random.seed()  function, which we do near the beginning of the file. During each trial, we are fitting the data set to the algorithm, predicting the accuracy and AUC score, and appending them to a list that we defined. When we evaluate our results from training, we observe the following:Summary Statistics (AUC):        Min       Max      Mean      SDev    Range0  0.965348   0.968378   0.967126   0.000882   0.00303Summary Statistics (Accuracy Scores):        Min      Max      Mean      SDev     Range0  0.990356   0.99116  0.990828   0.000234   0.000804Test Model Accuracy: 0.9744426318651441Test True Positive Rate: 0.8412698412698413Test False Positive Rate: 0.004410838059231254[[1580    7] [  40  212]]Fortunately, we see that logistic regression performs excellently on this problem. We have excellent accuracy and AUC scores, with very little variance from one trial to the next. Let’s evaluate the AUC score, as shown in Figure\\xa0 3-2.Chapter 3  Working With\\xa0raW text57Our test AUC score is 0.92. This algorithm would be deployable in an application to test for spam results. In the course of solution discovery, I suggest that you use this model rather than others. Although you are encouraged to find other methods, I observed that the gradient-boosted classification tree and random forests performed considerably worse, with AUC scores of roughly 0.72. Let’s discuss a more sophisticated term frequency scheme. Term Frequency Inverse Document FrequencyTerm frequency–inverse document frequency  (TFIDF) is based on BoW, but provides more detail than simply taking term frequency, as was done in the prior example. TFIDF yields a value that shows how important a given word is by not only looking at term frequency, but also analyzing how many times the word appears across all documents. The first portion, term frequency, is relatively straightforward.Figure 3-2.  Test set ROC curveChapter 3  Working With\\xa0raW text58Let’s look at an example to see how to calculate TFIDF .\\xa0We define a new body of text and use the sample text defined at the beginning of the chapter, as follows:text = “‘I was a student at the University of Pennsylvania, but now work onWall Street as a Lawyer. I have been living in New\\xa0York for roughly five yearsnow, however I am looking forward to eventually retiring to Texas once I havesaved up enough money to do so. ”’document_list = list([sample_text, text])Now that we have a list of documents, let’s look at exactly what the TFIDF algorithm does. The first portion, term frequency, has several variants, but we will focus on the standard raw count scheme. We simply sum the terms across all documents. The term frequency is equivalent to Equation 3.2. fftdtdtd,, ¢¢Îå (3.2)ft, d is equal to the frequency of the term across all documents. ftd¢, is equal to the frequency of that same term but within each individual document. In our code, we document these steps in the tf_idf_example()  function, as follows:def tf_idf_example(textblobs=[text, text2]):def term_frequency(word, textblob): (1)Chapter 3  Working With\\xa0raW text59return textblob.words.count(word)/float(len(textblob.words))def document_counter(word, text):return sum(1 for blob in text if word in blob)def idf(word, text): (2)return np.log(len(text) /1 + float(document_counter(word, text)))def tf_idf(word, blob, text):return term_frequency(word, blob) * idf(word, text)output = list()for i, blob in enumerate(textblobs):output.append({word: tf_idf(word, blob, textblobs) for word in blob.words})print(output)Thanks to the TextBlob package, we are able to fairly quickly re-create the TFIDF toy implementation. I will address each of the functions within the tf_idf_example()  function. You are aware of term frequency, so I can discuss inverse document frequency. We define inverse document frequency as a measure of how frequently a word appears across the entire corpus. Mathematically, this relationship is expressed in Equation 3.3. idf,tDNdD td() =ÎÎ{}log:  (3.3)This e quation calculates the log of the total number of documents in our corpus, divided by all the documents in which the term that we are evaluating appears. In our code, we calculate this with the function (2). Now, we are ready to proceed to the final step of the algorithm, which is multiplying the term frequency by the inverse document frequency, as shown in the preceding code. We then yield the following output:Chapter 3  Working With\\xa0raW text60 [{\\'up\\': \\'0.027725887222397813\\', \\'money\\': \\'0.021972245773362195\\', \\'am\\': \\'0.027725887222397813\\', \\'years\\': \\'0.027725887222397813\\', \\'as\\': \\'0.027725887222397813\\', \\'at\\': \\'0.027725887222397813\\', \\'have\\': \\'0.055451774444795626\\', \\'in\\': \\'0.027725887222397813\\', \\'New\\': \\'0.021972245773362195\\', \\'saved\\': \\'0.021972245773362195\\', \\'Texas\\': \\'0.021972245773362195\\', \\'living\\': \\'0.021972245773362195\\', \\'for\\': \\'0.027725887222397813\\', \\'to\\': \\'0.08317766166719343\\', \\'retiring\\': \\'0.027725887222397813\\', \\'been\\': \\'0.021972245773362195\\', \\'looking\\': \\'0.021972245773362195\\', \\'Pennsylvania\\': \\'0.021972245773362195\\', \\'enough\\': \\'0.021972245773362195\\', \\'York\\': \\'0.021972245773362195\\', \\'forward\\': \\'0.027725887222397813\\', \\'was\\': \\'0.027725887222397813\\', \\'eventually\\': \\'0.021972245773362195\\', \\'do\\': \\'0.027725887222397813\\', \\'I\\': \\'0.11090354888959125\\', \\'University\\': \\'0.027725887222397813\\', \\'however\\': \\'0.027725887222397813\\', \\'but\\': \\'0.021972245773362195\\', \\'five\\': \\'0.021972245773362195\\', \\'student\\': \\'0.021972245773362195\\', \\'now\\': \\'0.04394449154672439\\', \\'a\\': \\'0.055451774444795626\\', \\'on\\': \\'0.027725887222397813\\', \\'Wall\\': \\'0.021972245773362195\\', \\'of\\': \\'0.027725887222397813\\', \\'work\\': \\'0.021972245773362195\\', \\'roughly\\': \\'0.021972245773362195\\', \\'Street\\': \\'0.021972245773362195\\', \\'so\\': \\'0.021972245773362195\\', \\'Lawyer\\': \\'0.021972245773362195\\', \\'the\\': \\'0.027725887222397813\\', \\'once\\': \\'0.021972245773362195\\'}, {\\'and\\': \\'0.0207285337484549\\', \\'is\\': \\'0.0207285337484549\\', \\'each\\': \\'0.0207285337484549\\', \\'am\\': \\'0.026156497379620575\\', \\'years\\': \\'0.026156497379620575\\', \\'have\\': \\'0.05231299475924115\\', \\'in\\': \\'0.026156497379620575\\', \\'children\\': \\'0.0414570674969098\\', \\'considering\\': \\'0.0207285337484549\\', \\'retirement\\': \\'0.0207285337484549\\', \\'doctor\\': \\'0.0207285337484549\\', \\'retiring\\': Chapter 3  Working With\\xa0raW text61\\'0.026156497379620575\\', \\'two\\': \\'0.0207285337484549\\', \\'long\\': \\'0.0207285337484549\\', \\'next\\': \\'0.0207285337484549\\', \\'to\\': \\'0.05231299475924115\\', \\'forward\\': \\'0.026156497379620575\\', \\'was\\': \\'0.026156497379620575\\', \\'couple\\': \\'0.0207285337484549\\', \\'more\\': \\'0.0207285337484549\\', \\'ago\\': \\'0.0207285337484549\\', \\'them\\': \\'0.0207285337484549\\', \\'that\\': \\'0.0207285337484549\\', \\'I\\': \\'0.1046259895184823\\', \\'University\\': \\'0.026156497379620575\\', \\'who\\': \\'0.0414570674969098\\', \\'however\\': \\'0.026156497379620575\\', \\'quite\\': \\'0.0207285337484549\\', \\'me\\': \\'0.0207285337484549\\', \\'Yale\\': \\'0.0207285337484549\\', \\'with\\': \\'0.0207285337484549\\', \\'the\\': \\'0.05231299475924115\\', \\'a\\': \\'0.07846949213886173\\', \\'both\\': \\'0.0207285337484549\\', \\'look\\': \\'0.026156497379620575\\', \\'of\\': \\'0.026156497379620575\\', \\'grandfather\\': \\'0.0207285337484549\\', \\'spending\\': \\'0.0207285337484549\\', \\'three\\': \\'0.0207285337484549\\', \\'time\\': \\'0.0414570674969098\\', \\'making\\': \\'0.0207285337484549\\', \\'went\\': \\'0.0207285337484549\\'}]This brings us to the end of our toy example using TFIDF .\\xa0Before we jump into the example, let’s review how we would utilize this example in scikit-learn, such that we can input this data into a machine learning algorithm. Similar to CountVectorizer() , scikit-learn has provided a TfidfVectorizer()  method that comes in handy. The following shows its utilization. I will dive into a deeper use of its preprocessing methods later.def tf_idf_sklearn(document=document_list):     t = TfidfVectorizer(stop_words=\\'english\\',  token_pattern=r\\'\\\\w+\\')    x = t.fit_transform(document_list).todense()    print(x)Chapter 3  Working With\\xa0raW text62When we execute the function, it yields the following result:[[0.         0.         0.         0.         0.         0.24235766  0.17243947 0.          0.24235766 0.24235766 0.          0.  0.24235766 0.          0.24235766 0.24235766 0.24235766 0.  0.         0.17243947 0.24235766 0.24235766 0.          0.24235766  0.24235766 0.24235766 0.          0.17243947 0.24235766 0.  0.24235766 0.          0.17243947 0.24235766] [0.20840129 0.41680258 0.20840129 0.20840129 0.20840129 0.  0.14827924 0.20840129 0.          0.         0.20840129 0.20840129  0.         0.20840129 0.          0.         0.         0.20840129  0.20840129 0.14827924 0.          0.         0.20840129 0.  0.         0.         0.41680258 0.14827924 0.          0.20840129  0.         0.20840129 0.14827924 0.         ]]This function yields a 2 × 44 matrix, and it is ready for input into a machine learning algorithm for evaluation.Now let’s work through another example problem using TFIDF as our feature extractor while utilizing another machine learning algorithm as we did for the BoW feature extraction. Example Problem 2: Classifying Movie ReviewsWe obtained the following IMDB movie review data set from http://www.cs.cornell.edu/people/pabo/movie-review-data/ .We are going to work with the raw text directly, rather than using preprocessed text data sets often provided via various machine learning packages.Let’s take a snapshot of the data.tristar / 1 : 30 / 1997 / r ( language , violence , dennis rodman ) cast : jean-claude van damme ; mickey rourke ; dennis rodman ; natacha lindinger ; paul freeman director : tsui hark Chapter 3  Working With\\xa0raW text63screenplay : dan jakoby ; paul mones ripe with explosions , mass death and really weird hairdos , tsui hark\\'s \" double team \" must be the result of a tipsy hollywood power lunch that decided jean-claude van damme needs another notch on his bad movie-bedpost and nba superstar dennis rodman should have an acting career . actually , in \" double team , \" neither\\'s performance is all that bad . i\\'ve always been the one critic to defend van damme -- he possesses a high charisma level that some genre stars ( namely steven seagal ) never aim for ; it\\'s just that he\\'s never made a movie so exuberantly witty since 1994\\'s \" timecop . \" and rodman . . . well , he\\'s pretty much rodman . he\\'s extremely colorful , and therefore he pretty much fits his role to a t , even if the role is that of an ex-ciaAs you can see, this data is filled with lots of grammatical noise that we will need to remove, but is also rich in descriptive text. We will opt to use the TfidfVectorizer()  method on this data.First, I would like to direct you to two functions at the beginning of  the file:def remove_non_ascii(text):    return \".join([word for word in text if ord(word) < 128])Notice that we are using the native Python function ord() . This function expects a string, and it returns either the Unicode point for Unicode objects or the value of the byte. If the ord()  function returns an integer less than 128, this poses no problem for our preprocesser and therefore we keep the string in question; otherwise, we remove the character. We end this step by joining all the remaining words back together with the \".join()  function. The reasoning for preprocessing during data preparation is that our text preprocessor expects Unicode objects when being fed to it. When we are capturing raw text data, particularly if it is from an HTML page, many of the string objects Chapter 3  Working With\\xa0raW text64loaded before preprocessing and removal of stop words will not be Unicode-compatible.Let’s look at the function that loads our data.def load_data():     negative_review_strings = os.listdir(\\'/Users/tawehbeysolow/Downloads/review_data/tokens/neg\\')     positive_review_strings =  os.listdir(\\'/Users/tawehbeysolow/Downloads/review_data/tokens/pos\\')    negative_reviews, positive_reviews = [], []We start by loading the file names of all the .txt  files to be processed. To do this, we use the os.listdir()  function. I suggest you use this function when building similar applications that require preprocessing a large number of files.Next, we load our files with the open()  function, and then apply the remove_non_ascii()  function, as follows:    for positive_review in positive_review_strings:         with open(\\'/Users/tawehbeysolow/Downloads/review_data/tokens/pos/\\'+str(positive_review), \\'r\\') as positive_file:             positive_reviews.append(remove_non_ascii(positive_file.read()))    for negative_review in negative_review_strings:         with open(\\'/Users/tawehbeysolow/Downloads/review_data/tokens/neg/\\'+str(negative_review), \\'r\\') as negative_file:             negative_reviews.append(remove_non_ascii(negative_file.read()))With our initial preprocessing done, we end by concatenating both the positive and negative reviews, in addition to the respective vectors that contain their labels. Now, we can get to the meat and potatoes of this machine learning problem, starting with the train_logistic_model()  Chapter 3  Working With\\xa0raW text65function. In a similar fashion, we use logistic regression as the baseline for the problem. Although most of the following functions are similar in structure to Example Problem 1, let’s look at the beginning of this function to analyze what we have changed.#Load and preprocess text datax, y = load_data()t = TfidfVectorizer(min_df=10, max_df=300, stop_words=\\'english\\', token_pattern=r\\'\\\\w+\\')x = t.fit_transform(x).todense()We are utilizing two new arguments: min_df  corresponds to the minimum document frequency to retain a word, and max_df  refers to the maximum amount of documents that a word can appear in before it is omitted from the sparse matrix that we create. When increasing the maximum and minimum document frequencies, I noticed that the L1 penalty model performed better than the L2 penalty model. I would posit that this is likely due to the fact that as we increase the min_df  parameter, we are creating a considerably sparser matrix than if we had a denser matrix. You should keep this in mind so as to not overselect features if they performed any feature selection on their matrices beforehand.Let’s evaluate the results of the logistic regression, as shown in the following output (also see Figures\\xa0 3-3 and 3-4).Summary Statistics from Training Set (AUC):       Mean       Max  Range      Mean  SDev0  0.723874   0.723874     0.0  0.723874    0.0Summary Statistics from Training Set (Accuracy):       Mean       Max  Range      Mean  SDev0  0.726788   0.726788     0.0  0.726788    0.0Training Data Confusion Matrix:[[272 186] [ 70 409]]Chapter 3  Working With\\xa0raW text66Summary Statistics from Test Set (AUC):       Mean       Max  Range      Mean  SDev0  0.723874   0.723874     0.0  0.723874    0.0Summary Statistics from Test Set (Accuracy):        Mean       Max  Range      Mean  SDev0  0.726788   0.726788     0.0  0.726788    0.0Test Data Confusion Matrix:[[272 186] [ 70 409]]Summary Statistics from Training Set (AUC):       Mean       Max  Range      Mean  SDev0  0.981824   0.981824     0.0  0.981824    0.0Summary Statistics from Training Set (Accuracy):       Mean       Max  Range      Mean  SDev0  0.981857   0.981857     0.0  0.981857    0.0Training Data Confusion Matrix:[[449   9] [  8 471]]Summary Statistics from Test Set (AUC):       Mean       Max  Range      Mean  SDev0  0.981824   0.981824     0.0  0.981824    0.0Summary Statistics from Test Set (Accuracy):        Mean       Max  Range      Mean  SDev0  0.981857   0.981857     0.0  0.981857    0.0Test Data Confusion Matrix:[[449   9] [  8 471]]Chapter 3  Working With\\xa0raW text67Both in training and performance, logistic regression performs considerably better when utilizing the L2 weight regularization method, given the parameters we used for the TfidfVectorizer()  feature extraction algorithm.Figure 3-3.  L1 logistic regression test set ROC curveChapter 3  Working With\\xa0raW text68I created multiple solutions to evaluate: a random forest classifier, a naïve Bayes classifier, and a multilayer perceptron. We begin with a general overview of all of our methods and their respective orientations.Starting with the multilayer perceptron in the mlp_movie_classification_model.py  file, notice that much of the neural network is the same as the example in Chapter 2, with the exception of an extra hidden layer. That said, I would like to direct your attention to lines 92 through 94.regularization = tf.contrib.layers.l2_regularizer(scale=0.0005, scope=None)regularization_penalty = tf.contrib.layers.apply_regularization(regularization, weights.values())cross_entropy = cross_entropy + regularization_penaltyFigure 3-4.  L2 logi stic regression test set ROC curveChapter 3  Working With\\xa0raW text69In these lines, we are performing weight regularization, as discussed earlier in this chapter with the logistic regression L2 and L1 loss parameters. Those of you who wish to apply this in TensorFlow can rest assured that these are the only modifications needed to add a weight penalty to your neural network. While developing this solution, I tried weight regularization utilizing L1 and L2 loss penalties, and I experimented with dropout. Weight regularization is the process of limiting the scope to which the weights can grow when utilizing different vector norms. The two most referenced norms for weight regularization are L1 and L2 norms. The following are their respective equations, which are also illustrated in Figure\\xa0 3-5. L1===å v111iNiv  L2===å v212iNiv Figure 3-5.  L1 and L2 norm visualizationChapter 3  Working With\\xa0raW text70When initially utilizing both one and two hidden layer(s), I noticed  that both the test and training performance were considerably worse  with dropout, even using dropout percentages as low as 0.05. As such,  I cannot suggest that you utilize dropout for this problem. As for weight regularization, additional parameter selection is not advisable; however, I found negligible differences with L1 vs. L2 regularization. The confusion matrix and the ROC curve are shown in Figure\\xa0 3-6.Test Set Accuracy Score: 0.8285714285714286Test Set Confusion Matrix:[[122  26] [ 22 110]]Figure 3-6.  ROC curve for multilayer perceptronChapter 3  Working With\\xa0raW text71Let’s analyze the choice of parameters for the random forest and naïve Bayes classifiers. We kept our trees relatively short at a max_depth  of ten splits. As for the naïve Bayes classifier, the only parameter we chose is alpha, which we set to 0.005. Let’s evaluate Figures\\xa0 3-6 and 3-7 for the results of the model.The Figure\\xa0 3-8 shows the result of a naïve Bayes classifier.Figure 3-7.  ROC curve for random forestChapter 3  Working With\\xa0raW text72Summary Statistics from Training Set Random Forest (AUC):       Mean       Max  Range      Mean  SDev0  0.987991   0.987991     0.0  0.987991    0.0Summary Statistics from Training Set Random Forest (Accuracy):      Mean      Max  Range     Mean  SDev0  0.98826  0.98826    0.0  0.98826   0.0Training Data Confusion Matrix (Random Forest):[[447  11] [  0 479]]Summary Statistics from Training Set Naive Bayes (AUC):       Mean       Max  Range      Mean          SDev0  0.965362   0.965362     0.0  0.965362   2.220446e-16Figure 3-8.  ROC curve for naïve Bayes classifierChapter 3  Working With\\xa0raW text73Summary Statistics from Training Set Naive Bayes (Accuracy):       Mean       Max  Range      Mean          SDev0  0.964781   0.964781     0.0  0.964781   3.330669e-16Training Data Confusion Matrix (Naive Bayes):[[454   4] [ 29 450]]Test Data Confusion Matrix:[[189  27] [ 49 197]]Test Data Confusion Matrix (Random Forest):[[162  54] [ 19 227]]When evaluating the results, the neural network has a tendency to overfit to training data, but its test performance is very similar to logistic regression, although slightly less accurate. When assessing the results of the naïve Bayes classifier and the random forest classifier, we observe roughly similar AUC scores, with only a difference in false positives and true positives as the trade-off that we must accept. In this instance, it is important to consider our objective.If we are using these algorithms to label the reviews that users input, and then perform analytics on top of these reviews, we want to maximize the accuracy rate, or seek models with the highest true positive rate and true negative rate. In the instance of spam detection, we likely want the model that has the best ability to properly classify spam from normal mail.I have introduced and applied the bag-of-words schemes in both the logistic model and the naïve Bayes classifier. This brings us to the final part of this section, in which I discuss their relative advantages and disadvantages. You should be aware of this so as to not waste time altering subpar solutions. The major advantage of BoW is that it is a relatively straightforward algorithm that allows you to quickly turn text into a Chapter 3  Working With\\xa0raW text74format interpretable by a machine learning algorithm, and to attack NLP problems directly.The largest disadvantage of BoW is its relative simplicity. BoW does not account for the context of words, and as such, it does not make it the ideal feature extraction method for more complex NLP tasks. For example, “4” and “four” are considered semantically indistinguishable, but in BoW, they are considered two different words altogether. When we expand this to phrases, “I went to college for four years, ” and “For 4 years, I attended a university, ” are treated as orthogonal vectors. Another example of a BoW shortcoming is that it cannot distinguish the ordering of words. As such,  “I am stupid” and “ Am I stupid” appear as the same vector.Because of these shortcomings, it is appropriate for us to utilize more advanced models, such as word embeddings, for these difficult problems, which are discussed in detail in the next chapter. SummaryThis brings us to the end of Chapter 3! This chapter tackled working with text data in document classification problems. You also became familiar with two BoW feature extraction methods.Let’s take a moment to go over some of the most important lessons from this chapter. Just as with traditional machine learning, you must define the type of problem and analyze data. Is this simply document classification? Are we trying to find synonyms? We have to answer these questions before tackling any other steps.The removal of stop words, grammatical tokens, and frequent words improves the accuracy of our algorithms. Not every word in a document is informative, so you should know how to remove the noise. That said, over-  sele cting features can be detrimental to our model’s success, so you should be aware of this too!Chapter 3  Working With\\xa0raW text75Whenever you are working with a machine learning problem, within or outside the NLP domain, you must establish a baseline solution and then improve if necessary!  I suggest that you always start a deep learning problem by seeing how the solution appears, such as with a logistic regression. Although it is my goal to teach you how to apply deep learning to NLP-based problems, there is no reason to use overly complex methods where less complex methods will do better or equally as well (unless you like to practice your deep learning skills).Finally, while preprocessing methods are useful, BoW-based models are best utilized with document classification. For more advanced NLP problems, such as sentiment analysis, understanding semantics, and similarly abstract problems, BoW likely will not yield the best results.Chapter 3  Working With\\xa0raW text77© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5_4CHAPTER 4Topic Modeling and\\xa0 Word EmbeddingsNow that you have had an introduction to working with text data, let’s dive into one of the more advanced feature extraction algorithms. To accomplish some of the more difficult problems, it is reasonable for me to introduce you to other techniques to approach NLP problems. We will move through Word2Vec, Doc2Vec, and GloVe. Topic Model and\\xa0Latent Dirichlet  Allocation (LDA)Topic models are a method of extracting information from bodies of text to see what “topics” occur across all the documents. The intuition is that we expect certain topics to appear more in relevant documents and not as much in irrelevant documents. This might be useful when using the topics we associate with a document as keywords for better and more intuitive search, or when using it for shorthand summarization. Before we apply this model, let’s talk about how we actually extract topics.78Latent Dirichlet allocation (LDA) is a generative model developed in 2003 by David Blei, Andrew Ng, and Michael I.\\xa0Jordan. In their paper, they highlight the shortcomings of TFIDF . Most notably, TFIDF is unable to understand the semantics of words, or the position of a word in text. This led to the rise of LDA.\\xa0LDA is a generative model, meaning that it outputs all the possible outcomes for a given phenomenon. Mathematically, we can describe the assumptions as follows: 1. Choose N~Poisson (ξ) (a sequence of N words within a document have a Poisson distribution) 2. Choose θ~Dir(α) (a parameter θ has a Dirichlet distribution) 3. For each of the N words ( wn):• Choose topic zn~Multinomial (θ) (Each topic zn has a multinomial distribution.)• Choose wn from p (wn |\\xa0zn, β), a multinomial probability conditional on topic zn. (Each topic is represented as a distribution over words, where a probability is generated from the probability of the nth word, conditional upon the topic as well as β where βij\\xa0=\\xa0p(wj\\xa0=\\xa01| zi\\xa0=\\xa01) with dimensions k\\xa0x\\xa0V . ) β = probability of a given word, V = number of words in the vocabulary, k = the dimensionality of the Dirichlet distribution, θ = the random variable sampled from the probability simplex.Let’s discuss some of the distributions utilized in these assumptions. The Poisson distribution represents events that occur in a fixed time or space and at a constant rate, independently of the time since the last event. An example of this distribution is a model of the number of people who call a pizzeria for delivery during a given period of time. The multinomial Chapter 4  topiC Modeling and\\xa0Word eMbeddings79distribution is the k-outcome generalization of the binomial distribution; in other words, the same concept as the binomial distribution but expanded to cases where there are more than two outcomes.Finally, the Dirichlet distribution is a generalization of the beta distribution, but expanded to handle multivariate data. The beta distribution is a distribution of probabilities.LDA assumes that (1) words are generated from topics, which have fixed conditional distributions, and (2) that the topics within a document are infinitely exchangeable, which means that the joint probability distribution of these topics is not affected  by the order in which they are represented. Reviewing statements 1 and 2 allows us to state that words within a topic are not  infinitely exchangeable.Let’s discuss parameter θ (drawn from the Dirichlet distribution), which the dimensionality of the distribution, k, is utilized. We assume that k is known and fixed, and that k-dimensional Dirichlet random variable θ can take any values in the ( k\\xa0−\\xa01) probability simplex. Here, we define the probability simplex as the area of the distribution that we draw the random variable from, graphically represented as a multidimensional triangle with k + 1 vertices. The probability distribution on the simplex itself can be represented as follows: pikiikik qaaaqqaa|() =()()æèçççöø÷÷÷¼==-- åÕG1111111G,, (4.1)α = k-vector of positive real valued numbers. Γ(x) = gamma function.Subsequently, we define the joint distribution of a mixture of topics,  as follows: pp pz pw znNnn n (, ,| (| || qa bq aq b zw ,) ), = () ()=Õ1  (4.2)Chapter 4  topiC Modeling and\\xa0Word eMbeddings80Therefore, a given sequence of words and topics must have the following form: pp pz pw znNnn n wz,( |( | () =ò()æèçöø÷=Õ qq1) (4.3)In the LDA paper, the authors provide a useful illustration for Equations 4.1, 4.2, and 4.3, as shown in Figure\\xa0 4-1.The example given in the LDA paper describes Figure\\xa0 4-1 as an illustration of a topic simplex inside a word simplex comprised of three words. Each of the simplex points represent a given word and topic, respectively.Figure 4-1.  Topic and word simplexesChapter 4  topiC Modeling and\\xa0Word eMbeddings81Before we complete our discussion on the theory behind LDA, let’s re-create all the work in Python. Thankfully, scikit-learn provides an implementation of LDA that we will utilize in the upcoming example. Topic Modeling with\\xa0LDA on\\xa0Movie Review DataNext, we look at the same movie review data that we used in our document classification example. The following is an example of some of the code that we will utilize to first create out topic model. We’ll start with an implementation in sklearn.def create_topic_model(model, n_topics=10, max_iter=5, min_df=10, max_df=300, stop_words=\\'english\\', token_pattern=r\\'\\\\w+\\'):    print(model + \\' topic model: \\\\n\\')    data = load_data()[0]    if model == \\'tf\\':         feature_extractor = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words, token_pattern=r\\'\\\\w+\\')    else:         feature_extractor = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words, token_pattern=r\\'\\\\w+\\')    processed_data = feature_extractor.fit_transform(data)We load the movie reviews that we used in Chapter 3 for the classification problem. In this example, we will imagine that we want to make a topic model for a given number of movie reviews.Note  We are importing the load_data()  function from a previous file. to execute the lda_demo.py file , use a relative import from the code_applied_nlp_python  directory, and execute the following command: \\'python –m chapter4.topic_modeling\\'Chapter 4  topiC Modeling and\\xa0Word eMbeddings82We load the data in with our function and then prepare it for input to the LDA fit_transform()  method. Like other NLP problems, we cannot put raw text into any of our algorithms; we must always preprocess it in some form. However, for producing a topic model, we will utilize both the term frequency and the TFIDF algorithms, but mainly to compare results.Let’s move through the rest of the function. lda_model = LatentDirichletAllocation(n_topics=n_topics, learning_method=\\'online\\', learning_offset=50., max_iter=max_iter, verbose=1)lda_model.fit(processed_data)tf_features = feature_extractor.get_feature_names() print_topics(model=lda_model, feature_names=tf_features, n_top_words=n_top_words)When we execute the following function, we get this as our output:tf topic model:Topic #0: libby fugitive douglas sarah jones lee detective double innocent talkTopic #1: beatty joe hanks ryan crystal niro fox mail  kathleen shopTopic #2: wars phantom lucas effects menace neeson jedi anakin special computerTopic #3: willis mercury simon rising jackal bruce ray lynch baseball hughesTopic #4: godzilla broderick redman bvoice kim michael bloomington mission space yorkTopic #5: planet apes joe sci fi space ape alien gorilla newmanTopic #6: d american fun guy family woman day ll james bitTopic #7: bond brosnan bottle message blake theresa pierce tomorrow dies crownChapter 4  topiC Modeling and\\xa0Word eMbeddings83Topic #8: van spielberg amistad gibson en home american kevin ending senseTopic #9: scream 2 wild williamson horror smith kevin arquette sidney finnBeing that this is movie data, we can see that the topics refer to both the movie and the context surrounding it. For example, topic #4 lists “Godzilla” (ostensibly a character) and “Broderick” (ostensibly an actor). We can also produce topic models utilizing other feature extraction methods.Now let’s look at the results of the topic model when we use the TFIDF feature extractor.tfidf topic model:Topic #0: libby driver jacket attending terrorists tends finn doom tough parodiesTopic #1: godzilla beatty douglas arthur christ technology burns jesus york casesTopic #2: wars lucas episode menace jar niro jedi darth anakin phantomTopic #3: sub theron genre keaton cooper victor rita irene dating rulesTopic #4: midnight kim stiller mulan spice newman disney junkie troopers strangeTopic #5: clooney palma kevin pacino snake home toy woody pfeiffer spaceTopic #6: anna disney jude carpenter men wrong siege lee king familyTopic #7: scream got mail bond hanks book performances summer cute deweyTopic #8: en van z n er met reese die fallen louTopic #9: family american effects home guy woman michael original 10 jamesChapter 4  topiC Modeling and\\xa0Word eMbeddings84There are similar results, although we get slightly different results for some of the topics. In some ways, the TFIDF model can be less interpretable than the term-frequency model.Before we move forward, let’s discuss how to utilize the LDA model with a new package. Gensim is a machine learning library that is heavily focused on applying machine learning and deep learning to NLP tasks. The following is code that utilizes this package in the gensim_topic_model()  function:def gensim_topic_model():    def remove_stop_words(text): (1)        word_tokens = word_tokenize(text.lower())         word_tokens = [word for word in word_tokens if word not in stop_words and re.match(\\'[a-zA-Z\\\\-][a-zA-Z\\\\-]{2,}\\', word)]        return word_tokens    data = load_data()[0]     cleaned_data = [remove_stop_words(data[i]) for i in range(0, len(data))]When us ing this package, the Gensim LDA implementation expects a different input than the gensim implementation, although it still requires preprocessing. When looking at function, we have to remove stop words using a proprietary function, as we did earlier in Chapter 3. In addition to this, we should be mindful to remove words that appear too frequently, and not frequently enough. Thankfully, Gensim provides a method within the corpora.Dictionary()  function to do this, as shown here:dictionary = gensim.corpora.Dictionary(cleaned_data)dictionary.filter_extremes(no_below=100, no_above=300)corpus = [dictionary.doc2bow(text) for text in  cleaned_data] lda_model = models.LdaModel(corpus=corpus, num_topics=n_topics, id2word=dictionary, verbose=1)Chapter 4  topiC Modeling and\\xa0Word eMbeddings85Similar to the scikit-learn method, we can filter objects based on document frequency. The preprocessing steps we are taking here are slightly different than those present in the sklearn_topic_model()  function, which will become central to our discussion at the end of this section. Similar to what you saw before, what seems like a minor change in preprocessing steps can lead to a drastically different outcome.We execute the gensim_topic_model()  function and get the following result:Gensim LDA implemenation:Topic #0: 0.116*\"movie\" + 0.057*\"people\" + 0.051*\"like\" + 0.049*\"good\" + 0.041*\"well\" + 0.038*\"film\" + 0.037*\"one\" + 0.037*\"story\" + 0.033*\"great\" + 0.028*\"new\"Topic #1: 0.106*\"one\" + 0.063*\"movie\" + 0.044*\"like\" + 0.043*\"see\" + 0.041*\"much\" + 0.038*\"story\" + 0.033*\"little\" + 0.032*\"good\" + 0.032*\"way\" + 0.032*\"get\"Topic #2: 0.154*\"film\" + 0.060*\"one\" + 0.047*\"like\" + 0.039*\"movie\" + 0.037*\"time\" + 0.032*\"characters\" + 0.031*\"scene\" + 0.028*\"good\" + 0.028*\"make\" + 0.027*\"little\"Topic #3: 0.096*\"film\" + 0.076*\"one\" + 0.060*\"even\" + 0.053*\"like\" + 0.051*\"movie\" + 0.040*\"good\" + 0.036*\"time\" + 0.033*\"get\" + 0.030*\"would\" + 0.028*\"way\"Topic #4: 0.079*\"film\" + 0.068*\"plot\" + 0.058*\"one\" + 0.057*\"would\" + 0.049*\"like\" + 0.039*\"two\" + 0.038*\"movie\" + 0.036*\"story\" + 0.035*\"scenes\" + 0.033*\"much\"Topic #5: 0.136*\"film\" + 0.067*\"movie\" + 0.064*\"one\" + 0.039*\"first\" + 0.037*\"even\" + 0.037*\"would\" + 0.036*\"time\" + 0.035*\"also\" + 0.029*\"good\" + 0.027*\"like\"Topic #6: 0.082*\"movie\" + 0.072*\"get\" + 0.068*\"film\" + 0.059*\"one\" + 0.046*\"like\" + 0.036*\"even\" + 0.035*\"know\" + 0.027*\"much\" + 0.027*\"way\" + 0.026*\"story\"Chapter 4  topiC Modeling and\\xa0Word eMbeddings86Topic #7: 0.131*\"movie\" + 0.097*\"film\" + 0.061*\"like\" + 0.045*\"one\" + 0.032*\"good\" + 0.029*\"films\" + 0.027*\"see\" + 0.027*\"bad\" + 0.025*\"would\" + 0.025*\"even\"Topic #8: 0.139*\"film\" + 0.060*\"movie\" + 0.052*\"like\" + 0.044*\"story\" + 0.043*\"life\" + 0.043*\"could\" + 0.041*\"much\" + 0.032*\"well\" + 0.031*\"also\" + 0.030*\"time\"Topic #9: 0.116*\"film\" + 0.091*\"one\" + 0.059*\"movie\" + 0.035*\"two\" + 0.029*\"character\" + 0.029*\"great\" + 0.027*\"like\" + 0.026*\"also\" + 0.026*\"story\" + 0.026*\"life\"So far, the results from the scikit-learn implementation of LDA using term frequency as our feature extractor has given the most interpretable results. Most of the results are homogenous, which might not lead to much differentiation, making the results from this less useful.Using this same data set, let’s utilize another topic extraction model. Non-Negative Matrix Factorization (NMF)Non-negative matrix factorization (NMF) is an algorithm that takes a matrix and returns two matrices that have no non-negative elements. NMF is closely related to matrix factorization, except NMF only receives non-  negative values (0 and anything above 0).We want to utilize NMF rather than another type of matrix factorization because we need positive coefficients, as is the case when using LDA.\\xa0We can describe the process with the following mathematical formula: VWH= The matrix, V , is the original matrix that we input to the data. The two matrices that we output are W and H.\\xa0In this example, let’s assume matrix V has 1000 rows and 200 columns. Each row represents a word and each column represents a document. Therefore, we have a 1000-word vocabulary featured across 200 documents. As it relates to the preceding equation, V is an m×n matrix, W is an m×p matrix, and H is a p ×n matrix. W is a features matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings87Let’s say that we would like to find five features such that we generate matrix W with 1000 rows and 5 columns. Matrix H subsequently has a shape equivalent to 5 rows and 200 columns. When we perform matrix multiplication on W and H, we yield matrix V with 1000 rows and 200 columns, equivalent to the dimensionality described earlier. We consider that each document is built from a number of hidden features, which NMF would therefore generate. The following is the scikit-learn implementation of NMF that we will utilize for this example:def nmf_topic_model():     def create_topic_model(model, n_topics=10, max_iter=5,  min_df=10,                            max_df=300, stop_words=\\'english\\', token_pattern=r\\'\\\\w+\\'):        print(model + \\' NMF topic model: \\')        data = load_data()[0]        if model == \\'tf\\':             feature_extractor = CountVectorizer(min_df=min_df, max_df=max_df,                                  stop_words=stop_words,  token_pattern=token_pattern)        else:             feature_extractor = TfidfVectorizer(min_df=min_df, max_df=max_df,                                  stop_words=stop_words,  token_pattern=token_pattern)        processed_data = feature_extractor.fit_transform(data)         nmf_model = NMF(n_components=n_components,  max_iter=max_iter)        nmf_model.fit(processed_data)        tf_features = feature_extractor.get_feature_names()Chapter 4  topiC Modeling and\\xa0Word eMbeddings88         print_topics(model=nmf_model, feature_names=tf_features, n_top_words=n_topics)    create_topic_model(model=\\'tf\\')We invoke the NMF topic extraction in virtually the same manner that we invoke the LDA topic extraction model. Let’s look at the output of both the term frequency preprocessed data and the TFIDF preprocessed data.tf NMF topic model:Topic #0: family guy original michael sex wife woman r men playTopic #1: jackie tarantino brown ordell robert grier fiction pulp jackson michaelTopic #2: jackie hong drunken master fu kung chan arts martial iiTopic #3: scream 2 williamson horror sequel mr killer sidney kevin slasherTopic #4: webb jack girl gives woman ll male killed sir talkingTopic #5: musical musicals jesus death parker singing woman nation rise alanTopic #6: bulworth beatty jack political stanton black warren primary instead americanTopic #7: godzilla effects special emmerich star york computer monster city nickTopic #8: rock kiss city detroit got music tickets band soundtrack tryingTopic #9: frank chicken run shannon ca mun html sullivan particularly historyThe following is the TFIDF NMF topic model:Topic #0: 10 woman sense james sex wife guy school day endingTopic #1: scream horror williamson 2 sidney craven stab killer arquette 3Topic #2: wars phantom jedi lucas menace anakin jar effects darth gonChapter 4  topiC Modeling and\\xa0Word eMbeddings89Topic #3: space deep alien ship armageddon harry effects godzilla impact aliensTopic #4: disney mulan animated joe voice toy animation apes mermaid gorillaTopic #5: van amistad spielberg beatty cinque political slavery en slave hopkinsTopic #6: carpenter ott ejohnsonott nuvo subscribe reviews johnson net mail eTopic #7: hanks joe ryan kathleen mail shop online fox tom megTopic #8: simon sandler mercury adam rising willis wedding vincent kevin julianTopic #9: murphy lawrence martin eddie ricky kit robbins miles claude policeBefore we evaluate the results and have a more thorough discussion on both methods, let’s focus on visualizing the results. In the preceding example, we’ve reasonably reduced the complexity so that users can assess the different topics within the analyzed documents. However, this isn’t as helpful when we want to look at larger amounts of data and make inferences from this topic model relatively quickly.We’ll begin with what I believe is a useful plot, supplied by pyLDAvis. This software is extremely useful and works relatively easily when used with a Jupyter notebook, which are excellent for code visualization and results presentation. It is common to utilize a Jupyter notebook when using a virtual machine instance from either Amazon Web Services (AWS) or Google Cloud.Note  For those of you who have not worked with google Cloud   or aWs, i recommend these tutorials:  google Compute engine:   www.youtube.com/watch?v=zzMCKv1g5z0  aWs: www.youtube.com/watch?v=q1vVedHbkAYChapter 4  topiC Modeling and\\xa0Word eMbeddings90Set up an instance and start a Jupyter notebook. We will make some minor adjustments for running this on your local machine to running it in the cloud. In this example, the scikit-learn implementations—given the preprocessing algorithms provided—make gleaning interpretable topics much easier than the Gensim model. Although it gives more flexibility and has a lot of features, Gensim requires you to fine-tune the preprocessing steps from scratch. If you have the time to build results from scratch, this is not a problem; however, keep this in mind when building your own application, and consider the difficulties of having to use this method in Gensim.In this demo, NMF and LDA typically give similar results; however, the choice of one model vs. the other is often relative to the way we conceive of the data. LDA assumes that topics are infinitely exchangeable, but the words within a topic are not. As such, if we are not concerned about the topic probability per document remaining fixed (it assumedly would not be, as not all documents contain the same topics across large corpuses), LDA is a better choice. NMF might be a better choice if we have a heavy degree of certainty with respect to fixed topic probability and the data set is considerably smaller. Again, these statements should be taken in consideration when evaluating the results of the respective topic models, as with all machine learning problems.Let’s discuss a more advanced modeling technique that plays a role in sentiment analysis (in addition to more advanced NLP tasks): word embeddings. We begin by discussing a body of algorithms: Word2Vec. Word2VecTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean are credited with creating Word2Vec in 2014 while working at Google. Word2Vec represents a significant step forward in NLP-related tasks, as it provides a method for finding vector representations of words and phrases, and it can be expanded as much as the documents.Chapter 4  topiC Modeling and\\xa0Word eMbeddings91First, let’s examine the Skip-Gram model, which is a shallow neural network whose objective is to predict a word in a sequence based on the words around it. Let’s take the following sequence of training words: ww wwT =¼()12,, ,  The objective function is the average log probability, represented by the following: 110Tpw wtTcj cjtj t=- ££ ¹+ åå,log)(|  pw wp wwvvvvtj tO IwTwiwWwTwiO(| (|+===¢()¢() å))expexp1  c = size of the training context, T = the total number of training words,  t = index position of the current word, j = the window that determines which word in the sequence we are looking, wt = center word of the sequence, and W= number of words in the vocabulary.Before we move on, it’s important that you understand the formula and how it explains what the model does. An n-gram is a continuous grouping of n words. A Skip-Gram is a generalization of an n-gram, such that we have groupings of words, but they no longer need to be continuous; that is, we can skip words to create Skip-Grams. Mathematically, we typically define k-skip-n-grams as follows: {, ,, | ww wi ikii ijnjjn 1211 ¼- <=-å } Let’s assume the following is an input to the k-skip-n-gram model:“The cat down the street”Let’s also assume that we are seeking to create a 2-skip-bi-gram model. As such, the training examples are as follows:• “The, cat” , “The, down” , “The, the” , “The, street”• “cat, the” , “cat, down” , “cat, the” , “cat, street”Chapter 4  topiC Modeling and\\xa0Word eMbeddings92• “down, the” , “down, cat” , “down, the” , “down, street”• “the, The” , “the, cat” , “the, down” , “the, street”• “street, The” , “street, cat” , “street, down” , “street, the”So now you understand how the input data is represented as words.Let’s discuss how we represent these words with respect to a neural network. The input layer for the Skip-Gram model is a one-hot encoded vector with W components. In other words, every element of the vector is representative of a word in the vocabulary. The Skip-Gram architecture is graphically represented in Figure\\xa0 4-2.Figure 4-2.  Skip-Gram model architectureChapter 4  topiC Modeling and\\xa0Word eMbeddings93The goal of the neural network is to predict the word that has the highest probability of coming next in the input sequence. This is precisely why we want to use softmax, and ultimately how you intuitively understand the formula. We want to predict the word that is most probable given the input of the words, and we are calculating this probability based on the entirety of the input and output sequences observed by the neural network.Be that as it may, we have a minor problem. Softmax computation scales proportionally to the input size, which bodes poorly for this problem because accurate results will likely require large vocabulary sizes for training data. Hence, it is often suggested that we use an alternative method. One of the methods often referenced is negative sampling. Negative sampling is defined in the following equation: log~ log ss¢() + () -() éëùû=å vv Pw vvwoTwiikwi nw iTwi1\\uf045 Negative sampling achieves a cheaper computation than the softmax activation function by approximating its output. More precisely, we are only going to change K number of weights in the word embedding rather than computing them all. The Word2Vec paper suggests to sample  with 5 to 20 words in smaller data sets, but 2 to 5 words in larger data sets can achieve positive results.Beyond the training of the word embedding, what are we actually going to use it for? Unlike many neural networks, the main objective is not necessarily to use it for the purpose of prediction, but rather to obtain the trained hidden layer weight matrix. The hidden layer weight matrix is our trained word embedding . Once this hidden layer is trained, certain words cluster in areas of vector space, where they share similar contexts.Chapter 4  topiC Modeling and\\xa0Word eMbeddings94 Example Problem 4.2: Training a\\xa0Word Embedding (Skip-Gram)Let’s display the power of Word2Vec by working through a demo example, in Gensim and in TensorFlow. The following is some of the code that begins our implementation of the TensorFlow Word2Vec Skip-Gram model:def remove_non_ascii(text):    return \".join([word for word in text if ord(word) < 128])def load_data(max_pages=100):    return_string = StringIO()     device = TextConverter(PDFResourceManager(), return_string, codec=\\'utf-8\\', laparams=LAParams())     interpreter = PDFPageInterpreter(PDFResourceManager(), device=device)     filepath = file(\\'/Users/tawehbeysolow/Desktop/applied_nlp_python/datasets/economics_textbook.pdf\\', \\'rb\\')     for page in PDFPage.get_pages(filepath, set(), maxpages=max_pages, caching=True, check_extractable=True):        interpreter.process_page(page)    text_data = return_string.getvalue()    filepath.close(), device.close(), return_string.close()    return remove_non_ascii(text_data)For our example problem, we will utilize the PDFMiner Python module. For those of you who often parse data in different forms, this package is highly recommended. PDF data is notorious in parsing, as it is often filled with images and metadata that makes preprocessing the data a hassle. Thankfully, PDFMiner takes care of most of the heavy lifting, making our primary concerns only cleaning out stop words, grammatical characters, and other preprocessing steps, which are relatively straightforward. For this problem, we will read data from an economics textbook.Chapter 4  topiC Modeling and\\xa0Word eMbeddings95def gensim_preprocess_data():    data = load_data()    sentences = sent_tokenize(data)     tokenized_sentences = list([word_tokenize(sentence) for sentence in sentences])    for i in range(0, len(tokenized_sentences)):         tokenized_sentences[i] = [word for word in tokenized_sentences[i] if word not in punctuation]    return tokenized_sentencesWe now move to tokenizing the data based on sentences. Do not remove punctuation before this step . The NLTK sentence tokenizer relies on punctuation to determine where to split data based on sentences. If this is removed, it can cause you to debug something rather trivial. Regardless, the next format the data should take is that of a list, where every entry is a sentence whose words are tokenized, such that the words appear as follows: [[\\'This\\', \\'text\\', \\'adapted\\', \\'The\\', \\'Saylor\\', \\'Foundation\\', \\'Creative\\', \\'Commons\\', \\'Attribution-NonCommercial-ShareAlike\\', \\'3.0\\', \\'License\\', \\'without\\', \\'attribution\\', \\'requested\\', \\'works\\', \\'original\\', \\'creator\\', \\'licensee\\'], [\\'Saylor\\', \\'URL\\', \\'http\\', \\'//www.saylor.org/books\\', \\'Saylor.org\\', \\'1\\', \\'Preface\\', \\'We\\', \\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\'economics\\', \\'based\\', \\'two\\', \\'premises\\', \\'1\\'], [\\'Students\\', \\'motivated\\', \\'study\\', \\'economics\\', \\'see\\', \\'relates\\', \\'lives\\'], [\\'2\\'], [\\'Students\\', \\'learn\\', \\'best\\', \\'inductive\\', \\'approach\\', \\'first\\', \\'confronted\\', \\'question\\', \\'led\\', \\'process\\', \\'answer\\', \\'question\\'], [\\'The\\', \\'intended\\', \\'audience\\', \\'textbook\\', \\'first-year\\', \\'undergraduates\\', \\'taking\\', \\'courses\\', \\'principles\\', \\'macroeconomics\\', \\'microeconomics\\'], [\\'Many\\', \\'may\\', \\'never\\', \\'take\\', \\'another\\', \\'economics\\', \\'course\\'], [\\'We\\', \\'aim\\', \\'increase\\', \\'economic\\', \\'literacy\\', \\'developing\\', \\'aptitude\\', \\'economic\\', Chapter 4  topiC Modeling and\\xa0Word eMbeddings96\\'thinking\\', \\'presenting\\', \\'key\\', \\'insights\\', \\'economics\\', \\'every\\', \\'educated\\', \\'individual\\', \\'know\\'], [\\'Applications\\', \\'ahead\\', \\'Theory\\', \\'We\\', \\'present\\', \\'theory\\', \\'standard\\', \\'books\\', \\'principles\\', \\'economics\\'], [\\'But\\', \\'beginning\\', \\'applications\\', \\'also\\', \\'show\\', \\'students\\', \\'theory\\', \\'needed\\'], [\\'We\\', \\'take\\', \\'kind\\', \\'material\\', \\'authors\\', \\'put\\', \\'applications\\', \\'boxes\\', \\'place\\', \\'heart\\', \\'book\\'], [\\'Each\\', \\'chapter\\', \\'built\\', \\'around\\', \\'particular\\', \\'business\\', \\'policy\\', \\'application\\', \\'microeconomics\\', \\'minimum\\', \\'wages\\', \\'stock\\', \\'exchanges\\', \\'auctions\\', \\'macroeconomics\\', \\'social\\', \\'security\\', \\'globalization\\', \\'wealth\\', \\'poverty\\', \\'nations\\']Now that we have finished the preprocessing of our data, we can work with the Gensim implementation of the Skip-Gram model.def gensim_skip_gram():    sentences = gensim_preprocess_data()     skip_gram = Word2Vec(sentences=sentences, window=1,  min_count=10, sg=1)    word_embedding = skip_gram[skip_gram.wv.vocab] (1)Invoking the Skip-Gram model is relatively straightforward, and the training of the model is taken care of for us as well. The training process of a Skip-Gram model mimics that of all neural networks, in that we pass an input through all the layers and then backpropagate the error through each of the respective weights in each layer, updating them until we have reached a loss tolerance threshold or until the maximum number of epochs has been reached. Once the word embedding has been trained, we obtain the weight matrix by indexing the model with the wv.vocab  attribute of the model itself.Now, let’s discuss visualizing the words as vectors.    pca = PCA(n_components=2)    word_embedding = pca.fit_transform(word_embedding)Chapter 4  topiC Modeling and\\xa0Word eMbeddings97    #Plotting results from trained word embedding    plt.scatter(word_embedding[:, 0], word_embedding[:, 1])    word_list = list(skip_gram.wv.vocab)    for i, word in enumerate(word_list):         plt.annotate(word, xy=(word_embedding[i, 0],  word_embedding[i, 1]))Word embeddings are output in dimensions that are difficult to visualize in their raw formats. As such, we need to find a way to reduce the dimensionality of this matrix, while also retaining all the variance and attributes of the original data set. A preprocessing method that does this is principal components analysis (PCA). Briefly, PCA transforms a matrix so that it returns an eigen-decomposition called eigenvectors, in addition to eigenvalues. For the sake of showing a two-dimensional plot, we want to create a transformation that yields two  principal components. It is important to remember that these principal components are not exactly the same as the original matrix, but an orthogonal transformation of the word embedding that is related to it. Figure\\xa0 4-3 illustrates the matrix.Figure 4-3.  Skip-Gram word embeddings generated via GensimChapter 4  topiC Modeling and\\xa0Word eMbeddings98In the vector space, words that are closer to one another appear in similar contexts, and words that are further away from each other are more dissimilar in respect to the contexts in which they appear. Cosine similarity is a common method for measuring this. Mathematically, cosine distance is described as follows: cosq()=*ABAB  We intuitively describe cosine similarity as the sum of the product of all the respective elements of two given vectors, divided by the product of their Euclidean norms. Two vectors that have a 0-degree difference yield a cosine similarity of 1; whereas two vectors with a 90-degree difference yield a cosine similarity of 0. The following is an example of some of the cosine distances between different word vectors:Cosine distance for people   and Saylor -0.10727774727479297Cosine distance for URL   and people -0.137377917173043Cosine distance for money   and URL -0.03124461706797222Cosine distance for What   and money -0.007384979727807199Cosine distance for one   and What 0.022940581092187863Cosine distance for see   and one 0.05983065381073224Cosine distance for economic   and see -0.0530102968258333Gensim takes care of some of the uglier aspects of preprocessing the data. However, it is useful to know how to perform some of these things from scratch, so let’s try implementing a word embedding utilizing the same data, except this time we will do it in TensorFlow.Chapter 4  topiC Modeling and\\xa0Word eMbeddings99Let’s walk through a toy implementation to ensure that you are aware of what the model is doing, and then walk through an implementation that is easier to deploy.def tf_preprocess_data(window_size=window_size):    def one_hot_encoder(index, vocab_size):        vector = np.zeros(vocab_size)        vector[index] = 1        return vector    text_data = load_data()    vocab_size = len(word_tokenize(text_data))    word_dictionary = {}    for index, word in enumerate(word_tokenize(text_data)):        word_dictionary[word] = index    sentences = sent_tokenize(text_data)    tokenized_sentences = list([word_tokenize(sentence) for sentence in sentences])    n_gram_data = []We must prepare the data slightly differently for TensorFlow than we did for Gensim. The Gensim Word2Vec method takes care of most of the back-end things for us, but it is worthwhile to implement a simple proof of concept from scratch and walk through the algorithm.We begin by making a dictionary that matches a word with an index number. This index number forms the position in our one-hot encoded input and output vectors.Let’s continue preprocessing the data.#Creating word pairs for word2vec model    for sentence in tokenized_sentences:        for index, word in enumerate(sentence):            if word not in punctuation:Chapter 4  topiC Modeling and\\xa0Word eMbeddings100                for _word in sentence[max(index  - window_size, 0):                                       min(index + window_size, len(sentence)) + 1]:                    if _word != word:                        n_gram_data.append([word, _word])The preceding section of code effectively creates our n-grams, and ultimately simulates how the Skip-Gram model convolves over a sentence in such a way that it can predict the following word with the highest probability. We then create an m×n matrix, where m is the number of words in our input sequences, and n is the number words in the vocabulary. #One-hot encoding data and creating dataset intrepretable by skip-gram model x, y = np.zeros([len(n_gram_data), vocab_size]), np.zeros([len(n_gram_data), vocab_size])for i in range(0, len(n_gram_data)):     x[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i][0]], vocab_size=vocab_size)     y[i, :] = one_hot_encoder(word_dictionary[n_gram_data[i][1]], vocab_size=vocab_size)return x, y, vocab_size, word_dictionaryMoving forward to the function that we will use to construct our Skip-  Gram mo del, we begin by loading the data and the vocabulary size and word dictionary. As with other neural network models, we instantiate the placeholders, variables, and weights. Per the Skip-Gram model diagram shown in Figure\\xa0 4-2, we only need to contain a hidden and an output weight matrix.Chapter 4  topiC Modeling and\\xa0Word eMbeddings101def tensorflow_word_embedding(learning_rate=learning_rate, embedding_dim=embedding_dim):    x, y, vocab_size, word_dictionary = tf_preprocess_data()    #Defining tensorflow variables and placeholder    X = tf.placeholder(tf.float32, shape=(None, vocab_size))    Y = tf.placeholder(tf.float32, shape=(None, vocab_size))    weights = {\\'hidden\\': tf.Variable(tf.random_normal([vocab_size, embedding_dim])),                \\'output\\': tf.Variable(tf.random_normal([embedding_dim, vocab_size]))}     biases = {\\'hidden\\': tf.Variable(tf.random_normal([embedding_dim])),               \\'output\\':  tf.Variable(tf.random_normal([vocab_size]))}     input_layer = tf.add(tf.matmul(X, weights[\\'hidden\\']), biases[\\'hidden\\'])     output_layer = tf.add(tf.matmul(input_layer, weights[\\'output\\']), biases[\\'output\\'])In C hapter 5, we walk through implementing negative sampling. However, because the number of examples that we are using here is relatively miniscule, we can get away with utilizing the regular implementation of softmax as provided by TensorFlow. Finally, we execute our graph, as with other TensorFlow models, and observe the results shown in Figure\\xa0 4-4.Chapter 4  topiC Modeling and\\xa0Word eMbeddings102Cosine distance for dynamic   and limited 0.4128825113896724Cosine distance for four   and dynamic 0.2833843609582811Cosine distance for controversial   and four 0.3266445485300576Cosine distance for hanging   and controversial 0.37105348488163503Cosine distance for worked   and hanging 0.44684699747383416Cosine distance for Foundation   and worked 0.3751656692569623Figure 4-4.  Word vectors from toy implementation of Skip-  GramChapter 4  topiC Modeling and\\xa0Word eMbeddings103Again, the implementations provided here are not final examples of what well-trained word embeddings necessarily looks like. We will tackle that task more specifically in Chapter 5, as data collection is largely the issue that we must discuss in greater detail. However, the Skip-Gram model is only one of the word embeddings that we will likely encounter.We now will continue our discussion by tackling the continuous  bag-  of- words model. Continuous Bag-of-Words (CBoW)Similar to a Skip-Gram model, a continuous bag-of-words model (CBoW) is training on the objective of predicting a word. Unlike the Skip-Gram model, however, we are not trying to predict the next word in a given sequence. Instead, we are trying to predict some center word based on the context around the target label. Let’s imagine the following input data sentence:“The boy walked to the red house”In the context of the CBoW model, we could imagine that we would have an input vector that appeared as follows:“The, boy, to, the, red, house”Here, “walked” is the target that we are trying to predict. Visually, the CBoW model looks like Figure\\xa0 4-5.Chapter 4  topiC Modeling and\\xa0Word eMbeddings104Each word in the input is represented in a single one-hot encoded vector . Similar to the Skip-Gram model, the length of the input vector is equal to the number of words in the vocabulary. When evaluating our input data, a value of “1” is for the words that are present and a “0” is for the words that are not present. In Figure\\xa0 4-5, we are predicting a target word, w_t, based on the words w_t-2, w_ t-1, w_ t+1, and w_t+2.We then perform a weighted sum operation on this input vector with weight and bias matrices that pass these values to the projection layer, which is similar to the projection layer featured in the Skip-Gram model. Finally, we predict the class label with another weighted sum operation with the output weight and bias matrices in addition to utilizing a softmax classifier. The training method is the same as the one used in the Skip-  Gram model.Next, let’s work with a short example utilizing Gensim.Figure 4-5.  CBoW model representationChapter 4  topiC Modeling and\\xa0Word eMbeddings105 Example Problem 4.2: Training a\\xa0Word Embedding (CBoW)The Gensim implementation of CBoW requires that only a single parameter is changed, as shown here:cbow = Word2Vec(sentences=sentences, window=skip_gram_window_size, min_count=10, sg=0)We invoke this method and observe the results in the same manner that we did for the Skip-Gram model. Figure\\xa0 4-6 shows the results.Figure 4-6.  CBoW wor d embedding visualizationChapter 4  topiC Modeling and\\xa0Word eMbeddings106 Global Vectors for\\xa0Word  Representation (GloVe)GloVe is a contemporary and advanced method of vector representation of words. In 2014, Jeffrey Pennington, Richard Socher, and Christopher Manning wrote a paper in which they describe GloVe. This type of word embedding is an improvement over both matrix factorization–based representations of words, as well as the Skip-gram model. Matrix factorization–based methods of word representation are not particularly good at representing words with respect to their analogous nature. However, Skip-Gram and CBoW train on isolated windows of text and do not utilize the same information that a matrix-based factorization method does. Specifically, when we use LDA to create a topic model, we have to preprocess the text in a way that encodes each word with statistical information that represents the word in the context of the whole text. With Skip-Gram and CBoW, the one-hot encoded vector doesn’t capture that same type of complexity.GloVe specifically trains on “global word-to-word co-occurrence counts. ” Co-occurrence is the instance of two words appearing in a specific order alongside one another. By global , I mean the co-occurrence counts with respect to all documents in the corpus that we are analyzing. In this sense, GloVe is utilizing a bit of the intuition behind both models to try and overcome the respective shortcomings of the aforementioned alternatives.Let’s begin by defining a co-occurrence matrix, X.\\xa0Each entry in the matrix represents the co-occurrence count of two specific words. More specifically, X i, j represents the number of times word j appears in the context of word i. The following notation is also worth noting: Xi=åkikX, (4.4) PP jiXXijiji,,=() = | (4.5)Chapter 4  topiC Modeling and\\xa0Word eMbeddings107Equation 4.4 is defined as the number of times any word appears in the context of the word I.\\xa0Equation 4.5 is the probability of a word j given word i. We define this probability as the co-occurrence account of word j appears in the context of the word “I” with the total co-occurrence counts of word i.I suggest that the model should evaluate the ratios of co-occurrence probabilities, which we define as follows: Fw wwPPij kikjk,,\\uf025 () = (4.6)w\\xa0∈\\xa0ℝd= word vectors and \\uf025wkdÎ\\uf052 = context vectors, F = exp( x)∗You should observe that our definition of F has an asterisk above it, particularly to indicate the fact that the value of F can be a multitude of things; however, we often derive it to be the preceding definition. The purpose of F is to encode the value yielded from the co-occurrence probabilities into the word embedding.The following functions derive the target label and the error function we use to train the GloVe word embedding: Fw wPiTki k\\uf025() =, (4.7) ww bb XiTji ji j\\uf025\\uf025 ++ -log, (4.8) Jf Xw wb bXijVij iTji ji j = () ++ - ()=å,,, log12\\uf025\\uf025 (4.9)Where f\\xa0(Xij) = weighting functionAs detailed in the GloVe paper, the weighting function should obey a few rules. Foremost, if f is a continuous function, it should vanish as x\\xa0→\\xa00, f\\xa0(x) should be non-decreasing, and f(x) should be relatively small for large values of x. These rules are to ensure that rare or frequent co-occurrence values are not overweighted in the training of the word embedding. Chapter 4  topiC Modeling and\\xa0Word eMbeddings108Although the weighting function can be altered, the GloVe paper suggests the following equation: fxxxifxxotherwis emm()=æèçöø÷ <ìíïîïa1  xm = maximum value of x, fixed to 100. The weighting function yields the values shown in Figure\\xa0 4-7 with respect to the x value.Now that we have reviewed the model, it is useful for you to understand how to use pretrained word embeddings, particularly since not everyone will have the time or the ability to train these embeddings from scratch due to the difficult nature of acquiring all of this data. Although there is not necessarily one predetermined place to get a word embedding from, you should be aware of the following GitHub repository that contains Figure 4-7.  Weighting function for GloVeChapter 4  topiC Modeling and\\xa0Word eMbeddings109the files for multitudes of word embeddings:  https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models . You can feel free to experiment and deploy these word embeddings for different tasks where they see fit.For this example, we will use a GloVe word embedding that contains 6 billion words and 50 features. This word embedding was trained from data taken from Wikipedia and has a vocabulary containing 400,000 words. Now, let’s begin with the code, shown here:def load_embedding(embedding_path=\\'/path/to/glove.6B.50D.txt\\'):    vocabulary, embedding = [], []    for line in open(embedding_path, \\'rb\\').readlines():        row = line.strip().split(\\' \\')        vocabulary.append(row[0]), embedding.append(row[1:])     vocabulary_length, embedding_dim = len(vocabulary), len(embedding[0])    return vocabulary, np.asmatrix(embedding), vocabulary_length, embedding_dimWe begin this problem by loading the word embeddings using the native open()  function to read the file line by line. Each line in the file starts with a word in the vocabulary, and the subsequent entries in that line represent the values within each of that word’s vector. We iterate through all the lines in the file, appending the word and the word vector to their respective arrays. As such, we are able to create a list of words within a vocabulary and reconstruct the word embeddings from a .txt  file. This trained embedding should look like Figure\\xa0 4-8.Chapter 4  topiC Modeling and\\xa0Word eMbeddings110Figure 4-8 shows the representation of the first 50 words in the vocabulary when we look at the two principal components yielded from the transformation of our word embedding. Examples of words that seem to appear in similar contexts are had  and has, and , and as, in addition to his and he. When comparing the cosine similarities of other words in the vocabulary, we observe the following.Cosine Similarity Between so and u.s.: 0.5606769548631282Cosine Similarity Between them and so: 0.8815159254335486Cosine Similarity Between what and them: 0.8077565084355354Cosine Similarity Between him and what: 0.7972281857691554Cosine Similarity Between united and him: 0.5374600664967559Cosine Similarity Between during and united: 0.6205250403136882Cosine Similarity Between before and during: 0.8565694276984954Cosine Similarity Between may and before: 0.7855322363492923Cosine Similarity Between since and may: 0.7821437532357596Figure 4-8.  GloV e pretrained embeddingChapter 4  topiC Modeling and\\xa0Word eMbeddings111 Example Problem 4.4: Using Trained Word Embeddings with\\xa0LSTMsNow that we have visually inspected the word embedding, let’s focus on how to use trained embeddings with a deep learning algorithm. Let’s imagine that we would like to include the following paragraph as additional training data for our word embedding.sample_text = \"\\'Living in different places has been thegreatest experience that I have had in my life. It has allowedme to understand people from different walks of life, as well as to question some of my own biases I have had with respectto people who did not grow up as I did. If possible, everyoneshould take an opportunity to travel somewhere separate from where they grew up.\"\\'.replace(\\'\\\\n\\', \")With our sample data assigned to a variable, let’s begin by performing some of the same preprocessing steps that we have familiarized ourselves with, exemplified by the following body of code:def sample_text_dictionary(data=_sample_text):     count, dictionary = collections.Counter(data).most_common(), {} #creates list of word/count pairs;    for word, _ in count:         dictionary[word] = len(dictionary) #len(dictionary) increases each iteration         reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))     dictionary_list = sorted(dictionary.items(),  key = lambda x : x[1])    return dictionary, reverse_dictionary, dictionary_listChapter 4  topiC Modeling and\\xa0Word eMbeddings112We start by using a remove_stop_words()  function, a redefinition of a sample preprocessing text algorithm defined in Chapter 3 that removes stop words from relatively straightforward sample data. When you are using data that isn’t as clean as this sample data, I recommend that you preprocess the data in a manner similar to what you did using the economics textbook or War and Peace .Moving to the sample_text_dictionary()  function, we create a term frequency dictionary, and then return these variables. This process is important for you to understand, because this is an example of how we deal with words that are not in the vocabulary of a trained word embedding:for i in range(len(dictionary)):    word = dictionary_list[i][0]    if word in vocabulary:        _embedding_array.append(embedding_dictionary[word])    else:         _embedding_array.append(np.random.uniform(low=-0.2, high=0.2, size=embedding_dim))We b egin by creating a variable title: _embedding_array . This variable actually contains the word embedding representations of our sample text. To handle words that are not in the vocabulary, we will create a randomized distribution of numbers to simulate a word embedding, which we then feed as inputs to the neural network.Moving forward, we make the final transformations to the embedding data before we create our computation graph.embedding_array = np.asarray(_embedding_array) decision_tree = spatial.KDTree(embedding_array, leafsize=100)Chapter 4  topiC Modeling and\\xa0Word eMbeddings113We will use a k-nearest neighbors tree to find the embedding that is closest to the array that our neural network outputs. From this, we use reverse_dictionary  to find the word that matches the predicted embedding.Let’s build our computational graph, as follows:#Initializing placeholders and other variablesX = tf.placeholder(tf.int32, shape=(None, None, n_input))Y = tf.placeholder(tf.float32, shape=(None, embedding_dim)) weights = {\\'output\\': tf.Variable(tf.random_normal([n_hidden, embedding_dim]))} biases = {\\'output\\': tf.Variable(tf.random_normal([embedding_dim]))} _weights = tf.Variable(tf.constant(0.0, shape=[vocabulary_length, embedding_dim]), trainable=True) _embedding = tf.placeholder(tf.float32, [vocabulary_length, embedding_dim])embedding_initializer = _weights.assign(_embedding)embedding_characters = tf.nn.embedding_lookup(_weights, X) input_series = tf.reshape(embedding_characters, [-1, n_input])input_series = tf.split(input_series, n_input, 1)You will find most of this similar to the LSTM tutorial in Chapter 2, but direct your attention to the second grouping of code, specifically where we create the _weights  and _embedding  variables. When we are loading a trained word embedding, or have an embedding layer in our computational graph, the data must pass through this layer before it can get to the neural network. The dimension of the network is the number of words in the vocabulary by the number of features. Although the number of features when training one’s own embedding can be altered, this is a predetermined value when we load a word embedding.Chapter 4  topiC Modeling and\\xa0Word eMbeddings114We assign the weights variable to the _embedding  placeholder, which is ultimately the weights our optimizer is tuning, whereupon we create an embedding characters variable.  The tf.nn.embedding_lookup()  function specifically retrieves the index numbers of the _weights  variable. Finally, we transform the embedding_characters  variable into the input_series  variable, which is actually directly fed into the LSTM layer.From this point forward, the passage of data from the LSTM layer through the rest of the graph should be familiar from the tutorial. When executing the code, you should see output such as the following:Input Sequence: [\\'me\\', \\'to\\', \\'understand\\', \\'people\\']Actual Label: fromPredicted Label: anEpoch: 210Error: 45.62042Input Sequence: [\\'different\\', \\'walks\\', \\'of\\', \\'life,\\']Actual Label: asPredicted Label: withEpoch: 220Error: 64.55679Input Sequence: [\\'well\\', \\'as\\', \\'to\\', \\'question\\']Actual Label: somePredicted Label: hasEpoch: 230Error: 75.29771An immediate suggestion for improving the error rate is to load different sample texts, perhaps from an actual corpus of data to train on, as the limited amount of data does not allow the accuracy to improve significantly much.Chapter 4  topiC Modeling and\\xa0Word eMbeddings115Another suggestion is to use the load_data()  function that is commented out loading your own PDF file and experimenting from that point forward.Now that we have reviewed the methods in which we can represent words as vectors, let’s discuss other textual representations. Thankfully, since most of these are Word2Vec abstractions , it will not require nearly as much explanation this time around. Paragraph2Vec: Distributed Memory of\\xa0Paragraph Vectors (PV-DM)Paragraph2Vec is an algorithm that allows us to represent objects of varying length, from sentences to whole documents, for the same purposes that we represented words as vectors in the previous examples. This technique was developed by Quoc Le and Tomas Mikolov, and largely is based off the Word2Vec algorithm.In Paragraph2Vec, we represent each paragraph as a unique vector in a matrix, D. Every word is also mapped to a unique vector, represented by a column in matrix W. We subsequently construct a matrix, h, which is formed by concatenating matrices W and D. We think of this paragraph token as an analog to the cell state from the LSTM, in that it is providing memory to the current context in the form of the topic of the paragraph. Intuitively, that means that matrix W is the same across all paragraphs, such that we observe the same representation of a given word. Training occurs as it does in Word2Vec, and the negative sampling can occur in this instance by sampling from a fixed-length context in a random paragraph.To ensure that you understand how this works functionally, let’s look at one final example in this chapter.Chapter 4  topiC Modeling and\\xa0Word eMbeddings116 Example Problem 4.5: Paragraph2Vec Example with\\xa0Movie Review DataOnce again, Gensim thankfully has a Doc2Vec method that makes implementation of this algorithm relatively straightforward. In this example, we will keep things relatively simple and represent sentences in a vector space, rather than create or approximate a paragraph tokenizer, which we would likely want to be more precise than a heuristic that would be relatively quick to draw up (i.e., paragraphs comprised of four sentences each). In the doc2vec_example.py  file, there are only slight differences in the Doc2Vec model and the Word2Vec model, specifically the preprocessing.def gensim_preprocess_data(max_pages):    sentences = namedtuple(\\'sentence\\', \\'words tags\\')    _sentences = sent_tokenize(load_data(max_pages=max_pages))    documents = []    for i, text in enumerate(_sentences):        words, tags = text.lower().split(), [i]        documents.append(sentences(words, tags))    return documentsThe Doc2Vec implementation expects what is known as a named tuple object . This tuple contains a list of tokenized words contained in the sentence, as well as an integer that indexes this document. In online documentation, some people utilize a class object entitled LabledLineSentence() ; however, this performs the necessary preprocessing the same way. When we run our script, we iterate through all the sentences that we are analyzing, and view their associated cosine similarities. The following is an example of some of them:Document sentence(words=[\\'this\\', \\'text\\', \\'adapted\\', \\'the\\', \\'saylor\\', \\'foundation\\', \\'creative\\', \\'commons\\', \\'attribution-  Chapter 4  topiC Modeling and\\xa0Word eMbeddings117noncommercial-  sharealike\\', \\'3.0\\', \\'license\\', \\'without\\', \\'attribution\\', \\'requested\\', \\'works\\', \\'original\\', \\'creator\\', \\'licensee\\', \\'.\\'], tags=[0])Document sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], tags=[1])Cosine Similarity Between Documents: -0.025641936104727547Document sentence(words=[\\'saylor\\', \\'url\\', \\':\\', \\'http\\', \\':\\', \\'//www.saylor.org/books\\', \\'saylor.org\\', \\'1\\', \\'preface\\', \\'we\\', \\'written\\', \\'fundamentally\\', \\'different\\', \\'text\\', \\'principles\\', \\'economics\\', \\',\\', \\'based\\', \\'two\\', \\'premises\\', \\':\\', \\'1\\', \\'.\\'], tags=[1])Document sentence(words=[\\'students\\', \\'motivated\\', \\'study\\', \\'economics\\', \\'see\\', \\'relates\\', \\'lives\\', \\'.\\'], tags=[2])Cosine Similarity Between Documents:0.06511943195883922Beyond this, Gensim also allows us to infer vectors without having to retrain our models on these vectors. This is particularly important in Chapter 5, where we apply word embeddings in a practical setting. You can see this functionality when we execute the code with the training_example  parameter set to False. We have two sample documents, which we define at the beginning of the file:sample_text1 = \"\\'I love italian food. My favorite items arepizza and pasta, especially garlic bread. The best italian foodI have had has been in New  York. Little Italy was very fun\"\\'Chapter 4  topiC Modeling and\\xa0Word eMbeddings118sample_text2 = \"\\'My favorite time of italian food is pasta withalfredo sauce. It is very creamy but the cheese is the bestpart. Whenevr I go to an italian restaurant, I am always certain to get a plate.\"\\'These two examples are fairly similar. When we train our model—more than 300 pages worth of data from an economics textbook, we get the following results: Cosine Similarity Between Sample Texts:0.9911814256706748Again, you should be aware that they will likely need significantly larger amounts of data to get reasonable results across unseen data. These examples show them how to train and infer vectors using various frameworks. For those who are dedicated to training their own word embeddings, the path forward should be fairly clear. SummaryBefore we move on to work on natural language processing tasks, let’s recap some of the most important things learned in this chapter. As you saw in Chapter 3, preprocessing data correctly is the majority of the work that we need to perform when applying deep learning to natural language processing. Beyond cleaning out stop words, punctuation, and statistical noise, you should be prepared to wrangle data and organize it in an interpretable format for the neural network. Well-trained word embeddings often require the collection of billions of tokens.Chapter 4  topiC Modeling and\\xa0Word eMbeddings119Making sure that you aggregate the right data is extremely important, as a couple billion tokens from radically different data sources can leave you with an embedding that doesn’t yield much of anything useful. Although some of our examples yielded positive results, it does not mean these applications would work in a production environment. You must (responsibly) collect large amounts of text data from sources while maintaining homogeneity in vocabulary and context.In the following chapter, we conclude the book by working on applications of recurrent neural networks.Chapter 4  topiC Modeling and\\xa0Word eMbeddings121© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5_5CHAPTER 5Text Generation, Machine Translation, and\\xa0Other Recurrent Language Modeling TasksIn Chapter 4, I introduced you to some of the more advanced deep learning and NLP techniques, and I discussed how to implement these models in some basic problems, such as mapping word vectors. Before we conclude this book, I will discuss a handful of other NLP tasks that are more domain-specific, but nonetheless useful to go through.By this point, you should be relatively comfortable with preprocessing text data in various formats, and you should understand a few NLP tasks, such as document classification, well enough to perform them. As such, this chapter focuses on combining many of the skills we have worked with by tackling a couple of problems. All solutions provided in this chapter are feasible. You are more than welcome to present or complete new solutions that outperform them.122 Text Generation with\\xa0LSTMsText generation is increasingly an important feature in AI-based tools. Particularly when working with large amounts of data, it is useful for systems to be able to communicate with users to provide a more immersive and informative experience. For text generation, the main goal is to create a generative model that provides some sort of insight with respect to the data. You should be aware that text generation should not necessarily create a summary of the document, but generate an output that is descriptive of the input text. Let’s start by inspecting the problem.Initially, for such a task, we need a data source. From that, our data source changes the results. For this task, we start by working with Harry Potter and the Sorcerer’s Stone . I chose this book since the context should provide some fairly notable results with respect to the topics that are contained within the generated text.Let’s go through the steps that we’ve become accustomed to. We will utilize the load_data()  preprocessing function that we used in word_embeddings.py ; however, the only change that we will make is loading harry_potter.pdf  instead of economics_textbook.pdf .That said, this function allows you to easily utilize the preprocessing function for whatever purpose, so long as the directory and other arguments are changed. Being that this is a text generation example, we should not clean the data beyond removing non-ASCII characters.The following is an example of how the data appears:\"Harry Potter Sorcerer\\'s Stone CHAPTER ONE THE BOY WHO LIVED Mr. Mrs. Dursley, number four, Privet Drive, proud say perfectly normal, thank much. They last people \\'d expect involved anything strange mysterious, n\\'t hold nonsense. Mr. Dursley director firm called Grunnings, made drills. He big, beefy man hardly neck, although large mustache. Mrs. Dursley thin blonde nearly twice usual amount neck, came useful spent CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS123much time craning garden fences, spying neighbors. The Dursleys small son called Dudley opinion finer boy anywhere. The Dursleys everything wanted, also secret, greatest fear somebody would discover. They think could bear anyone found Potters. Mrs. Potter Mrs. Dursley\\'s sister, n\\'t met several years; fact, Mrs. Dursley pretended n\\'t sister, sister good-for-nothing husband unDursleyish possible. The Dursleys shuddered think neighbors would say Potters arrived street. The Dursleys knew Potters small son,, never even seen. This boy another good reason keeping Potters away; n\\'t want Dudley mixing child like. When Mr. Mrs. Dursley woke dull, gray Tuesday story starts, nothing cloudy sky outside suggest strange mysterious things would soon happening country. Mr. Dursley hummed picked boring tie work, Mrs. Dursley gossiped away happily wrestled screaming Dudley high chair. None noticed large, tawny owl flutter past window. At half past eight, Mr. Dursley picked briefcase, pecked Mrs. Dursley cheek, tried kiss Dudley good-bye missed, 1 Dudley tantrum throwing cereal walls. `` Little tyke, \"chortled Mr. Dursley left house. He got car backed number four\\'s drive. It corner street noticed first sign something peculiar -- cat reading map. For second, Mr. Dursley n\\'t realize seen -- jerked head around look. There tabby cat standing corner Privet Drive, n\\'t map sight. What could thinking ? It must trick light. Mr. Dursley blinked stared cat. It stared back. As Mr. Dursley drove around corner road, watched cat mirror. It reading sign said Privet Drive --, looking sign; cats...\"Let’s inspect our preprocessing function.def preprocess_data(sequence_length=sequence_length, max_pages=max_pages, pdf_file=pdf_file):    text_data = load_data(max_pages=max_pages, pdf_file=pdf_file)    characters = list(set(text_data.lower()))CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS124     character_dict = dict((character, i) for i, character in enumerate(characters))     int_dictionary = dict((i, character) for i, character in enumerate(characters))    num_chars, vocab_size = len(text_data), len(characters)    x, y = [], []    for i in range(0, num_chars  - sequence_length, 1):        input_sequence = text_data[i: i+sequence_length]        output_sequence = text_data[i+sequence_length]         x.append([character_dict[character.lower()] for character in input_sequence])        y.append(character_dict[output_sequence.lower()])        for k in range(0, len(x)): x[i] = [_x for _x in x[i]]    x = np.reshape(x, (len(x), sequence_length, 1))    x, y = x/float(vocab_size), np_utils.to_categorical(y)     return x, y, num_chars, vocab_size, character_dict,  int_dictionaryWhen inspecting the function, we use methods similar to the tf_preprocess_data()  function in the toy example of a Skip-Gram model. Our input and output sequences are fixed lengths, and we will transform the y variable to a one-hot encoded vector, where each entry in the vector represents a possible character. We represent the sequence of characters as a matrix, where each row represents the entire observation and each column represents a character.Let’s look at the first example of Keras code used in the book.    def create_rnn(num_units=num_units, activation=activation):        model = Sequential()         model.add(LSTM(num_units, activation=activation,  input_shape=(None, x.shape[1])))        model.add(Dense(y.shape[1], activation=\\'softmax\\'))CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS125         model.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\')        model.summary()        return modelKeras, unlike TensorFlow, is considerably less verbose. As such, this makes changing the architecture of a model relatively easy. We instantiate a model by assigning it to a variable, and then simply add layer types with the Sequential().add()  function.After running the network with 200 epochs, we get the following result:  driv, proud say perfecdly normal, thanp much. they last people \\'d expect involved anytsing strange mysterious, s\\'t hold donsense. mr. dursley director firm called grunnings, made drills. he big, berfy man, ardly neck, althougl larte mustache. mrs. dursley thic -londe. early twece uiual amount necd, came ueeful spent much time craning geddon fences, spying neighbors. the dursleys small son called dudley opinion finer boy anyw   rd. the dursleys everything wanted, slso secret, greatest fear somebody would discover. they thinn could bear antone found potters. mrs. potterimrs. dursley\\'s sister, n\\'t met several years; fact, mrs. dursley pretended n\\'t sister, sister good-sur-notding husband undursleyir   pousible. the dursleys suuddered think auigybors would say potters arrived strett. the dursleys knew potters small. on,   ever even seen. thit boy another good reason keeping potters away; n\\'e want dudley mixing child like. wten mr. mrs. dursley woke dull, gray tuesday story startss, nothing cloudy skycoutside suggest strange mytter ous taings would soon darpening codntry. mr. dursley tummed picked boring tie work, mrs. dursley gosudaed away happily wrestled screaming dudley aigh cuair. noneoloticed large, tawny owl flutter past wincow. at ialf past, ight, mr. dursley picked CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS126briefcase, pecked mrs. dursley cheek, tried kiss dudley good-bye missed, 1 dudley tantrum,hrowigg cereal walls. `` lwttle tykp, \"chortled mr. dursley left house. he got car backel number four\\'s drive. it corner street noticed fir t sign somathing pcculilr -- cat feading,ap. for sicond, mr. dursley r\\'t realize scen -- jerked head around look. thereytab y cat standing corneraprivet drive, n\\'tamap sight. what sould thinking ? it muse trick light. mr. dursley blinked stared cat. it stayed back. as mr. dursley drove around corner road, watched catcmirror. it reading sign saidsprivet druve --, lookingtsign; cats could n\\'t read maps signs. mr. dursNote  Some of the text is interpretable, but obviously not ever ything is as good as it could be. In this instance, I suggest that you allow the neural network to train longer and to add more data. Also consider using different models and model architectures. Beyond this example, it would be useful to present a more advanced version of the LSTM that is also useful for speech modeling. Bidirectional RNNs (BRNN)BRNNs were created in 1997 by Mike Schuster and Kukdip Paliwal, who introduced the technique to a signal-processing academic journal. The purpose of the model was to utilize information moving in both a “positive and negative time direction. ” Specifically, they wanted to utilize both the information moving up to the prediction, as well as the same stream of inputs moving in the opposite direction. Figure\\xa0 5-1 illustrates the architecture of a BRNN.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS127Let’s imagine that that we have a sequence of words, such as the following: The man walks down the boardwalk.In a regular RNN, assuming that we wanted to predict the word boardwalk , the input data would be The , man , walks , down , the. If we input bigrams, it would be The , man , man , walks , and so forth. We keep moving through the input data, predicting the word that is most likely to come next at each time step, culminating in our final target label, a probability that corresponds to the one-hot encoded vector that is most likely to be present given the input data. The only difference in a BRNN is that while we are predicting the sequence left-to-right, we also are predicting the sequence right-to-left.BRNNs have been particularly useful for NLP tasks. The following is the code for building a BRNN:def create_lstm(input_shape=(1, x.shape[1])):        model = Sequential()        model.add(Bidirectional(LSTM(unites=n_units,                                     activation=activation),                                     input_shape=input_shape))        model.add(Dense(train_y.shape[1]), activation=out_act)         model.compile(loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])        return model...AA AAX0 X1 X2 Xis 0siy0 y1 y2 yis0Figure 5-1.  Bidirectional RNNCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS128The structure of the bidirectional RNN is nearly identical in that we are only adding a Bidirectional()  cast over our layer. This often increases the time it takes to train neural networks, but in general, it outperforms traditional RNN architectures in many tasks. With this in mind, let’s apply our model. Creating a\\xa0Name Entity Recognition TaggerPeople who have worked with NLTK or similar packages have likely come across the name entity recognition  (NER) tagger. NER taggers typically output a label that identifies the entity within larger categories (person, organization, location, etc.). Creating an NER tagger requires a large amount of annotated data.For this task, we will use a data set from Kaggle. When we unzip the data, we see that it comes in the following format: played    on    Monday    (    home    team  in    CAPS )    :VBD        IN    NNP       (    NN      NN    IN    NNP  )    :O          O     O         O    O       O     O     O    0    OAmerican    LeagueNNP        NNPB-MISC     I-MISCCleveland   2     DETROIT   1NNP        CD    NNP       CDB-ORG      O     B-ORG     OBALTIMORE   12    Oakland   11   (       10    innings         )VB         CD    NNP       CD   (       CD    NN         )B-ORG      O     B-ORG     O    O       O     O     OTORONTO    5     Minnesota  3TO         CD    NNP       CDB-ORG      O     B-ORG     OMilwaukee   3     CHICAGO   2CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS129NNP        CD    NNP       CDB-ORG      O     B-ORG     OBoston     4     CALIFORNIA 1The data is tab-delimited but also in .txt  format. This requires some data wrangling before we get to training the BRNN.Let’s start by turning the text data into an interpretable format, as follows:def load_data():     text_data = open(\\'/Users/tawehbeysolow/Downloads/train.txt\\', \\'rb\\').readlines()     text_data = [text_data[k].replace(\\'\\\\t\\', \\' \\').split() for k in range(0, len(text_data))]    index = range(0, len(text_data), 3)    #Transforming data to matrix format for neural network    input_data =   list()    for i in range(1, len(index)-1):        rows = text_data[index[i-1]:index[i]]         sentence_no = np.array([i for i in np.repeat(i, len(rows[0]))], dtype=str)        rows.append(np.array(sentence_no))        rows = np.array(rows).T        input_data.append(rows)We must first iterate through each line of the .txt  file. Notice that the data is organized in groups of three. A typical grouping looks like the following:text_data[0][\\'played\\', \\'on\\', \\'Monday\\', \\'(\\', \\'home\\', \\'team\\', \\'in\\', \\'CAPS\\', \\')\\', \\':\\'] text_data[1]CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS130[\\'VBD\\', \\'IN\\', \\'NNP\\', \\'(\\', \\'NN\\', \\'NN\\', \\'IN\\', \\'NNP\\', \\')\\', \\':\\']text_data[2][\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']The first set of observations contains the text itself, the second set of observations contains the name entity tag, and the final set contains the specific tag. Back to the preprocessing function, we take the groupings of sentences and append an array that contains a sentence number label, which I will discuss the importance of shortly.When looking at a snapshot of the input_data  variable, we see the following:input_data[0:1][array([[\\'played\\', \\'VBD\\', \\'O\\', \\'1\\'],       [\\'on\\', \\'IN\\', \\'O\\', \\'1\\'],       [\\'Monday\\', \\'NNP\\', \\'O\\', \\'1\\'],       [\\'(\\', \\'(\\', \\'O\\', \\'1\\'],       [\\'home\\', \\'NN\\', \\'O\\', \\'1\\'],       [\\'team\\', \\'NN\\', \\'O\\', \\'1\\'],       [\\'in\\', \\'IN\\', \\'O\\', \\'1\\'],       [\\'CAPS\\', \\'NNP\\', \\'O\\', \\'1\\'],       [\\')\\', \\')\\', \\'O\\', \\'1\\'],       [\\':\\', \\':\\', \\'O\\', \\'1\\']], dtype=\\'|S6\\')]We need to remove the sentence label while observing the data in such a fashion that the neural network implicitly understands how these sentences are grouped. The reason we want to remove this label is that neural networks read categorical labels (which the sentence number is an analog for) in such a way that higher-numbered sentences explicitly have a greater importance than lower-numbered sentences. For this task, I assume most you understand we do not want to bake this into the training process. As such, we move to the following body of code:CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS131    input_data = pan.DataFrame(np.concatenate([input_data[j] for j in range(0,len(input_data))]),                        columns=[\\'word\\', \\'pos\\', \\'tag\\', \\'sent_no\\'])    labels, vocabulary = list(set(input_data[\\'tag\\'].values)), list(set(input_data[\\'word\\'].values))    vocabulary.append(\\'endpad\\'); vocab_size = len(vocabulary); label_size = len(labels)  aggregate_function = lambda input: [(word, pos, label) for word, pos, label in zip(input[\\'word\\'].values.tolist(),   input[\\'pos\\'].values.tolist(),   input[\\'tag\\'].values.tolist())]We organize the input_data  into a data frame, and then create a couple of other variables that we will use in the function later, as well as the train_brnn_keras()  function. Some of these variables are familiar to others present in the scripts from the prior chapter ( vocab_size  represents the number of words in the vocabulary, for example). However, the important parts are mainly the last two variables, which is what you should focus on to solve this problem.The lambda function, aggregate_function , takes a data frame as an input, and then returns a three-tuple for each observation within a grouping. This is precisely how we will group all the observations within one sentence. A snapshot of our data after this transformation yields the following: sentences[0][(\\'played\\', \\'VBD\\', \\'O\\'), (\\'on\\', \\'IN\\', \\'O\\'), (\\'Monday\\', \\'NNP\\', \\'O\\'), (\\'(\\', \\'(\\', \\'O\\'), (\\'home\\', \\'NN\\', \\'O\\'), (\\'team\\', \\'NN\\', \\'O\\'), (\\'in\\', \\'IN\\', \\'O\\'), (\\'CAPS\\', \\'NNP\\', \\'O\\'), (\\')\\', \\')\\', \\'O\\'), (\\':\\', \\':\\', \\'O\\')]CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS132We have nearly finished all the necessary preprocessing; however, there is a key step that you should be aware of.     x = [[word_dictionary[word[0]] for word in sent] for sent in sentences]     x = pad_sequences(maxlen=input_shape, sequences=x, padding=\\'post\\', value=0)     y = [[label_dictionary[word[2]] for word in sent] for sent in sentences]     y = pad_sequences(maxlen=input_shape, sequences=y, padding=\\'post\\', value=0)      = [np_utils.to_categorical(label, num_classes=label_size) for label in y]In the preceding lines of code, we are transforming our words to their integer labels as we did in many other examples, and creating a one-  hot encoded matrix. This is similar to the previous chapter, however, we should specifically not use the pad_sequences()  function.When working with sentence data, we do not always get sentences of equal length; however, the input matrix for the neural network has to have an equal number of features across all observations. Zero padding  is used to add the extra features that normalize the size of all observations.With this step done, we are now ready to move to training our neural network. Our model is as follows:def create_brnn():        model = Sequential()         model.add(Embedding(input_dim=vocab_size+1,  output_dim=output_dim,                             input_length=input_shape,  mask_zero=True))         model.add(Bidirectional(LSTM(units=n_units, activation=activation,                                     return_sequences=True)))CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS133         model.add(TimeDistributed(Dense(label_size, activation=out_act)))         model.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])        model.summary()        return modelMost of our model is similar to the prior Keras models built in this chapter; however, we have an embedding layer (analogous to a word embedding) that is stacked on top of the bidirectional LSTM, which is subsequently staked on top of a fully connected output layer.We train our network on roughly 90% of the data we have, and then subsequently evaluate the results. We find that our tagger on the training data yields an accuracy of 90% and higher, depending on the number of epochs we train it for.Now that we have dealt with this classification task and sufficiently worked with BRNNs, let’s move on to another neural network model and discuss how it can be effectively applied to another NLP task. Sequence-to-Sequence Models (Seq2Seq)Sequence-to-sequence models (seq2seq) are notable because they take in an input sequence and return an output sequence, both of variable length. This makes this model particularly powerful, and it is predisposed to perform well on language modeling tasks. The particular model that we will utilize is best summarized in a paper by Sutskever et\\xa0al. Figure\\xa0 5-2 illustrates the model.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS134The model is generally comprised of two parts: an encoder and a decoder. Both the encoder and the decoder are RNNs. The encoder reads the input sequence and outputs a fixed-length vector in addition to the hidden and cell states from the LSTM unit. Subsequently, the decoder takes this fixed-length vector, in addition to the output hidden and cell states, and uses them as inputs to the first of its LSTM units. The decoder outputs a fixed-length vector, which we will evaluate as a target label. We will perform prediction one character at a time, which easily allows us to evaluate sequences of varying length from one observation to the next. Next, you see this model in action. Question and\\xa0Answer with\\xa0Neural Network ModelsOne popular application of deep learning to NLP is the chatbot. Many companies use chatbots to handle generic customer service requests, which require them to be flexible in translating questions into answers. While the test case that we look at is a microcosm of questions and howa re you ?Ia m goodDECODER ENCODER<GO>EmbeddingFigure 5-2.  Encoder-decoder modelCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS135answers, it is an example of how we can train a neural network to properly answer a question. We will use the Stanford Question Answering Dataset. Although it is more representative of general knowledge, you would do well to recognize the way in which these problems are structured.Let’s begin by examining how we will preprocess the data by utilizing the following function:     dataset = json.load(open(\\'/Users/tawehbeysolow/Downloads/qadataset.json\\', \\'rb\\'))[\\'data\\']    questions, answers = [], []    for j in range(0, len(dataset)):        for k in range(0, len(dataset[j])):             for i in range(0, len(dataset[j][\\'paragraphs\\'][k][\\'qas\\'])):                 questions.append(remove_non_ascii(dataset[j][\\'paragraphs\\'][k][\\'qas\\'][i][\\'question\\']))               answers.append(remove_non_ascii(dataset[j][\\'paragraphs\\'][k][\\'qas\\'][i][\\'answers\\'][0][\\'text\\']))When we look at a snapshot of the data, we observe the following structure:[{u\\'paragraphs\\': [{u\\'qas\\': [{u\\'question\\': u\\'To whom did the Virgin Mary allegedly appear in 1858  in Lourdes France?\\', u\\'id\\': u\\'5733be284776f41900661182\\', u\\'answers\\': [{u\\'text\\': u\\'Saint Bernadette Soubirous\\', u\\'answer_start\\': 515}]}, {u\\'question\\': u\\'What is in front of the Notre Dame Main Building?\\', u\\'id\\': u\\'5733be284776f4190066117f\\', u\\'answers\\': [{u\\'text\\': u\\'a copper statue of Christ\\', u\\'answer_start\\': 188}]}, {u\\'question\\': u\\'The Basilica of the Sacred heart at Notre Dame is beside to which structure?\\', u\\'id\\': u\\'5733be284776f41900661180\\', u\\'answers\\': [{u\\'text\\': u\\'the Main Building\\', u\\'answer_start\\': 279}]}, {u\\'question\\': u\\'What is the CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS136Grotto at Notre Dame?\\', u\\'id\\': u\\'5733be284776f41900661181\\', u\\'answers\\': [{u\\'text\\': u\\'a Marian place of prayer and reflection\\', u\\'answer_start\\': 381}]}, {u\\'question\\': u\\'What sits on top of the Main Building at Notre Dame?\\', u\\'id\\': u\\'5733be284776f4190066117e\\', u\\'answers\\': [{u\\'text\\': u\\'a golden statue of the Virgin Mary\\', u\\'answer_start\\': 92}]}], u\\'context\\': u\\'Architecturally, the school has a Catholic character. Atop the Main Building\\\\\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\'}, {u\\'qas\\': [{u\\'question\\': u\\'When did the Scholastic Magazine of Notre dame begin publishing?\\', u\\'id\\': u\\'5733bf84d058e614000b61be\\', u\\'answers\\'We have a JSON file with question and answers. Similar to the name entity recognition task, we need to preprocess our data into a matrix format that we can input into a neural network. We must first collect the questions that correspond to the proper answers. Then we iterate through the JSON file, and append each of the questions and answers to the corresponding arrays.Now let’s discuss how we are actually going to frame the problem for the neural network. Rather than have the neural network predict each word, we are going to have the neural network predict each character given an input sequence of characters. Since this is a multilabel classification problem, we will output a softmax probability for each element of the CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS137output vector, and then choose the vector with the highest probability. This represents the character that is most likely to proceed given the prior input sequence.After we have done this for the entire output sequence, we will concatenate this array of outputted characters so that we get a human-  readable message. As such, we move forward to the following part of the code:    input_chars, output_chars = set(), set()    for i in range(0, len(questions)):        for char in questions[i]:             if char not in input_chars: input_chars.add(char.lower())    for i in range(0, len(answers)):        for char in answers[i]:             if char not in output_chars:  output_chars.add(char.lower())     input_chars, output_chars = sorted(list(input_chars)), sorted(list(output_chars))     n_encoder_tokens, n_decoder_tokens = len(input_chars), len(output_chars)We iterated through each of the questions and answers, and collected all the unique individual characters in both the output and input sequences. This yields the following sets, which represent the input and output characters, respectively.input_chars; output_chars[u\\' \\', u\\'\"\\', u\\'#\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\',\\', u\\'-\\', u\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', u\\'6\\', u\\'7\\', u\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'>\\', u\\'?\\', u\\'_\\', u\\'a\\', u\\'b\\', u\\'c\\', u\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', u\\'l\\', u\\'m\\', u\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', u\\'v\\', u\\'w\\', u\\'x\\', u\\'y\\', u\\'z\\']CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS138[u\\' \\', u\\'!\\', u\\'\"\\', u\\'$\\', u\\'%\\', u\\'&\\', u\"\\'\", u\\'(\\', u\\')\\', u\\'+\\', u\\',\\', u\\'-\\', u\\'.\\', u\\'/\\', u\\'0\\', u\\'1\\', u\\'2\\', u\\'3\\', u\\'4\\', u\\'5\\', u\\'6\\', u\\'7\\', u\\'8\\', u\\'9\\', u\\':\\', u\\';\\', u\\'?\\', u\\'[\\', u\\']\\', u\\'a\\', u\\'b\\', u\\'c\\', u\\'d\\', u\\'e\\', u\\'f\\', u\\'g\\', u\\'h\\', u\\'i\\', u\\'j\\', u\\'k\\', u\\'l\\', u\\'m\\', u\\'n\\', u\\'o\\', u\\'p\\', u\\'q\\', u\\'r\\', u\\'s\\', u\\'t\\', u\\'u\\', u\\'v\\', u\\'w\\', u\\'x\\', u\\'y\\', u\\'z\\']The two lists contain 53 and 55 characters, respectively; however, they are virtually homogenous and contain all the letters of the alphabet, plus some grammatical and numerical characters.We move to the most important part of the preprocessing, in which we transform our input sequences to one-hot encoded vectors that are interpretable by the neural network.(code redacted, please see github)     x_encoder = np.zeros((len(questions), max_encoder_len,  n_encoder_tokens))     x_decoder = np.zeros((len(questions), max_decoder_len,  n_decoder_tokens))     y_decoder = np.zeros((len(questions), max_decoder_len,  n_decoder_tokens))     for i, (input, output) in enumerate(zip(questions, answers)):        for _character, character in enumerate(input):             x_encoder[i, _character, input_dictionary[character.lower()]] = 1.        for _character, character in enumerate(output):             x_decoder[i, _character, output_dictionary[character.lower()]] = 1.             if i > 0: y_decoder[i, _character,  output_dictionary[character.lower()]] = 1.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS139We start by instantiating two input vectors and an output vector, denoted by x_encoder , x_decoder , and y_encoder . Sequentially, this represents the order in which the data passes through the neural network and validated against the target label. While the one-hot encoding that we chose to create here is similar, we make a minor change by creating a three-dimensional array to evaluate each question and answer. Each row represents a question, each time step represents a character, and each column represents the type of character within our set of characters. We repeat this process for each question-and-answer pair until we have an array with the entire data set, which yields 4980 observations of data.The last step defines the model, as given by the encoder_decoder()  function.def encoder_decoder(n_encoder_tokens, n_decoder_tokens):    encoder_input = Input(shape=(None, n_encoder_tokens))    encoder = LSTM(n_units, return_state=True)     encoder_output, hidden_state, cell_state = encoder(encoder_input)    encoder_states = [hidden_state, cell_state]    decoder_input = Input(shape=(None, n_decoder_tokens))     decoder = LSTM(n_units, return_state=True,  return_sequences=True)     decoder_output, _, _ = decoder(decoder_input,  initial_state=encoder_states)     decoder = Dense(n_decoder_tokens, activation=\\'softmax\\')(decoder_output)    model = Model([encoder_input, decoder_input], decoder)     model.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\',   metrics=[\\'accuracy\\'])    model.summary()    return modelCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS140We instantiated our model slightly differently than other Keras models. This method of creating a model is done through using the Functional API, rather than relying on the sequential model, as we have often done. Specifically, this method is useful when creating more complex models, such as seq2seq models, and is relatively straightforward once you have learned how to use the sequential model. Rather than adding layers to the sequential model, we instantiate different layers as variables and then pass the data by calling the tensor we created. We see this when observing the encoder_output  variable when we instantiate it by calling encoder(encoder_input). We keep doing this through the encoder-decoder phase until we reach an output vector, which we define as a dense/fully connected layer  with a softmax activation function.Finally, we move to training, and observe the following results:Model Prediction: saint bernadette soubiroustActual Output: saint bernadette soubirousModel Prediction: a copper statue of christActual Output: a copper statue of christModel Prediction: the main buildingActual Output: the main buildingModel Prediction: a marian place of prayer and reflectionActual Output: a marian place of prayer and reflectionModel Prediction: a golden statue of the virgin maryActual Output: a golden statue of the virgin maryModel Prediction: september 18760Actual Output: september 1876Model Prediction: twiceCHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS141Actual Output: twiceModel Prediction: the observerActual Output: the observerModel Prediction: threeActual Output: threeModel Prediction: 19877Actual Output: 1987As you can see, this model performs considerably well, with only three epochs. Although there are some problems with the spelling from adding extra characters, the messages themselves are correct in most instances. Feel free to keep experimenting with this problem, particularly by altering the model architecture to see if there is one that yields better accuracy. SummaryWith the chapter coming to a close, we should review the concepts that are most important in helping us successfully train our algorithms. Primarily, you should take note of the model types that are appropriate for different problems. The encoder-decoder model architecture introduces the “many-  to- many” input-output scheme and shows where it is appropriate to apply it.Secondarily, you should take note of where preprocessing techniques can be applied to seemingly different but related problems. The translation of data from one language to another uses the same preprocessing steps as creating a neural network that answered questions based on different responses. Paying attention to these modeling steps and how they relate to the underlying structure of the data can save you time on seemingly innocuous tasks.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS142 Conclusion and\\xa0Final StatementsWe have reached the end of this book. We solved a wide array of NLP problems of varying complexities and domains. There are many concepts that are constant across all problem types, most specifically data preprocessing. The vast majority of what makes machine learning difficult is preprocessing data. You saw that similar problem types share preprocessing steps, as we often reused parts of solutions as we moved to more advanced problems.There are some final principles that are worth remembering from this point forward. NLP with deep learning can require large amounts of text data. Collect it carefully and responsibly, and consider your options when dealing with large data sets with respect to choice of language for optimized run time (C/C++ vs. Python, etc.).Neural networks, by and large, are fairly straightforward models to work with. The difficulty is finding good data that has predictive power, in addition to structuring it in such a way that our neural network can find patterns to exploit.Study carefully the preprocessing steps to take for document classification, creating a word embedding, or creating an NER tagger, for example. Each of these represents feature extraction schemes that can be applied to different problems and illuminate a path forward during your research.Although intelligent preprocessing of data spoken about fairly often in the machine learning community, it is particularly true of the NLP paradigm of deep learning and data science. The models that we have trained give you a roadmap on how to work with similar data sets in professional or academic environments. However, this does not mean that the models we have deployed could be used in production and work well.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS143There are a considerable number of variables that I did not discuss, being that they are problems of maintaining production systems rather than the theory behind a model. Examples include unknown words in vocabularies that appear over time, when to retrain models, how to evaluate multiple models’ outputs simultaneously, and so forth.In my experience, finding out when to retrain models has best been solved by collecting large amounts of live performance data. See when signals deprecate, if they do at all, and track the effect of retraining, as well as the persistence in retraining of models. Even if your model is accurate, it does not mean that it will be easy to use in practice.Think carefully about how to handle false classifications, particularly if the penalty for misclassification could cause the loss of money and/or other resources. Do not be afraid to utilize multiple models for multiple problem types. When experimenting, start simple and gradually add complexity as needed. This is significantly easier than trying to design something very complex in the beginning and then trying to debug a system that you do not understand.You are encouraged to reread this book at your leisure, as well as for reference, in addition to utilizing the code on my GitHub page to tackle the problems in their own unique fashion. While reading this book provides a start, the only way to become proficient in data science is to practice the problems on your own.I hope you have enjoyed learning about natural language processing and deep learning as much as I have enjoyed explaining it.CHAPTER 5   TEXT GENERATION, MACHINE TRANSLATION AND\\xa0OTHER RECURRENT  LANGUAGE MODELING TASKS145© Taweh Beysolow II 2018 T . Beysolow II, Applied Natural Language Processing with Python ,  https://doi.org/10.1007/978-1-4842-3733-5IndexA, BBackpropagation through time (BPTT), 36Bag-of-words (BoW) modeladvantages, 73CountVectorizer, 51–52definition, 50disadvantages, 74feature extraction algorithm, 50machine learning algorithm, 50movie reviews ( see IMDB  movie review data set)scikit-learn library, 51spam detectionaccuracy and  AUC scores, 55–56CountVectorizer(), 54data set, 54fit() method, 55inbox, 53load_spam_data(), 54logistic regression, 54–56np.random.seed()  function, 56ridge regression, 55ROC curve, 57SMS message length histogram, 53text_classifiction_demo.py file, 54unwanted advertisements/malicious files, 53TFIDF , 57Bidirectional RNNs  (BRNNs), 126–128,  133CContinuous bag-of-words (CBoW) model, 103–105D, E, FDeep learningapplicationslanguage modeling  tasks, 11NLP techniques and document  classification, 10RNNs, 11topic modeling, 10word embeddings, 10–11Keras, 7–8models, 4TensorFlow, 4–7Theano, 8–9146G, HGlobal Vectors for Word Representation (GloVe)co-occurrence, 106–107cosine similarities, 110description, 106error function, 107GitHub repository, 108matrix-based factorization method, 106open() function, 109pretrained embeddings, 108–110weighting function, 107–108Wikipedia, 109IIMDB movie review data set” .join() function, 63L1 and L2 norm visualization, 69load data, 64logistic regression, 65–66machine learning packages, 62min_df and max_df, 65mlp_movie_classification_model.py file, 68open() function, 64ord() function, 63os.listdir() function, 64positive and negative rate, 73remove_non_ascii() function, 64ROC curveL1 and L2 logistic regression test set, 67–68multilayer perceptron, 70naïve Bayes classifier, 71–73random forest, 71TfidfVectorizer() method, 63train_logistic_model()  function, 65JJupyter notebook, 89KKeras, 7–8LLatent Dirichlet allocation (LDA)assumptions, 78beta distribution, 79joint probability  distribution, 79movie review datadocument classification, 81fit_transform() method, 82Gensim, 84–85scikit-learn  implementation, 86sklearn_topic_model() function, 85TFIDF model, 82–84topics, 82–83multinomial distribution, 78Poisson distribution, 78probability distribution, 79Index147TFIDF , 78topics and words simplexes, 80Long short-term memory (LSTM)BasicLSTMCell() function, 41formulae, 38gates, 39placeholder variables, 40–41sigmoid activation  function, 38–39tanh activation function, 38units/blocks, 37–38word embeddingscomputation graph, 112_embedding_array, 112error rate, 114executing code, 114load_data() function, 115preprocessing steps, 111remove_stop_words() function, 112reverse_dictionary, 113sample data, 111sample_text_dictionary() function, 112tf.nn.embedding_lookup() function, 114training data, 111_weights and _embedding variables, 113MMean squared error (MSE), 29–30Modeling stock returnsLSTM, 40MLPs, 15RNNs, 32Multilayer perceptron  models (MLPs)cross entropy, 30error function, 18–19Ford Motor Company (F), 15learning rateactivation function, 16, 24–25Adam optimization algorithm, 20–21epochs, 22floating-point value, 20optimal solution, 20parameter, 20, 22placeholder variables, 23ReLU activation function, 26sigmoid activation  function, 24–25TensorFlow, 22–23training and test sets, 22vanishing gradient, 26weights, neural network, 24MSE and RMSE loss  function, 29–30neural networks, 17normal distribution, 17sentiment analysis, 30SLPs, 13standard normal distribution, 14TensorFlow, 15tf.random_normal(), 17train_data, 15Index148vanishing gradients and  ReLU, 27–28visualization, 14weight and bias units, 17–18N, OName entity recognition (NER) taggeraggregate_function, 131categories, 128data set, Kaggle, 128embedding layer, 133feature extraction, 142input_data variable, 130–131integer labels, 132neural network, 130,  132text data, 129train_brnn_keras() function, 131zero padding, 132Natural language processing (NLP)Bayesian statistics, 3bifurcation, 3complexities and domains, 142computational linguistics, 2computing power, 3deep learning, Python ( see  Deep learning)definition, 1formal language theory, 2machine learning concepts, 4principles, 142SLP , 2–3spell-check, sentences, 31Natural Language Toolkit (NLTK) module, 45–46Natural language understanding (NLU), 3Neural networkscharacters, 136–138chatbots, 134dense/fully connected layer, 140encoder_decoder() function, 139–140JSON file, 136Keras models, 140one-hot encoded  vectors, 138–139seq2seq models, 140Stanford Question Answering Dataset, 135–136Non-negative matrix factorization (NMF)features, 87Gensim model, 90Jupyter notebook, 89–90and LDA, 90mathematical formula, 86scikit-learn implementation, 87–88,  90topic extraction, 88P,  QParagraph2Vec algorithm, 115movie review data, 116–118Principal components analysis (PCA), 97Multilayer perceptron models (MLPs) ( cont. )Index149RRecurrent neural networks (RNNs)activation function, 35BPTT , 36build_rnn() function, 32chain rule, 36data set, 33, 35floating-point decimal, 35gradient descent algorithm, 35hidden state, 32, 33LSTM ( see Long short-term memory (LSTM))placeholder variables, 34sigmoid activation function, 37state_size, 32–33structure, 31–32tanh activation and derivative function, 36–37TensorFlow, 32vanishing gradient, 36–37Root mean squared error  (RMSE), 29–30SSequence-to-sequence models (seq2seq), 133–134Sigmoid activation  function, 24–25Single-layer perceptron (SLP),  2–3,  13Skip-Gram modelarchitecture, 92k-skip-n-grams, 91negative sampling, 93neural network, 93n-gram, 91objective function, 91one-hot encoded vector, 922-skip-bi-gram model, 91training words, 91word embeddingcosine similarity, 98Gensim, 96–99hidden layer weight  matrix, 93index number, 99negative sampling, 101neural networks, 96one-hot encoding data, 100PCA, 97PDFMiner, 94TensorFlow, 94, 101–102tokenizing data, 95visualizing, 96–97vocabulary size and word dictionary, 100T , U, VTensorFlow, 4–7Term frequency–inverse document frequency (TFIDF), 57Text generation, LSTMsAI-based tools, 122BRNNs, 126–128data, 122epochs, 125–126Index150Harry Potter and the Sorcerer’s Stone, 122Keras code, 124load_data(), 122preprocessing function, 123–124Sequential().add()  function, 125Skip-Gram model, 124tf_preprocess_data(), 124Theano, 8–9Tokenization and stop wordsBoolean variable, 47data set, 44feature extraction  algorithms, 48–49function words, 46grammatical characters, 48lowercase, 47mistake() and advised_preprocessing()  functions, 47–48NLTK module, 45–46sample_sent_tokens, 46sample text, 44sample_word_tokens, 45, 49single string objects, 44uppercase, 48Topic models, 10description, 77LDA ( see Latent Dirichlet allocation (LDA))NMF ( see Non-negative matrix factorization (NMF))Word2Vec, 90–93W, X, YWord embeddings, 10–11CBoW, 103–105GloVe, 106–110LSTM ( see Long short-term memory (LSTM))Paragraph2Vec, 115–118Skip-Gram model ( see Skip-  Gram mo del)ZZero padding, 132Text generation, LSTMs ( cont. )Index'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = context.replace(\"\\n\",\"\")\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISBN-13 (electronic): 978-1-4842-3733-5https://doi.org/10.1007/978-1-4842-3733-5Library of Congress Control Number: 2018956300Copyright © 2018 by Taweh Beysolow II This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",google_api_key = GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_index = Chroma.from_texts(text,embeddings).as_retriever(search_kwargs={\"k\":3})\n",
    "# vector_index\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "# Create a FAISS HNSW index\n",
    "dimension = len(text_embeddings[0]) # dimensionality of the embeddings\n",
    "index = faiss.IndexHNSWFlat(dimension,32) #32 is the no of neighbours in HNSW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# convert embeddings to numpy array\n",
    "embedding_matrix = np.array(text_embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(texts))}  # Map FAISS indices to document IDs\n",
    "documents = [Document(page_content=text) for text in texts]     # Create a list of Document objects\n",
    "docstore = InMemoryDocstore(dict(zip(index_to_docstore_id.values(), documents)))  # Create a SimpleDocstore\n",
    "#Create vector store from faiss\n",
    "\n",
    "# 5. Create the FAISS vector store\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    index=index,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    docstore=docstore\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert in document extration. Answer the questions with relevant information by refering the context provided\n",
    "{context}\n",
    "Question :{question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\":QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "622",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is neural networks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:151\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    147\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 151\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:271\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[1;34m(self, question, run_manager)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    266\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    268\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_core\\retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    257\u001b[0m         result,\n\u001b[0;32m    258\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_core\\retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1085\u001b[0m             )\n\u001b[0;32m   1086\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:641\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    623\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:514\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 514\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Projects\\Langchain\\Rag_doc\\rag_doc_env\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:425\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# This happens when not enough docs are returned.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m _id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    426\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39msearch(_id)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Document):\n",
      "\u001b[1;31mKeyError\u001b[0m: 622"
     ]
    }
   ],
   "source": [
    "question = \"What is neural networks\"\n",
    "result = qa_chain({\"query\":question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='in the MLP , utilizing functions such as ReLU.\\xa0However, one of the more straightforward solutions is to use a model devised in the 1990s by Sepp Hochreiter and Jürgen Schmidhuber: the long short-term memory unit, or LSTM. Let’s start with what this model looks like, as shown in Figure\\xa0 2-7.Figure 2-6.  Tanh activation and derivative functionChapter 2  review of\\xa0Deep Learning38LSTMs are distinguished structurally by the fact that we observe them as blocks, or units, rather than the traditional'),\n",
       " Document(metadata={}, page_content='(NMF)features, 87Gensim model, 90Jupyter notebook, 89–90and LDA, 90mathematical formula, 86scikit-learn implementation, 87–88,  90topic extraction, 88P,  QParagraph2Vec algorithm, 115movie review data, 116–118Principal components analysis (PCA), 97Multilayer perceptron models (MLPs) ( cont. )Index149RRecurrent neural networks (RNNs)activation function, 35BPTT , 36build_rnn() function, 32chain rule, 36data set, 33, 35floating-point decimal, 35gradient descent algorithm, 35hidden state, 32,'),\n",
       " Document(metadata={}, page_content='4principles, 142SLP , 2–3spell-check, sentences, 31Natural Language Toolkit (NLTK) module, 45–46Natural language understanding (NLU), 3Neural networkscharacters, 136–138chatbots, 134dense/fully connected layer, 140encoder_decoder() function, 139–140JSON file, 136Keras models, 140one-hot encoded  vectors, 138–139seq2seq models, 140Stanford Question Answering Dataset, 135–136Non-negative matrix factorization (NMF)features, 87Gensim model, 90Jupyter notebook, 89–90and LDA, 90mathematical formula,')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_doc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
